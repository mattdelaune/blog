<!DOCTYPE html>

<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/png" href="/blog/assets/favicon/favicon.png" >

  
    
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Credit Card Approval Prediction (End-To-End Machine Learning Project) | MIB</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Credit Card Approval Prediction (End-To-End Machine Learning Project)" />
<meta name="author" content="Matt Delaune" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Welcome back, forks! After a long period of not posting here, I am happy to share that I am back again on MIB. In this post, we will work on an end-to-end machine learning project. I firmly believe this is one of the most detailed and comprehensive end-to-end ML project blog post on the internet. This project is perfect for the beginner in Machine Learning and seasoned ML engineers who could still learn one or two things from this post. This project was featured on Luke Barousse Youtube channel, click here to watch the video." />
<meta property="og:description" content="Welcome back, forks! After a long period of not posting here, I am happy to share that I am back again on MIB. In this post, we will work on an end-to-end machine learning project. I firmly believe this is one of the most detailed and comprehensive end-to-end ML project blog post on the internet. This project is perfect for the beginner in Machine Learning and seasoned ML engineers who could still learn one or two things from this post. This project was featured on Luke Barousse Youtube channel, click here to watch the video." />
<link rel="canonical" href="http://localhost:4000/blog/2022/10/12/credit-card-approval-prediction.html" />
<meta property="og:url" content="http://localhost:4000/blog/2022/10/12/credit-card-approval-prediction.html" />
<meta property="og:site_name" content="MIB" />
<meta property="og:image" content="http://localhost:4000/blog/assets/post_images/cc.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-10-12T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/blog/assets/post_images/cc.jpeg" />
<meta property="twitter:title" content="Credit Card Approval Prediction (End-To-End Machine Learning Project)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Matt Delaune"},"dateModified":"2022-10-12T00:00:00-05:00","datePublished":"2022-10-12T00:00:00-05:00","description":"Welcome back, forks! After a long period of not posting here, I am happy to share that I am back again on MIB. In this post, we will work on an end-to-end machine learning project. I firmly believe this is one of the most detailed and comprehensive end-to-end ML project blog post on the internet. This project is perfect for the beginner in Machine Learning and seasoned ML engineers who could still learn one or two things from this post. This project was featured on Luke Barousse Youtube channel, click here to watch the video.","headline":"Credit Card Approval Prediction (End-To-End Machine Learning Project)","image":"http://localhost:4000/blog/assets/post_images/cc.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/2022/10/12/credit-card-approval-prediction.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/blog/assets/logo/logo_1.png"},"name":"Matt Delaune"},"url":"http://localhost:4000/blog/2022/10/12/credit-card-approval-prediction.html"}</script>
<!-- End Jekyll SEO tag -->

  
  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/blog/assets/stylesheets/main.css">
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans:400,400i,600,600i|Fira+Sans+Condensed">
  

  
    
    <link rel="alternate" type="application/atom+xml" title="MIB" href="/blog/feed.xml">
  
</head>
 <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 

  <body class="layout--post credit-card-approval-prediction-end-to-end-machine-learning-project">

    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>


    <div class="sidebar-toggle-wrapper">
      
        <button class="search-toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <title>Search</title>
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
      

      <button class="toggle navicon-button larr" type="button">
        <span class="toggle-inner">
          <span class="sidebar-toggle-label visually-hidden">Menu</span>
          <span class="navicon"></span>
        </span>
      </button>
    </div>

    <div id="sidebar" class="sidebar">
      <div class="inner">
        <nav id="primary-nav" class="site-nav" itemscope itemtype="http://schema.org/SiteNavigationElement" aria-label="Main navigation">
  <ul id="menu-main-navigation" class="menu">
    <!-- Home link -->
    <li class="menu-item">
      <a href="/blog/" itemprop="url">
        <span itemprop="name">Home</span>
      </a>
    </li>

    <!-- site.pages links -->
    
    

    
      
      
        <li class="menu-item">
          <a href="/blog/archives/" itemprop="url">
            <span itemprop="name">Archives</span>
          </a>
        </li>
      
    
      
      
        <li class="menu-item">
          <a href="/blog/about/" itemprop="url">
            <span itemprop="name">About</span>
          </a>
        </li>
      
    
  </ul>
</nav>

        <ul class="contact-list">
  
    <li>
      <a href="mailto:ssemasuka@gmail.com">
        <span class="icon icon--email"><svg xmlns="http://www.w3.org/2000/svg" viewBox="313.1 3.7 16 16"><path d="M318.5 8.9c0-.2.2-.4.4-.4h4.5c.2 0 .4.2.4.4s-.2.4-.4.4h-4.5c-.3 0-.4-.2-.4-.4zm.4 2.1h4.5c.2 0 .4-.2.4-.4s-.2-.4-.4-.4h-4.5c-.2 0-.4.2-.4.4s.1.4.4.4zm3.5 1.2c0-.2-.2-.4-.4-.4h-3.1c-.2 0-.4.2-.4.4s.2.4.4.4h3.1c.2.1.4-.1.4-.4zm-1.5-8.4l-1.7 1.4c-.2.1-.2.4 0 .6s.4.2.6 0l1.4-1.2 1.4 1.2c.2.1.4.1.6 0s.1-.4 0-.6l-1.7-1.4c-.3-.1-.5-.1-.6 0zm7.8 6.2c.1.1.1.2.1.3v7.9c0 .8-.7 1.5-1.5 1.5h-12.5c-.8 0-1.5-.7-1.5-1.5v-7.9c0-.1.1-.2.1-.3l1.6-1.3c.2-.1.4-.1.6 0s.1.4 0 .6l-1.2 1 1.8 1.3v-4c0-.6.5-1.1 1.1-1.1h7.5c.6 0 1.1.5 1.1 1.1v4l1.8-1.3-1.2-1c-.2-.1-.2-.4 0-.6s.4-.2.6 0l1.6 1.3zm-11.6 2.2l4 2.8 4-2.8V7.6c0-.1-.1-.2-.2-.2h-7.5c-.1 0-.2.1-.2.2v4.6zm10.9-1l-4.7 3.4 3.4 2.6c.2.1.2.4.1.6-.1.2-.4.2-.6.1l-3.6-2.8-1.2.8c-.1.1-.3.1-.5 0l-1.2-.8-3.6 2.8c-.2.1-.4.1-.6-.1-.1-.2-.1-.4.1-.6l3.4-2.6-4.7-3.4v7.1c0 .4.3.6.6.6h12.5c.4 0 .6-.3.6-.6v-7.1z"/></svg></span>
        <span class="label">Email</span>
      </a>
    </li>
  

  
    <li><a href="https://github.com/semasuka">
  <span class="icon icon--github"><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M8 0C3.58 0 0 3.582 0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385 0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953 0-.873.31-1.587.823-2.147-.09-.202-.36-1.015.07-2.117 0 0 .67-.215 2.2.82.64-.178 1.32-.266 2-.27.68.004 1.36.092 2 .27 1.52-1.035 2.19-.82 2.19-.82.43 1.102.16 1.915.08 2.117.51.56.82 1.274.82 2.147 0 3.073-1.87 3.75-3.65 3.947.28.24.54.73.54 1.48 0 1.07-.01 1.93-.01 2.19 0 .21.14.46.55.38C13.71 14.53 16 11.53 16 8c0-4.418-3.582-8-8-8"/></svg></span>
  <span class="label">GitHub</span>
</a>
</li>
  

  
    <li><a href="https://www.linkedin.com/in/stern-semasuka">
  <span class="icon icon--linkedin"><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M13.632 13.635h-2.37V9.922c0-.886-.018-2.025-1.234-2.025-1.235 0-1.424.964-1.424 1.96v3.778h-2.37V6H8.51v1.04h.03c.318-.6 1.092-1.233 2.247-1.233 2.4 0 2.845 1.58 2.845 3.637v4.188zM3.558 4.955c-.762 0-1.376-.617-1.376-1.377 0-.758.614-1.375 1.376-1.375.76 0 1.376.617 1.376 1.375 0 .76-.617 1.377-1.376 1.377zm1.188 8.68H2.37V6h2.376v7.635zM14.816 0H1.18C.528 0 0 .516 0 1.153v13.694C0 15.484.528 16 1.18 16h13.635c.652 0 1.185-.516 1.185-1.153V1.153C16 .516 15.467 0 14.815 0z" fill-rule="nonzero"/></svg></span>
  <span class="label">Linkedin</span>
</a>
</li>
  

  <li>
    
      <a href="/blog/feed.xml" title="Atom Feed">
        <span class="icon icon--rss"><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194 11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0 13.806c0-1.21.983-2.195 2.194-2.195zM10.606 16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"/></svg></span>
        <span class="label">RSS</span>
      </a>
    
  </li>
  <li>
    <!-- Begin Mailchimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{
        background:#fff;
        clear:left;
        font:14px Helvetica,Arial,sans-serif;
        width:100%;
    }
    #mc_embed_signup input.email {
        display: none
    }
    #mc_embed_signup .button{
        background-color: #539F30;
        -webkit-box-transition: all 0.5s ease;
        -moz-box-transition: all 0.5s ease;
        transition: all 0.5s ease;
        border-radius: 8px;
        -webkit-box-shadow: 9px 5px 52px -7px rgba(0, 0, 0, 0.52);
        -moz-box-shadow: 9px 5px 52px -7px rgba(0, 0, 0, 0.52);
        box-shadow: 9px 5px 52px -7px rgba(0, 0, 0, 0.52);
        }
    #mc_embed_signup .button:hover {
        background-color: rgb(52, 99, 30);
        -webkit-box-transition: all 0.5s ease;
        -moz-box-transition: all 0.5s ease;
        transition: all 0.5s ease;
        color: rgb(187, 187, 187);
        -webkit-box-shadow: none;
        -moz-box-shadow: none;
        box-shadow: none;
        }
    #mc_embed_signup {width: 80%;}
</style>
<div id="mc_embed_signup">
<form action="https://github.us7.list-manage.com/subscribe/post?u=a42c4a501e1abb66402e41f08&amp;id=8b7e08e218" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<label for="mce-EMAIL">Get notified each time there's a new post by subscribing</label>
	<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_a42c4a501e1abb66402e41f08_8b7e08e218" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe Now" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
  </li>
</ul>

      </div>
    </div>

    <div class="canvas">
      <div class="wrapper">
        

<header id="masthead">
  <div class="inner">
    <div class="title-area">
      
        <p class="site-title">
          <a href="/blog/">
            <img src="/blog/assets/logo/logo_1.png" alt="" class="site-logo">
            <span>MIB</span>
          </a>
        </p>
      
    </div>
  </div>
</header>

        <div class="initial-content">
          
<header class="intro">
  
    
    
    

    <div class="intro-image">
        <img src="/blog/assets/post_images/cc.jpeg" alt="Credit Card Approval Prediction (End-To-End Machine Learning Project)">
        <div class="pattern"></div>
        <h1 id="page-title" class="intro-title">Credit Card Approval Prediction (End-To-End Machine Learning Project)
</h1>
        <a href="#entry-id"><div class="scroll-down"></div></a>
    </div>
  

  <div class="inner">
    <div class="intro-text">
      

      
        


        <p class="entry-meta">
          
            <span class="byline-item">by Matt Delaune</span>
          
          <span class="byline-item"><span class="icon"><img src=" /blog/assets/logo/tags.png" height="42" width="42"></span>python, machine learning, tutorial, classification, project, deployment</span>
          <span class="byline-item"><span class="icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="379 72 16 16"><g><g><path fill="none" d="M380.8,86.7h12.3v-8.8h-12.3V86.7z M389.5,78.8h1.7v1.4h-1.7V78.8z M389.5,81.3h1.7v1.4h-1.7V81.3z M389.5,83.8h1.7v1.4h-1.7V83.8z M386.1,78.8h1.7v1.4h-1.7V78.8z M386.1,81.3h1.7v1.4h-1.7V81.3z M386.1,83.8h1.7v1.4h-1.7V83.8z M382.8,78.8h1.7v1.4h-1.7V78.8z M382.8,81.3h1.7v1.4h-1.7V81.3z M382.8,83.8h1.7v1.4h-1.7V83.8z"/><polygon fill="none" points="384.7 75.1 383.4 75.1 383.4 74.3 380.8 74.3 380.8 76.6 393.2 76.6 393.2 74.3 390.6 74.3 390.6 75.1 389.3 75.1 389.3 74.3 384.7 74.3"/><rect x="382.8" y="78.8" width="1.7" height="1.4"/><rect x="386.1" y="78.8" width="1.7" height="1.4"/><rect x="389.5" y="78.8" width="1.7" height="1.4"/><rect x="382.8" y="81.3" width="1.7" height="1.4"/><rect x="386.1" y="81.3" width="1.7" height="1.4"/><rect x="389.5" y="81.3" width="1.7" height="1.4"/><rect x="382.8" y="83.8" width="1.7" height="1.4"/><rect x="386.1" y="83.8" width="1.7" height="1.4"/><rect x="389.5" y="83.8" width="1.7" height="1.4"/><path d="M383.4,72v1.1h-3.8V88h14.9V73.1h-3.8V72h-1.3v1.1h-4.7V72H383.4z M393.2,86.7h-12.3v-8.8h12.3L393.2,86.7L393.2,86.7z M389.3,74.3v0.8h1.3v-0.8h2.5v2.3h-12.3v-2.3h2.5v0.8h1.3v-0.8H389.3z"/></g></g></svg></span><time datetime="2022-10-12T00:00:00-05:00">Wed October 12, 2022</time></span>
          
            <span class="byline-item"><span class="icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="15 309.7 16 16"><g><path d="M23.9 315.1v3.6c0 .5-.4.9-.9.9s-.9-.4-.9-.9v-3.6h1.8z"/><path d="M30.1 317.7c.5 3.9-2.3 7.5-6.2 7.9-3.9.5-7.5-2.3-7.9-6.2-.5-3.9 2.3-7.5 6.2-7.9v-1.8H24v1.8c1.1.1 2.7.7 3.5 1.4l1.3-1.3 1.3 1.3-1.3 1.3c.5.9 1.2 2.5 1.3 3.5zm-1.8.9c0-2.9-2.4-5.3-5.3-5.3s-5.3 2.4-5.3 5.3 2.4 5.3 5.3 5.3 5.3-2.3 5.3-5.3z"/></g></svg></span>85 min read</span>
          
        </p>
      

      

      
    </div>
  </div>
</header>


<main id="main" class="page-content" aria-label="Content">
  <div class="inner">
    <article class="entry-wrap">
      <div id="entry" class="entry-content">
        <p>Welcome back, forks! After a long period of not posting here, I am happy to share that I am back again on MIB. In this post, we will work on an end-to-end machine learning project. I firmly believe this is one of the most detailed and comprehensive end-to-end ML project blog post on the internet. This project is perfect for the beginner in Machine Learning and seasoned ML engineers who could still learn one or two things from this post. This project was featured on Luke Barousse Youtube channel, click <a href="https://www.youtube.com/watch?v=5Q0gB7imNOo&amp;t=222s">here</a> to watch the video.<!-- more --></p>

<p>Here is the roadmap we will follow:</p>
<ul>
  <li>We will start with exploratory data analysis(EDA)</li>
  <li>Feature engineering</li>
  <li>Feature selection</li>
  <li>Data preprocessing</li>
  <li>Model training</li>
  <li>Model selection</li>
  <li>Model storage on AWS blob storage</li>
  <li>Build a web app interface for the model using Streamlit.</li>
  <li>Finally, deploy the model.</li>
</ul>

<p>The goal is to predict whether an application for a credit card will be approved or not, using the applicant data.</p>

<p>I chose this project because when applying for a loan, credit card, or any other type of credit at any financial institution, there is a hard inquiry that affects your credit score negatively. This app predicts the probability of being approved without affecting your credit score. This app can be used by applicants who want to find out if they will be approved for a credit card without affecting their credit score.</p>

<p><strong><em>For those who are in a hurry, here is the key insights results from the analysis of this project:</em></strong></p>

<p>Correlation between the features.</p>

<p><img src="/blog/assets/post_cont_image/heatmap_cc_approval.png" alt="heatmap" /></p>

<p>Confusion matrix of gradient boosting classifier.</p>

<p><img src="/blog/assets/post_cont_image/cm_cc_approval.png" alt="Confusion matrix" /></p>

<p>ROC curve of gradient boosting classifier.</p>

<p><img src="/blog/assets/post_cont_image/roc_cc_approval.png" alt="ROC curve" /></p>

<p>Top 3 models (with default parameters)</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Recall score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Support vector machine</td>
      <td>88%</td>
    </tr>
    <tr>
      <td>Gradient boosting</td>
      <td>90%</td>
    </tr>
    <tr>
      <td>Adaboost</td>
      <td>79%</td>
    </tr>
  </tbody>
</table>

<ul>
  <li><strong>The final model used for this project: Gradient boosting</strong></li>
  <li><strong>Metrics used: Recall</strong></li>
  <li>
    <p><strong>Why choose recall as metrics</strong>:
Since the objective of this problem is to minimize the risk of a credit default, the metrics to use depends on the current economic situation:</p>

    <ul>
      <li>
        <p>During a bull market (when the economy is expanding), people feel wealthy and are employed. Money is usually cheap, and the risk of default is low because of economic stability and low unemployment. The financial institution can handle the risk of default; therefore, it is not very strict about giving credit. The financial institution can handle some bad clients as long as most credit card owners are good clients (aka those who pay back their credit in time and in total).In this case, having a good recall (sensitivity) is ideal.</p>
      </li>
      <li>
        <p>During a bear market (when the economy is contracting), people lose their jobs and money through the stock market and other investment venues. Many people struggle to meet their financial obligations. The financial institution, therefore, tends to be more conservative in giving out credit or loans. The financial institution can’t afford to give out credit to many clients who won’t be able to pay back their credit. The financial institution would rather have a smaller number of good clients, even if it means that some good clients are denied credit. In this case, having a good precision (specificity) is desirable.</p>

        <p><strong><em>Note</em></strong>: There is always a trade-off between precision and recall. Choosing the right metrics depends on the problem you are solving.</p>

        <p><strong><em>Conclusion</em></strong>: Since the time I worked on this project (beginning 2022), we were in the longest bull market (excluding March 2020 flash crash) ever recorded; we will use recall as our metric.</p>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Lessons learned and recommendation</strong></p>

<ul>
  <li>Based on this project’s analysis, income, family member headcount, and employment length are the three most predictive features in determining whether an applicant will be approved for a credit card. Other features like age and working employment status are also helpful. The least useful features are the type of dwelling and car ownership.</li>
  <li>The recommendation would be to focus more on the most predictive features when looking at the applicant profile and pay less attention to the least predictive features.</li>
</ul>

<p><strong><em>For the rest of my nerdy friends, let’s get started from scratch</em></strong></p>

<h3>Pre-requisites</h3>

<p>Wait! no, so fast! Before we start writing code, we need to have our python/jupyter environment ready, and Ken Jee has a fantastic video on this; click <a href="https://www.youtube.com/watch?v=C4OPn58BLaU">here</a> to watch it.</p>

<h3>Import necessary libraries</h3>

<p>Now we can import all the required libraries. Feel free to visit my <a href="https://semasuka.github.io/blog/2019/01/06/introduction-to-jupyter-notebook.html">other post</a>, where I talk about installing these libraries in the jupyter environment.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">missingno</span> <span class="k">as</span> <span class="n">msno</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">pandas.core.common</span> <span class="kn">import</span> <span class="n">SettingWithCopyWarning</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">probplot</span><span class="p">,</span> <span class="n">chi2_contingency</span><span class="p">,</span> <span class="n">chi2</span><span class="p">,</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">RandomizedSearchCV</span><span class="p">,</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">cross_val_predict</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.calibration</span> <span class="kn">import</span> <span class="n">CalibratedClassifierCV</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">MinMaxScaler</span><span class="p">,</span> <span class="n">OrdinalEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">SMOTE</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span><span class="p">,</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">GradientBoostingClassifier</span><span class="p">,</span> <span class="n">BaggingClassifier</span><span class="p">,</span> <span class="n">AdaBoostClassifier</span><span class="p">,</span> <span class="n">ExtraTreesClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">permutation_importance</span>
<span class="kn">import</span> <span class="nn">scikitplot</span> <span class="k">as</span> <span class="n">skplt</span>
<span class="kn">from</span> <span class="nn">yellowbrick.model_selection</span> <span class="kn">import</span> <span class="n">FeatureImportances</span>
<span class="kn">import</span> <span class="nn">joblib</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<p>I will briefly explain what each library does and why we need it for this project.</p>

<ul>
  <li>NumPy is a library for manipulating multidimensional arrays and matrices. In this project, we will use NumPy to change the sequences of the elements in a list and also transform an array with negative values into absolute ones.</li>
  <li>Pandas is a library to manipulate tabular data stored as dataframes (More than two columns) and Series(when dealing with one column); we will use it in this project to import the data into our notebook, create dataframes, merge and concatenate dataframes.</li>
  <li>MissingNo is a great library to visualize at a glance missing value in a Pandas dataframe.</li>
  <li>Scipy is a library that contains mathematical modules like statistics, optimization, linear algebra, etc</li>
  <li>Pathlib is a built-in python library with useful path functionalities. Pathlib will use it in the project to check if a file exists at a specific path, then use the joblib to save it.</li>
  <li>Matplotlib is a data visualization library to plot different types of plots like histograms, line plots, scatter plots, contour plots, etc. It is built on top of NumPy.</li>
  <li>Seaborn is another data visualization library built on top of Matplotlib with added features and simpler syntax than Matplotlib. We will mainly use this library for our exploratory data analysis.</li>
  <li>Warnings is a python builtin library to control the warnings at the execution time</li>
  <li>Scikit-learn, also called sklearn, is the industry standard machine learning library from which all the machine learning algorithms are imported. It is built on NumPy, Scipy, and Matplotlib.</li>
  <li>Imbalance learn is a library based on sklearn, which provides tools when dealing with classification with imbalanced classes. Here classes mean the prediction results, which in this case, are approved or denied for a credit card. In this project, we have two outcomes (we have a binary classification), and one of the outcomes is less likely to happen, which is reflected in the data. So we use the SMOTE technique to balance the outcomes because we don’t want to train on unbalanced data as we try to avoid bias.</li>
  <li>Scikit-plot is a helpful library that plots scikit-learn objects; for this project, Scikit-plot will use to plot the ROC curve.</li>
  <li>Yellowbrick extends the scikit-learn API library to make a model selection. In this project, we have used it to plot the feature importance.</li>
  <li>Joblib is a builtin python library to save models as files; those models will deploy on AWS S3</li>
  <li>os is a builtin library to access some of the operating system functionality</li>
  <li>Finally, magic command <code class="language-plaintext highlighter-rouge">%matplotlib inline</code> will make your plot outputs appear and be stored within the notebook.</li>
</ul>

<h3>Import the data</h3>

<p>After importing the libraries, we will now import the datasets. The datasets are from Kaggle. Here is the <a href="https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction">link</a>.</p>

<p>There are two ways to import the CSV, we can download the file and pass the local machine path to the <code class="language-plaintext highlighter-rouge">read_csv</code> pandas function, or we can host the data on GitHub and directly read the hosted CSV file as a raw data. In this case, we went with the latter method.</p>

<p>The first dataset is the application record with all the information about the applicants like gender, age, income, etc. The second dataset is the credit record which holds information about the credit status and balance. we will store those two dataset in <code class="language-plaintext highlighter-rouge">cc_data_full_data</code> and <code class="language-plaintext highlighter-rouge">credit_status</code> respectively.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cc_data_full_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/semasuka/Credit-card-approval-prediction-classification/main/datasets/application_record.csv'</span><span class="p">)</span>
<span class="n">credit_status</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/semasuka/Credit-card-approval-prediction-classification/main/datasets/credit_record.csv'</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s glance at the first five rows using each Pandas’ <code class="language-plaintext highlighter-rouge">head</code>` method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cc_data_full_data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>CODE_GENDER</th>
      <th>FLAG_OWN_CAR</th>
      <th>FLAG_OWN_REALTY</th>
      <th>CNT_CHILDREN</th>
      <th>AMT_INCOME_TOTAL</th>
      <th>NAME_INCOME_TYPE</th>
      <th>NAME_EDUCATION_TYPE</th>
      <th>NAME_FAMILY_STATUS</th>
      <th>NAME_HOUSING_TYPE</th>
      <th>DAYS_BIRTH</th>
      <th>DAYS_EMPLOYED</th>
      <th>FLAG_MOBIL</th>
      <th>FLAG_WORK_PHONE</th>
      <th>FLAG_PHONE</th>
      <th>FLAG_EMAIL</th>
      <th>OCCUPATION_TYPE</th>
      <th>CNT_FAM_MEMBERS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5008804</td>
      <td>M</td>
      <td>Y</td>
      <td>Y</td>
      <td>0</td>
      <td>427500.0</td>
      <td>Working</td>
      <td>Higher education</td>
      <td>Civil marriage</td>
      <td>Rented apartment</td>
      <td>-12005</td>
      <td>-4542</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5008805</td>
      <td>M</td>
      <td>Y</td>
      <td>Y</td>
      <td>0</td>
      <td>427500.0</td>
      <td>Working</td>
      <td>Higher education</td>
      <td>Civil marriage</td>
      <td>Rented apartment</td>
      <td>-12005</td>
      <td>-4542</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5008806</td>
      <td>M</td>
      <td>Y</td>
      <td>Y</td>
      <td>0</td>
      <td>112500.0</td>
      <td>Working</td>
      <td>Secondary / secondary special</td>
      <td>Married</td>
      <td>House / apartment</td>
      <td>-21474</td>
      <td>-1134</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Security staff</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5008808</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>0</td>
      <td>270000.0</td>
      <td>Commercial associate</td>
      <td>Secondary / secondary special</td>
      <td>Single / not married</td>
      <td>House / apartment</td>
      <td>-19110</td>
      <td>-3051</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>Sales staff</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5008809</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>0</td>
      <td>270000.0</td>
      <td>Commercial associate</td>
      <td>Secondary / secondary special</td>
      <td>Single / not married</td>
      <td>House / apartment</td>
      <td>-19110</td>
      <td>-3051</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>Sales staff</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">credit_status</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>MONTHS_BALANCE</th>
      <th>STATUS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5001711</td>
      <td>0</td>
      <td>X</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5001711</td>
      <td>-1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5001711</td>
      <td>-2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5001711</td>
      <td>-3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5001712</td>
      <td>0</td>
      <td>C</td>
    </tr>
  </tbody>
</table>
</div>

<p>Now let’s look at the metadata of the datasets to understand the data better.</p>

<p>For the application record dataset.</p>

<p><img src="/blog/assets/post_cont_image/cc_app_meta.png" alt="appli_rec_metadata" /></p>

<p>And for the credit record dataset.</p>

<p><img src="/blog/assets/post_cont_image/credit_meta.png" alt="appli_rec_metadata" /></p>

<h3>Creating a target variable</h3>

<p>As you may have noticed from our first dataset, we don’t have a target variable that states whether the client is good or not (a client who will not default on their credit card would be called a good client). We will use the credit record to come up with the target variable. We use the <a href="https://www.listendata.com/2019/09/credit-risk-vintage-analysis.html">vintage analysis</a> for this.</p>

<p>For simplicity purposes, we will say that the applicants over 60 days overdue are considered bad clients. When the target variable is 1, that means a bad client, and when it is 0, that represents a good client. That is what the following script does.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">begin_month</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">credit_status</span><span class="p">.</span><span class="n">groupby</span><span class="p">([</span><span class="s">'ID'</span><span class="p">])[</span><span class="s">'MONTHS_BALANCE'</span><span class="p">].</span><span class="n">agg</span><span class="p">(</span><span class="nb">min</span><span class="p">))</span>
<span class="n">begin_month</span><span class="o">=</span><span class="n">begin_month</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'MONTHS_BALANCE'</span><span class="p">:</span><span class="s">'Account age'</span><span class="p">})</span>
<span class="n">cc_data_full_data</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">merge</span><span class="p">(</span><span class="n">cc_data_full_data</span><span class="p">,</span><span class="n">begin_month</span><span class="p">,</span><span class="n">how</span><span class="o">=</span><span class="s">'left'</span><span class="p">,</span><span class="n">on</span><span class="o">=</span><span class="s">'ID'</span><span class="p">)</span>
<span class="n">credit_status</span><span class="p">[</span><span class="s">'dep_value'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">credit_status</span><span class="p">[</span><span class="s">'dep_value'</span><span class="p">][</span><span class="n">credit_status</span><span class="p">[</span><span class="s">'STATUS'</span><span class="p">]</span> <span class="o">==</span><span class="s">'2'</span><span class="p">]</span><span class="o">=</span><span class="s">'Yes'</span>
<span class="n">credit_status</span><span class="p">[</span><span class="s">'dep_value'</span><span class="p">][</span><span class="n">credit_status</span><span class="p">[</span><span class="s">'STATUS'</span><span class="p">]</span> <span class="o">==</span><span class="s">'3'</span><span class="p">]</span><span class="o">=</span><span class="s">'Yes'</span>
<span class="n">credit_status</span><span class="p">[</span><span class="s">'dep_value'</span><span class="p">][</span><span class="n">credit_status</span><span class="p">[</span><span class="s">'STATUS'</span><span class="p">]</span> <span class="o">==</span><span class="s">'4'</span><span class="p">]</span><span class="o">=</span><span class="s">'Yes'</span>
<span class="n">credit_status</span><span class="p">[</span><span class="s">'dep_value'</span><span class="p">][</span><span class="n">credit_status</span><span class="p">[</span><span class="s">'STATUS'</span><span class="p">]</span> <span class="o">==</span><span class="s">'5'</span><span class="p">]</span><span class="o">=</span><span class="s">'Yes'</span>
<span class="n">cpunt</span><span class="o">=</span><span class="n">credit_status</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'ID'</span><span class="p">).</span><span class="n">count</span><span class="p">()</span>
<span class="n">cpunt</span><span class="p">[</span><span class="s">'dep_value'</span><span class="p">][</span><span class="n">cpunt</span><span class="p">[</span><span class="s">'dep_value'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="s">'Yes'</span>
<span class="n">cpunt</span><span class="p">[</span><span class="s">'dep_value'</span><span class="p">][</span><span class="n">cpunt</span><span class="p">[</span><span class="s">'dep_value'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="s">'No'</span>
<span class="n">cpunt</span> <span class="o">=</span> <span class="n">cpunt</span><span class="p">[[</span><span class="s">'dep_value'</span><span class="p">]]</span>
<span class="n">cc_data_full_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">merge</span><span class="p">(</span><span class="n">cc_data_full_data</span><span class="p">,</span><span class="n">cpunt</span><span class="p">,</span><span class="n">how</span><span class="o">=</span><span class="s">'inner'</span><span class="p">,</span><span class="n">on</span><span class="o">=</span><span class="s">'ID'</span><span class="p">)</span>
<span class="n">cc_data_full_data</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">]</span><span class="o">=</span><span class="n">cc_data_full_data</span><span class="p">[</span><span class="s">'dep_value'</span><span class="p">]</span>
<span class="n">cc_data_full_data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">cc_data_full_data</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">]</span><span class="o">==</span><span class="s">'Yes'</span><span class="p">,</span><span class="s">'Is high risk'</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>
<span class="n">cc_data_full_data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">cc_data_full_data</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">]</span><span class="o">==</span><span class="s">'No'</span><span class="p">,</span><span class="s">'Is high risk'</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
<span class="n">cc_data_full_data</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'dep_value'</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">pd</span><span class="p">.</span><span class="n">options</span><span class="p">.</span><span class="n">mode</span><span class="p">.</span><span class="n">chained_assignment</span> <span class="o">=</span> <span class="bp">None</span> <span class="c1"># hide warning SettingWithCopyWarning
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/var/folders/bb/dzx22n7n1t1gkqfhhky4j2ch0000gn/T/ipykernel_29855/1467211908.py:5: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  credit_status['dep_value'][credit_status['STATUS'] =='2']='Yes'
/var/folders/bb/dzx22n7n1t1gkqfhhky4j2ch0000gn/T/ipykernel_29855/1467211908.py:6: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  credit_status['dep_value'][credit_status['STATUS'] =='3']='Yes'
/var/folders/bb/dzx22n7n1t1gkqfhhky4j2ch0000gn/T/ipykernel_29855/1467211908.py:7: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  credit_status['dep_value'][credit_status['STATUS'] =='4']='Yes'
/var/folders/bb/dzx22n7n1t1gkqfhhky4j2ch0000gn/T/ipykernel_29855/1467211908.py:8: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  credit_status['dep_value'][credit_status['STATUS'] =='5']='Yes'
</code></pre></div></div>

<p>Let’s print the first 5 rows of the dataframe, with the newly created target column <code class="language-plaintext highlighter-rouge">Is high risk</code> at the end.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cc_data_full_data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>CODE_GENDER</th>
      <th>FLAG_OWN_CAR</th>
      <th>FLAG_OWN_REALTY</th>
      <th>CNT_CHILDREN</th>
      <th>AMT_INCOME_TOTAL</th>
      <th>NAME_INCOME_TYPE</th>
      <th>NAME_EDUCATION_TYPE</th>
      <th>NAME_FAMILY_STATUS</th>
      <th>NAME_HOUSING_TYPE</th>
      <th>DAYS_BIRTH</th>
      <th>DAYS_EMPLOYED</th>
      <th>FLAG_MOBIL</th>
      <th>FLAG_WORK_PHONE</th>
      <th>FLAG_PHONE</th>
      <th>FLAG_EMAIL</th>
      <th>OCCUPATION_TYPE</th>
      <th>CNT_FAM_MEMBERS</th>
      <th>Account age</th>
      <th>Is high risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5008804</td>
      <td>M</td>
      <td>Y</td>
      <td>Y</td>
      <td>0</td>
      <td>427500.0</td>
      <td>Working</td>
      <td>Higher education</td>
      <td>Civil marriage</td>
      <td>Rented apartment</td>
      <td>-12005</td>
      <td>-4542</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>-15.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5008805</td>
      <td>M</td>
      <td>Y</td>
      <td>Y</td>
      <td>0</td>
      <td>427500.0</td>
      <td>Working</td>
      <td>Higher education</td>
      <td>Civil marriage</td>
      <td>Rented apartment</td>
      <td>-12005</td>
      <td>-4542</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>-14.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5008806</td>
      <td>M</td>
      <td>Y</td>
      <td>Y</td>
      <td>0</td>
      <td>112500.0</td>
      <td>Working</td>
      <td>Secondary / secondary special</td>
      <td>Married</td>
      <td>House / apartment</td>
      <td>-21474</td>
      <td>-1134</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Security staff</td>
      <td>2.0</td>
      <td>-29.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5008808</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>0</td>
      <td>270000.0</td>
      <td>Commercial associate</td>
      <td>Secondary / secondary special</td>
      <td>Single / not married</td>
      <td>House / apartment</td>
      <td>-19110</td>
      <td>-3051</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>Sales staff</td>
      <td>1.0</td>
      <td>-4.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5008809</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>0</td>
      <td>270000.0</td>
      <td>Commercial associate</td>
      <td>Secondary / secondary special</td>
      <td>Single / not married</td>
      <td>House / apartment</td>
      <td>-19110</td>
      <td>-3051</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>Sales staff</td>
      <td>1.0</td>
      <td>-26.0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>Since the features (columns) names are not very descriptive, we will change them to make them more readable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># rename the features to more readable feature names
</span><span class="n">cc_data_full_data</span> <span class="o">=</span> <span class="n">cc_data_full_data</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span>
    <span class="s">'CODE_GENDER'</span><span class="p">:</span><span class="s">'Gender'</span><span class="p">,</span>
    <span class="s">'FLAG_OWN_CAR'</span><span class="p">:</span><span class="s">'Has a car'</span><span class="p">,</span>
    <span class="s">'FLAG_OWN_REALTY'</span><span class="p">:</span><span class="s">'Has a property'</span><span class="p">,</span>
    <span class="s">'CNT_CHILDREN'</span><span class="p">:</span><span class="s">'Children count'</span><span class="p">,</span>
    <span class="s">'AMT_INCOME_TOTAL'</span><span class="p">:</span><span class="s">'Income'</span><span class="p">,</span>
    <span class="s">'NAME_INCOME_TYPE'</span><span class="p">:</span><span class="s">'Employment status'</span><span class="p">,</span>
    <span class="s">'NAME_EDUCATION_TYPE'</span><span class="p">:</span><span class="s">'Education level'</span><span class="p">,</span>
    <span class="s">'NAME_FAMILY_STATUS'</span><span class="p">:</span><span class="s">'Marital status'</span><span class="p">,</span>
    <span class="s">'NAME_HOUSING_TYPE'</span><span class="p">:</span><span class="s">'Dwelling'</span><span class="p">,</span>
    <span class="s">'DAYS_BIRTH'</span><span class="p">:</span><span class="s">'Age'</span><span class="p">,</span>
    <span class="s">'DAYS_EMPLOYED'</span><span class="p">:</span> <span class="s">'Employment length'</span><span class="p">,</span>
    <span class="s">'FLAG_MOBIL'</span><span class="p">:</span> <span class="s">'Has a mobile phone'</span><span class="p">,</span>
    <span class="s">'FLAG_WORK_PHONE'</span><span class="p">:</span> <span class="s">'Has a work phone'</span><span class="p">,</span>
    <span class="s">'FLAG_PHONE'</span><span class="p">:</span> <span class="s">'Has a phone'</span><span class="p">,</span>
    <span class="s">'FLAG_EMAIL'</span><span class="p">:</span> <span class="s">'Has an email'</span><span class="p">,</span>
    <span class="s">'OCCUPATION_TYPE'</span><span class="p">:</span> <span class="s">'Job title'</span><span class="p">,</span>
    <span class="s">'CNT_FAM_MEMBERS'</span><span class="p">:</span> <span class="s">'Family member count'</span><span class="p">,</span>
    <span class="s">'Account age'</span><span class="p">:</span> <span class="s">'Account age'</span>
    <span class="p">})</span>
</code></pre></div></div>

<p>Now we will split the <code class="language-plaintext highlighter-rouge">cc_data_full_data</code> into a training and testing set. We will use 80% of the data for training and 20% for testing and store them respectively in <code class="language-plaintext highlighter-rouge">cc_train_original</code> and <code class="language-plaintext highlighter-rouge">cc_test_original</code> variables.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># split the data into train and test dataset
</span><span class="k">def</span> <span class="nf">data_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="p">):</span>
    <span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="c1"># reset the indexes
</span>    <span class="k">return</span> <span class="n">train_df</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">test_df</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># we set the test_size to 0.2, which means that the train_size will be 0.8
</span><span class="n">cc_train_original</span><span class="p">,</span> <span class="n">cc_test_original</span> <span class="o">=</span> <span class="n">data_split</span><span class="p">(</span><span class="n">cc_data_full_data</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
</code></pre></div></div>

<p>Dataframe’s <code class="language-plaintext highlighter-rouge">shape</code> function helps us know the dimension of the dataframe. Here we have 20 features(columns) and 29165 observations(rows) for the training dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cc_train_original</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(29165, 20)
</code></pre></div></div>

<p>And 20 features(columns) and 7292 observations(rows) for the testing dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cc_test_original</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(7292, 20)
</code></pre></div></div>

<p>Finally, we will export the data as a CSV file on our local machine and create a copy of the dataset. Please note that these steps are optional. It is best practice to keep the original dataset untouched as a backup and work with the copy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cc_train_original</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">'dataset/train.csv'</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cc_test_original</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">'dataset/test.csv'</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># creating a copy of the dataset so that the original stays untouched
</span><span class="n">cc_train_copy</span> <span class="o">=</span> <span class="n">cc_train_original</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">cc_test_copy</span> <span class="o">=</span> <span class="n">cc_test_original</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
</code></pre></div></div>

<h3>Data at a glance</h3>

<p>Now that we have split the dataset into training and testing datasets, we will focus on the training dataset for now and use the test dataset toward the end of this post.</p>

<p>Let’s review the first 5 rows again with the <code class="language-plaintext highlighter-rouge">head()</code> function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cc_data_full_data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>Gender</th>
      <th>Has a car</th>
      <th>Has a property</th>
      <th>Children count</th>
      <th>Income</th>
      <th>Employment status</th>
      <th>Education level</th>
      <th>Marital status</th>
      <th>Dwelling</th>
      <th>Age</th>
      <th>Employment length</th>
      <th>Has a mobile phone</th>
      <th>Has a work phone</th>
      <th>Has a phone</th>
      <th>Has an email</th>
      <th>Job title</th>
      <th>Family member count</th>
      <th>Account age</th>
      <th>Is high risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5008804</td>
      <td>M</td>
      <td>Y</td>
      <td>Y</td>
      <td>0</td>
      <td>427500.0</td>
      <td>Working</td>
      <td>Higher education</td>
      <td>Civil marriage</td>
      <td>Rented apartment</td>
      <td>-12005</td>
      <td>-4542</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>-15.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5008805</td>
      <td>M</td>
      <td>Y</td>
      <td>Y</td>
      <td>0</td>
      <td>427500.0</td>
      <td>Working</td>
      <td>Higher education</td>
      <td>Civil marriage</td>
      <td>Rented apartment</td>
      <td>-12005</td>
      <td>-4542</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>-14.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5008806</td>
      <td>M</td>
      <td>Y</td>
      <td>Y</td>
      <td>0</td>
      <td>112500.0</td>
      <td>Working</td>
      <td>Secondary / secondary special</td>
      <td>Married</td>
      <td>House / apartment</td>
      <td>-21474</td>
      <td>-1134</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Security staff</td>
      <td>2.0</td>
      <td>-29.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5008808</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>0</td>
      <td>270000.0</td>
      <td>Commercial associate</td>
      <td>Secondary / secondary special</td>
      <td>Single / not married</td>
      <td>House / apartment</td>
      <td>-19110</td>
      <td>-3051</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>Sales staff</td>
      <td>1.0</td>
      <td>-4.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5008809</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>0</td>
      <td>270000.0</td>
      <td>Commercial associate</td>
      <td>Secondary / secondary special</td>
      <td>Single / not married</td>
      <td>House / apartment</td>
      <td>-19110</td>
      <td>-3051</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>Sales staff</td>
      <td>1.0</td>
      <td>-26.0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>Now let’s see the data types of each of the features with the <code class="language-plaintext highlighter-rouge">info()</code> function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cc_data_full_data</span><span class="p">.</span><span class="n">info</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 36457 entries, 0 to 36456
Data columns (total 20 columns):
 #   Column               Non-Null Count  Dtype  
---  ------               --------------  -----  
 0   ID                   36457 non-null  int64  
 1   Gender               36457 non-null  object 
 2   Has a car            36457 non-null  object 
 3   Has a property       36457 non-null  object 
 4   Children count       36457 non-null  int64  
 5   Income               36457 non-null  float64
 6   Employment status    36457 non-null  object 
 7   Education level      36457 non-null  object 
 8   Marital status       36457 non-null  object 
 9   Dwelling             36457 non-null  object 
 10  Age                  36457 non-null  int64  
 11  Employment length    36457 non-null  int64  
 12  Has a mobile phone   36457 non-null  int64  
 13  Has a work phone     36457 non-null  int64  
 14  Has a phone          36457 non-null  int64  
 15  Has an email         36457 non-null  int64  
 16  Job title            25134 non-null  object 
 17  Family member count  36457 non-null  float64
 18  Account age          36457 non-null  float64
 19  Is high risk         36457 non-null  object 
dtypes: float64(3), int64(8), object(9)
memory usage: 5.8+ MB
</code></pre></div></div>

<p>Let’s digest the information above. The first column is the indexes of the features; the second is the names; the third is the count of non-null values(only the job title has missing values); and the fourth is datatypes (objects which mean strings datatype, float or integer).</p>

<p>The <code class="language-plaintext highlighter-rouge">describe()</code> function gives us statistics about the numerical features in the dataset. These statistics include each numerical feature’s count, mean, standard deviation, interquartile range(25%, 50%, 75%), and minimum and maximum values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cc_data_full_data</span><span class="p">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>Children count</th>
      <th>Income</th>
      <th>Age</th>
      <th>Employment length</th>
      <th>Has a mobile phone</th>
      <th>Has a work phone</th>
      <th>Has a phone</th>
      <th>Has an email</th>
      <th>Family member count</th>
      <th>Account age</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>3.645700e+04</td>
      <td>36457.000000</td>
      <td>3.645700e+04</td>
      <td>36457.000000</td>
      <td>36457.000000</td>
      <td>36457.0</td>
      <td>36457.000000</td>
      <td>36457.000000</td>
      <td>36457.000000</td>
      <td>36457.000000</td>
      <td>36457.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>5.078227e+06</td>
      <td>0.430315</td>
      <td>1.866857e+05</td>
      <td>-15975.173382</td>
      <td>59262.935568</td>
      <td>1.0</td>
      <td>0.225526</td>
      <td>0.294813</td>
      <td>0.089722</td>
      <td>2.198453</td>
      <td>-26.164193</td>
    </tr>
    <tr>
      <th>std</th>
      <td>4.187524e+04</td>
      <td>0.742367</td>
      <td>1.017892e+05</td>
      <td>4200.549944</td>
      <td>137651.334859</td>
      <td>0.0</td>
      <td>0.417934</td>
      <td>0.455965</td>
      <td>0.285787</td>
      <td>0.911686</td>
      <td>16.501854</td>
    </tr>
    <tr>
      <th>min</th>
      <td>5.008804e+06</td>
      <td>0.000000</td>
      <td>2.700000e+04</td>
      <td>-25152.000000</td>
      <td>-15713.000000</td>
      <td>1.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>-60.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>5.042028e+06</td>
      <td>0.000000</td>
      <td>1.215000e+05</td>
      <td>-19438.000000</td>
      <td>-3153.000000</td>
      <td>1.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>-39.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>5.074614e+06</td>
      <td>0.000000</td>
      <td>1.575000e+05</td>
      <td>-15563.000000</td>
      <td>-1552.000000</td>
      <td>1.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>-24.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>5.115396e+06</td>
      <td>1.000000</td>
      <td>2.250000e+05</td>
      <td>-12462.000000</td>
      <td>-408.000000</td>
      <td>1.0</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>-12.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>5.150487e+06</td>
      <td>19.000000</td>
      <td>1.575000e+06</td>
      <td>-7489.000000</td>
      <td>365243.000000</td>
      <td>1.0</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>20.000000</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>

<p>We will use the <a href="https://github.com/ResidentMario/missingno">Missingno</a> to visualize the missing values per feature using its <code class="language-plaintext highlighter-rouge">matrix</code> function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msno</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">cc_data_full_data</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_52_0.png" alt="png" /></p>

<p>Here we can see that the Job title is the only feature with missing values. Slim white lines represent missing values.</p>

<p>To see a clear representation of the missing values count, we can use its <code class="language-plaintext highlighter-rouge">bar()</code> function to have a barplot with the count of non-null values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">msno</span><span class="p">.</span><span class="n">bar</span><span class="p">(</span><span class="n">cc_data_full_data</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_55_0.png" alt="png" /></p>

<p>Now we will create functions to analyze each feature(Univariate analysis). Don’t worry too much about understanding these functions, as we will see how they are used during the exploratory data analysis section.</p>

<p>Our first function <code class="language-plaintext highlighter-rouge">value_cnt_norm_cal</code> is used to calculate the count of each class in a feature with its frequency (normalized on a scale of 100)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">value_cnt_norm_cal</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">):</span>
    <span class="s">'''Function that will return the value count and frequency of each observation within a feature'''</span>
    <span class="c1"># get the value counts of each feature
</span>    <span class="n">ftr_value_cnt</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">].</span><span class="n">value_counts</span><span class="p">()</span>
    <span class="c1"># normalize the value counts on a scale of 100
</span>    <span class="n">ftr_value_cnt_norm</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">].</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="c1"># concatenate the value counts with normalized value count column wise
</span>    <span class="n">ftr_value_cnt_concat</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">ftr_value_cnt</span><span class="p">,</span> <span class="n">ftr_value_cnt_norm</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># give it a column name
</span>    <span class="n">ftr_value_cnt_concat</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Count'</span><span class="p">,</span> <span class="s">'Frequency (%)'</span><span class="p">]</span>
    <span class="c1"># return the dataframe
</span>    <span class="k">return</span> <span class="n">ftr_value_cnt_concat</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">gen_info_feat</code> returned the description, the datatype, statistics, the value counts and frequencies</p>

<p>Note: I have used the if statement to handle features differently depending on their data type and characteristics. For example, I divided age by 365.25 and changed it to a positive value because it is expressed in days instead of years. Same as employment length; however, we did not print the value count for account age.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gen_info_feat</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">):</span>
    <span class="s">'''function to display general information about the feature'''</span>
    <span class="c1"># if the feature is Age
</span>    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Age'</span><span class="p">:</span>
        <span class="c1"># change the feature to be expressed in positive numbers of days and divide by 365.25 to be expressed in years and get the description
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'Description:</span><span class="se">\n</span><span class="s">{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span><span class="o">/</span><span class="mf">365.25</span><span class="p">).</span><span class="n">describe</span><span class="p">()))</span>
        <span class="c1"># print separators
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'*'</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
        <span class="c1"># print the datatype
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'Object type:{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">].</span><span class="n">dtype</span><span class="p">))</span>
    <span class="c1"># if the feature is employment length
</span>    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Employment length'</span><span class="p">:</span>
        <span class="c1"># select only the rows where the rows are negative values to ignore those who have retired or are unemployed
</span>        <span class="n">employment_len_no_ret</span> <span class="o">=</span> <span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">][</span><span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span>
        <span class="c1"># change the negative values to positive values
</span>        <span class="n">employment_len_no_ret_yrs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">employment_len_no_ret</span><span class="p">)</span><span class="o">/</span><span class="mf">365.25</span>
        <span class="c1"># print the descriptions
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'Description:</span><span class="se">\n</span><span class="s">{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">((</span><span class="n">employment_len_no_ret_yrs</span><span class="p">).</span><span class="n">describe</span><span class="p">()))</span>
        <span class="c1"># print separators
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'*'</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
        <span class="c1"># print the datatype
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'Object type:{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">employment_len_no_ret</span><span class="p">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="c1"># if the feature is account age
</span>    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Account age'</span> <span class="ow">or</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Income'</span><span class="p">:</span>
        <span class="c1"># change the account age to a positive number of months and get the description
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'Description:</span><span class="se">\n</span><span class="s">{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">])).</span><span class="n">describe</span><span class="p">()))</span>
        <span class="c1"># print separators
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'*'</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
        <span class="c1"># print the datatype
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'Object type:{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">].</span><span class="n">dtype</span><span class="p">))</span>
    <span class="c1"># if it is any other feature
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># get the description
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'Description:</span><span class="se">\n</span><span class="s">{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">].</span><span class="n">describe</span><span class="p">()))</span>
        <span class="c1"># print separators
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'*'</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
        <span class="c1"># print the datatype
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'Object type:</span><span class="se">\n</span><span class="s">{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">].</span><span class="n">dtype</span><span class="p">))</span>
        <span class="c1"># print separators
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'*'</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
        <span class="c1"># calling the value_cnt_norm_cal function previously seen
</span>        <span class="n">value_cnt</span> <span class="o">=</span> <span class="n">value_cnt_norm_cal</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">)</span>
        <span class="c1"># print the result
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'Value count:</span><span class="se">\n</span><span class="s">{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">value_cnt</span><span class="p">))</span>

</code></pre></div></div>

<p>The following function prints a pie chart.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_pie_plot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">):</span>
    <span class="s">'''function to create a pie chart plot'''</span>
    <span class="c1"># if the feature is dwelling or education level
</span>    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Dwelling'</span> <span class="ow">or</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Education level'</span><span class="p">:</span>
        <span class="c1"># calling the value_cnt_norm_cal function previously seen
</span>        <span class="n">ratio_size</span> <span class="o">=</span> <span class="n">value_cnt_norm_cal</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">feature</span><span class="p">)</span>
        <span class="c1"># get how many classes we have
</span>        <span class="n">ratio_size_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ratio_size</span><span class="p">.</span><span class="n">index</span><span class="p">)</span>
        <span class="n">ratio_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># loop till the max range
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ratio_size_len</span><span class="p">):</span>
            <span class="c1">#append the ratio of each feature to the list
</span>            <span class="n">ratio_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ratio_size</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s">'Frequency (%)'</span><span class="p">])</span>
        <span class="c1"># create a subplot
</span>        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="c1"># %1.2f%% display decimals in the pie chart with 2 decimal places
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">pie</span><span class="p">(</span><span class="n">ratio_list</span><span class="p">,</span> <span class="n">startangle</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">wedgeprops</span><span class="o">=</span><span class="p">{</span><span class="s">'edgecolor'</span> <span class="p">:</span><span class="s">'black'</span><span class="p">})</span>
        <span class="c1"># add a title to the chart
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Pie chart of {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="c1"># add a legend to the chart
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'best'</span><span class="p">,</span><span class="n">labels</span><span class="o">=</span><span class="n">ratio_size</span><span class="p">.</span><span class="n">index</span><span class="p">)</span>
        <span class="c1"># center the plot in the subplot
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>

        <span class="c1"># return the plot
</span>        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="c1"># for other features
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="n">ratio_size</span> <span class="o">=</span> <span class="n">value_cnt_norm_cal</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">feature</span><span class="p">)</span>
        <span class="n">ratio_size_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ratio_size</span><span class="p">.</span><span class="n">index</span><span class="p">)</span>
        <span class="n">ratio_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ratio_size_len</span><span class="p">):</span>
            <span class="n">ratio_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ratio_size</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s">'Frequency (%)'</span><span class="p">])</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="c1"># %1.2f%% display decimals in the pie chart with 2 decimal places
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">pie</span><span class="p">(</span><span class="n">ratio_list</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">ratio_size</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">autopct</span><span class="o">=</span><span class="s">'%1.2f%%'</span><span class="p">,</span> <span class="n">startangle</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">wedgeprops</span><span class="o">=</span><span class="p">{</span><span class="s">'edgecolor'</span> <span class="p">:</span><span class="s">'black'</span><span class="p">})</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Pie chart of {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'best'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>The next function create a bar plot.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_bar_plot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">):</span>
    <span class="s">'''function to create a bar chart plot'''</span>
    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Marital status'</span> <span class="ow">or</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Dwelling'</span> <span class="ow">or</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Job title'</span> <span class="ow">or</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Employment status'</span> <span class="ow">or</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Education level'</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
        <span class="c1"># create a barplot using seaborn with X-axis the indexes from value_cnt_norm_cal function and Y axis we use the value counts from the same function
</span>        <span class="n">sns</span><span class="p">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">value_cnt_norm_cal</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">).</span><span class="n">index</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">value_cnt_norm_cal</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">).</span><span class="n">values</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
        <span class="c1"># set the plot's tick labels to the index from the value_cnt_norm_cal function, rotate those ticks by 45 degrees
</span>        <span class="n">ax</span><span class="p">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">value_cnt_norm_cal</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">).</span><span class="n">index</span><span class="p">,</span><span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s">'right'</span><span class="p">)</span>
        <span class="c1"># Give the X-axis the same label as the feature name
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="c1"># Give the Y-axis the label "Count"
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Count'</span><span class="p">)</span>
        <span class="c1"># Give the plot a title
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'{} count'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="c1"># Return the title
</span>        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">value_cnt_norm_cal</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">).</span><span class="n">index</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">value_cnt_norm_cal</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">).</span><span class="n">values</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Count'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'{} count'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>This function will create a box plot for continuous variables.</p>

<p>Note: Depending on which transformation needs to be done on each feature, we have used a switch statement to handle the different feature that requires different handling.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_box_plot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">):</span>
    <span class="s">'''function to create a box plot'''</span>
    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Age'</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="c1"># change the feature to be expressed in positive numbers days
</span>        <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span><span class="o">/</span><span class="mf">365.25</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'{} distribution(Boxplot)'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Children count'</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'{} distribution(Boxplot)'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="c1"># use the numpy arrange to populate the Y ticks starting from 0 till the max count of children with an interval of 1 as follows np.arange(start, stop, step)
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">].</span><span class="nb">max</span><span class="p">(),</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Employment length'</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="n">employment_len_no_ret</span> <span class="o">=</span> <span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">][</span><span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span>
        <span class="c1"># employment length in days is a negative number, so we need to change it to positive and change it to years
</span>        <span class="n">employment_len_no_ret_yrs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">employment_len_no_ret</span><span class="p">)</span><span class="o">/</span><span class="mf">365.25</span>
        <span class="c1"># create a boxplot with seaborn
</span>        <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">employment_len_no_ret_yrs</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'{} distribution(Boxplot)'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">employment_len_no_ret_yrs</span><span class="p">.</span><span class="nb">max</span><span class="p">(),</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Income'</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'{} distribution(Boxplot)'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="c1"># suppress the scientific notation
</span>        <span class="n">ax</span><span class="p">.</span><span class="n">get_yaxis</span><span class="p">().</span><span class="n">set_major_formatter</span><span class="p">(</span>
            <span class="n">matplotlib</span><span class="p">.</span><span class="n">ticker</span><span class="p">.</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">format</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">','</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Account age'</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">]))</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'{} distribution(Boxplot)'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'{} distribution(Boxplot)'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>This function will plot a histogram.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_hist_plot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">,</span> <span class="n">the_bins</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="s">'''function to create a histogram plot'''</span>
    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Age'</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
        <span class="c1"># change the feature to be expressed in positive numbers days
</span>        <span class="n">sns</span><span class="p">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span><span class="o">/</span><span class="mf">365.25</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">the_bins</span><span class="p">,</span><span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'{} distribution'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Income'</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">],</span><span class="n">bins</span><span class="o">=</span><span class="n">the_bins</span><span class="p">,</span><span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># suppress scientific notation
</span>        <span class="n">ax</span><span class="p">.</span><span class="n">get_xaxis</span><span class="p">().</span><span class="n">set_major_formatter</span><span class="p">(</span>
            <span class="n">matplotlib</span><span class="p">.</span><span class="n">ticker</span><span class="p">.</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">format</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">','</span><span class="p">)))</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'{} distribution'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Employment length'</span><span class="p">:</span>
        <span class="n">employment_len_no_ret</span> <span class="o">=</span> <span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">][</span><span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span>
        <span class="c1"># change the feature to be expressed in positive numbers days
</span>        <span class="n">employment_len_no_ret_yrs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">employment_len_no_ret</span><span class="p">)</span><span class="o">/</span><span class="mf">365.25</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">employment_len_no_ret_yrs</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">the_bins</span><span class="p">,</span><span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'{} distribution'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Account age'</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">]),</span><span class="n">bins</span><span class="o">=</span><span class="n">the_bins</span><span class="p">,</span><span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'{} distribution'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">],</span><span class="n">bins</span><span class="o">=</span><span class="n">the_bins</span><span class="p">,</span><span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'{} distribution'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>This function will plot two box plots, one is for low-risk (good client), and the other is for high-risk (bad client) applicants. On the Y axis, we have the continuous features we are studying. Again don’t worry too much, as we will see these functions in action in the sections below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">low_high_risk_box_plot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">):</span>
    <span class="s">'''High risk vs low risk applicants compared on a box plot'''</span>
    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Age'</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'Is high risk'</span><span class="p">)[</span><span class="n">feature</span><span class="p">].</span><span class="n">mean</span><span class="p">()</span><span class="o">/</span><span class="mf">365.25</span><span class="p">))</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="c1"># Place on the Y-axis age and X-axis the two box plot (is high risk: No and Yes)
</span>        <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">])</span><span class="o">/</span><span class="mf">365.25</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">])</span>
        <span class="c1"># add ticks to the X axis
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s">'no'</span><span class="p">,</span><span class="s">'yes'</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'High risk individuals grouped by age'</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Income'</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'Is high risk'</span><span class="p">)[</span><span class="n">feature</span><span class="p">].</span><span class="n">mean</span><span class="p">()))</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">]),</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s">'no'</span><span class="p">,</span><span class="s">'yes'</span><span class="p">])</span>
        <span class="c1"># suppress the scientific notation
</span>        <span class="n">ax</span><span class="p">.</span><span class="n">get_yaxis</span><span class="p">().</span><span class="n">set_major_formatter</span><span class="p">(</span>
            <span class="n">matplotlib</span><span class="p">.</span><span class="n">ticker</span><span class="p">.</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">format</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">','</span><span class="p">)))</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'High risk individuals grouped by {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">feature</span> <span class="o">==</span> <span class="s">'Employment length'</span><span class="p">:</span>
        <span class="c1"># checking is an applicant is high risk or not (for those who have negative employment length mean only those who are employed)
</span>        <span class="n">employment_no_ret</span> <span class="o">=</span> <span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">][</span><span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">]</span> <span class="o">&lt;</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">employment_no_ret_idx</span> <span class="o">=</span> <span class="n">employment_no_ret</span><span class="p">.</span><span class="n">index</span>
        <span class="n">employment_len_no_ret_yrs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">employment_no_ret</span><span class="p">)</span><span class="o">/</span><span class="mf">365.25</span>
        <span class="c1"># extract those who are employed from the original dataframe and return only the employment length and Is high risk columns
</span>        <span class="n">employment_no_ret_df</span> <span class="o">=</span> <span class="n">cc_train_copy</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">employment_no_ret_idx</span><span class="p">][[</span><span class="s">'Employment length'</span><span class="p">,</span><span class="s">'Is high risk'</span><span class="p">]]</span>
        <span class="c1"># return the mean employment length group by how risky is the applicant
</span>        <span class="n">employment_no_ret_is_high_risk</span> <span class="o">=</span> <span class="n">employment_no_ret_df</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'Is high risk'</span><span class="p">)[</span><span class="s">'Employment length'</span><span class="p">].</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">employment_no_ret_is_high_risk</span><span class="p">)</span><span class="o">/</span><span class="mf">365.25</span><span class="p">)</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">employment_len_no_ret_yrs</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s">'no'</span><span class="p">,</span><span class="s">'yes'</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'High vs low risk individuals grouped by {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'Is high risk'</span><span class="p">)[</span><span class="n">feature</span><span class="p">].</span><span class="n">mean</span><span class="p">()))</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">]),</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s">'no'</span><span class="p">,</span><span class="s">'yes'</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'High risk individuals grouped by {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>This function is similar to the previous one; the only difference is that it uses a bar plot which is a count of classes for comparison purposes between high risk and low risk.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">low_high_risk_bar_plot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">feature</span><span class="p">):</span>
    <span class="s">'''High risk vs low risk applicants compared on a bar plot'''</span>
    <span class="c1"># get the sum of high-risk clients grouped by a specific feature
</span>    <span class="n">is_high_risk_grp</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">feature</span><span class="p">)[</span><span class="s">'Is high risk'</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span>
    <span class="c1"># sort is a descending order
</span>    <span class="n">is_high_risk_grp_srt</span> <span class="o">=</span> <span class="n">is_high_risk_grp</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">is_high_risk_grp_srt</span><span class="p">))</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="c1"># plot on the X axis the indexes which correspond to classes, and on the Y axis, the count
</span>    <span class="n">sns</span><span class="p">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">is_high_risk_grp_srt</span><span class="p">.</span><span class="n">index</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">is_high_risk_grp_srt</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>
    <span class="c1"># add the labels to the plot
</span>    <span class="n">ax</span><span class="p">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">is_high_risk_grp_srt</span><span class="p">.</span><span class="n">index</span><span class="p">,</span><span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s">'right'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Count'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'High risk applicants count grouped by {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Now let’s properly start our exploratory data analysis with a univariate analysis. Univariate analysis is an analysis of each feature individually in the dataset.</p>

<h3>Univariate analysis</h3>

<h3>Gender</h3>

<p>We start with <code class="language-plaintext highlighter-rouge">Gender</code>. We call <code class="language-plaintext highlighter-rouge">gen_info_feat</code> and see that we have two unique classes <code class="language-plaintext highlighter-rouge">F</code> (for female) and <code class="language-plaintext highlighter-rouge">M</code> (for male), with 19549 and 9616 occurrences, respectively. Percentage-wise we have 67.02% females and 32.97% males.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Gender'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count     29165
unique        2
top           F
freq      19549
Name: Gender, dtype: object
**************************************************
Object type:
object
**************************************************
Value count:
   Count  Frequency (%)
F  19549      67.028973
M   9616      32.971027
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Gender'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_78_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_pie_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Gender'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_79_0.png" alt="png" /></p>

<h3>Age</h3>

<p>Now let’s look at <code class="language-plaintext highlighter-rouge">Age</code>; since age is a continuous variable, we will process it differently than <code class="language-plaintext highlighter-rouge">Gender</code>. Using the <code class="language-plaintext highlighter-rouge">gen_info_feat</code> function, we look at the mean, standard deviation, minimum, maximum and interquartile range. Then we plot that information on a box plot by calling the <code class="language-plaintext highlighter-rouge">create_box_plot</code> function. With that, we can see that the youngest applicant(s) is 21 years old while the oldest is 68. With an average of 43.7 and a median of 42.6 (outliers insensitive)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Age'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count    29165.000000
mean        43.749425
std         11.507180
min         21.095140
25%         34.154689
50%         42.614648
75%         53.234771
max         68.862423
Name: Age, dtype: float64
**************************************************
Object type:int64
Description:
count    29165.000000
mean    -15979.477490
std       4202.997485
min     -25152.000000
25%     -19444.000000
50%     -15565.000000
75%     -12475.000000
max      -7705.000000
Name: Age, dtype: float64
**************************************************
Object type:
int64
**************************************************
Value count:
        Count  Frequency (%)
-12676     44       0.150866
-15519     44       0.150866
-16896     33       0.113149
-16053     26       0.089148
-16768     26       0.089148
...       ...            ...
-18253      1       0.003429
-23429      1       0.003429
-15478      1       0.003429
-21648      1       0.003429
-19564      1       0.003429

[6794 rows x 2 columns]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_box_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Age'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_83_0.png" alt="png" /></p>

<p>After that, we plot its histogram with the kernel density estimator. ``` Age `` is not normally distributed; it is slightly positively skewed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_hist_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Age'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_85_0.png" alt="png" /></p>

<p>Now we perform a quick bivariate analysis (comparison of two features) of <code class="language-plaintext highlighter-rouge">Age</code> and the target variable <code class="language-plaintext highlighter-rouge">Is high risk</code>. The blue box plot represents a good client (is high risk = No), and the green box plot represents a bad client (is high risk = Yes). We can see no significant difference between the age of those who are high risk and those who are not. The mean age for both groups is around 43 years old, and there is no correlation between the age and risk factors of the applicant.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">low_high_risk_box_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Age'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Is high risk
0    43.753103
1    43.538148
Name: Age, dtype: float64
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_87_1.png" alt="png" /></p>

<h3>Marital status</h3>

<p>There are 5 unique classes for this feature. Married constitutes the most significant proportion of marital status, with 68% far ahead of single, as seen on the pie chart and bar charts. Another interesting observation is that even though we have a higher number of applicants who are separated than widows, it seems that widow applicants are bad clients than those who are separated by a small margin.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Marital status'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count       29165
unique          5
top       Married
freq        20044
Name: Marital status, dtype: object
**************************************************
Object type:
object
**************************************************
Value count:
                      Count  Frequency (%)
Married               20044      68.726213
Single / not married   3864      13.248757
Civil marriage         2312       7.927310
Separated              1712       5.870050
Widow                  1233       4.227670
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_pie_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Marital status'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_91_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Marital status'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_92_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">low_high_risk_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Marital status'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'Married': 320, 'Single / not married': 87, 'Civil marriage': 34, 'Widow': 34, 'Separated': 24}
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_93_1.png" alt="png" /></p>

<h3>Family member count</h3>

<p>Family member count is a numerical feature, with the median of 2 family members representing 53% (count = 15552) of all the counts, followed by a single family member with 19% (count = 5613). Looking at the box plot, we have 6 outliers; 2 are extreme, with 20 and 15 members in their household.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Family member count'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count    29165.000000
mean         2.197531
std          0.912189
min          1.000000
25%          2.000000
50%          2.000000
75%          3.000000
max         20.000000
Name: Family member count, dtype: float64
**************************************************
Object type:
float64
**************************************************
Value count:
      Count  Frequency (%)
2.0   15552      53.324190
1.0    5613      19.245671
3.0    5121      17.558718
4.0    2503       8.582205
5.0     309       1.059489
6.0      48       0.164581
7.0      14       0.048003
9.0       2       0.006858
15.0      2       0.006858
20.0      1       0.003429
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_box_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Family member count'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_97_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Family member count'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_98_0.png" alt="png" /></p>

<h3>Children count</h3>

<p>From the chart below, we can see that most applicants don’t have any children. Again, we have 6 outliers, most probably the same seen from the family member count.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Children count'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count    29165.000000
mean         0.430790
std          0.741882
min          0.000000
25%          0.000000
50%          0.000000
75%          1.000000
max         19.000000
Name: Children count, dtype: float64
**************************************************
Object type:
int64
**************************************************
Value count:
    Count  Frequency (%)
0   20143      69.065661
1    6003      20.582890
2    2624       8.997086
3     323       1.107492
4      52       0.178296
5      15       0.051432
7       2       0.006858
14      2       0.006858
19      1       0.003429
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_box_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Children count'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_102_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Children count'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_103_0.png" alt="png" /></p>

<h3>Dwelling type</h3>

<p>89% of applicants live in houses/apartments by a substantial margin.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Dwelling'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count                 29165
unique                    6
top       House / apartment
freq                  26059
Name: Dwelling, dtype: object
**************************************************
Object type:
object
**************************************************
Value count:
                     Count  Frequency (%)
House / apartment    26059      89.350249
With parents          1406       4.820847
Municipal apartment    912       3.127036
Rented apartment       453       1.553232
Office apartment       208       0.713184
Co-op apartment        127       0.435453
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_pie_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Dwelling'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_107_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Dwelling'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_108_0.png" alt="png" /></p>

<h3>Income</h3>

<p>Looking at the results from the <code class="language-plaintext highlighter-rouge">gen_info_feat</code> function, we can see that the average mean income is 186890, but this amount factors in outliers. Most people make 157500 (median income) if we ignore the outliers. We have 3 applicants who make more than 1000000.</p>

<p>This feature is also positively skewed. Focusing on the income box plot of good and bad clients, they all have roughly similar incomes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd</span><span class="p">.</span><span class="n">set_option</span><span class="p">(</span><span class="s">'display.float_format'</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s">'%.2f'</span> <span class="o">%</span> <span class="n">x</span><span class="p">)</span>
<span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Income'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count     29165.00
mean     186890.39
std      101409.64
min       27000.00
25%      121500.00
50%      157500.00
75%      225000.00
max     1575000.00
Name: Income, dtype: float64
**************************************************
Object type:float64
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_box_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Income'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_112_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_hist_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Income'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_113_0.png" alt="png" /></p>

<ul>
  <li>bivariate analysis with target variable</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">low_high_risk_box_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Income'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Is high risk
0   186913.94
1   185537.26
Name: Income, dtype: float64
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_115_1.png" alt="png" /></p>

<h3>Job title</h3>

<p>The most common Job title is laborers by a large margin (24.85%), followed by core staff (14.23%), sales staff (13.77%) and managers (12.03%). We also have 30.95% of missing data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Job title'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count        20138
unique          18
top       Laborers
freq          5004
Name: Job title, dtype: object
**************************************************
Object type:
object
**************************************************
Value count:
                       Count  Frequency (%)
Laborers                5004          24.85
Core staff              2866          14.23
Sales staff             2773          13.77
Managers                2422          12.03
Drivers                 1722           8.55
High skill tech staff   1133           5.63
Accountants              998           4.96
Medicine staff           956           4.75
Cooking staff            521           2.59
Security staff           464           2.30
Cleaning staff           425           2.11
Private service staff    287           1.43
Low-skill Laborers       138           0.69
Waiters/barmen staff     127           0.63
Secretaries              122           0.61
HR staff                  72           0.36
Realty agents             60           0.30
IT staff                  48           0.24
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">job_title_nan_count</span> <span class="o">=</span> <span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Job title'</span><span class="p">].</span><span class="n">isna</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span>
<span class="n">job_title_nan_count</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9027
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rows_total_count</span> <span class="o">=</span> <span class="n">cc_train_copy</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'The percentage of missing rows is {:.2f} %'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">job_title_nan_count</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">/</span> <span class="n">rows_total_count</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The percentage of missing rows is 30.95 %
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Job title'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_122_0.png" alt="png" /></p>

<h3>Employment status</h3>

<p>Most applicants are working (51.62%); the next most represented status is commercial associate, followed by the pensioner.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Employment status'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count       29165
unique          5
top       Working
freq        15056
Name: Employment status, dtype: object
**************************************************
Object type:
object
**************************************************
Value count:
                      Count  Frequency (%)
Working               15056          51.62
Commercial associate   6801          23.32
Pensioner              4920          16.87
State servant          2381           8.16
Student                   7           0.02
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Employment status'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_126_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_pie_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Employment status'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_127_0.png" alt="png" /></p>

<h3>Education level</h3>

<p>Most applicants have completed their secondary degree (67.90%) ¼ completed their higher education.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Education level'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count                             29165
unique                                5
top       Secondary / secondary special
freq                              19803
Name: Education level, dtype: object
**************************************************
Object type:
object
**************************************************
Value count:
                               Count  Frequency (%)
Secondary / secondary special  19803          67.90
Higher education                7910          27.12
Incomplete higher               1129           3.87
Lower secondary                  298           1.02
Academic degree                   25           0.09
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_pie_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Education level'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_131_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Education level'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_132_0.png" alt="png" /></p>

<h3>Employment length</h3>

<p>Most applicants have been working between 5 to 7 years on average, and we also have many outliers who have been working for more than 20 years+. The employment length histogram is positively skewed. Finally, bad clients have a low employment length of 5 versus 7 years for good clients.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Employment length'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count   24257.00
mean        7.26
std         6.46
min         0.05
25%         2.68
50%         5.45
75%         9.60
max        43.02
Name: Employment length, dtype: float64
**************************************************
Object type:int64
Description:
count    29165.00
mean     59257.76
std     137655.88
min     -15713.00
25%      -3153.00
50%      -1557.00
75%       -412.00
max     365243.00
Name: Employment length, dtype: float64
**************************************************
Object type:
int64
**************************************************
Value count:
         Count  Frequency (%)
 365243   4908          16.83
-401        61           0.21
-200        55           0.19
-2087       53           0.18
-1539       51           0.17
...        ...            ...
-8369        1           0.00
-6288        1           0.00
-6303        1           0.00
-3065        1           0.00
-8256        1           0.00

[3483 rows x 2 columns]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_box_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Employment length'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_136_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_hist_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Employment length'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_137_0.png" alt="png" /></p>

<ul>
  <li>bivariate analysis with target variable</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># distribution of employment length for good vs bad client
# Here 0 means No and 1 means Yes
</span><span class="n">low_high_risk_box_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Employment length'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Is high risk
0   7.29
1   5.75
Name: Employment length, dtype: float64
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_139_1.png" alt="png" /></p>

<h3>Has a car</h3>

<p>Most applicants don’t own a car (62% of applicants).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has a car'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count     29165
unique        2
top           N
freq      18128
Name: Has a car, dtype: object
**************************************************
Object type:
object
**************************************************
Value count:
   Count  Frequency (%)
N  18128          62.16
Y  11037          37.84
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has a car'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_143_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_pie_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has a car'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_144_0.png" alt="png" /></p>

<h3>Has a property</h3>

<p>Most applicants own a property (67% of applicants)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has a property'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count     29165
unique        2
top           Y
freq      19557
Name: Has a property, dtype: object
**************************************************
Object type:
object
**************************************************
Value count:
   Count  Frequency (%)
Y  19557          67.06
N   9608          32.94
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has a property'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_148_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_pie_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has a property'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_149_0.png" alt="png" /></p>

<h3>Has a work phone</h3>

<p>More than ¾ of applicants don’t have a work phone</p>

<p>Note: Here, 0 represent no and 1 represents yes</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has a work phone'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count   29165.00
mean        0.22
std         0.42
min         0.00
25%         0.00
50%         0.00
75%         0.00
max         1.00
Name: Has a work phone, dtype: float64
**************************************************
Object type:
int64
**************************************************
Value count:
   Count  Frequency (%)
0  22623          77.57
1   6542          22.43
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has a work phone'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_153_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_pie_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has a work phone'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_154_0.png" alt="png" /></p>

<h3>Has a mobile phone</h3>

<p>All the applicants, without exception, have a mobile phone.</p>

<p>Note: Here, 0 is no and 1 is yes</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has a mobile phone'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count   29165.00
mean        1.00
std         0.00
min         1.00
25%         1.00
50%         1.00
75%         1.00
max         1.00
Name: Has a mobile phone, dtype: float64
**************************************************
Object type:
int64
**************************************************
Value count:
   Count  Frequency (%)
1  29165         100.00
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_pie_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has a mobile phone'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_158_0.png" alt="png" /></p>

<h3>Has a phone</h3>

<p>70% of applicants don’t have a phone (probably a home phone)</p>

<p>Note: Here, 0 is no and 1 is yes</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has a phone'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count   29165.00
mean        0.29
std         0.46
min         0.00
25%         0.00
50%         0.00
75%         1.00
max         1.00
Name: Has a phone, dtype: float64
**************************************************
Object type:
int64
**************************************************
Value count:
   Count  Frequency (%)
0  20562          70.50
1   8603          29.50
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has a phone'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_162_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_pie_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has a phone'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_163_0.png" alt="png" /></p>

<h3>Has an email</h3>

<p>Interestingly, more than 90 % of applicants don’t have an email</p>

<p>Note: Here, 0 is no and 1 is yes</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has an email'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count   29165.00
mean        0.09
std         0.29
min         0.00
25%         0.00
50%         0.00
75%         0.00
max         1.00
Name: Has an email, dtype: float64
**************************************************
Object type:
int64
**************************************************
Value count:
   Count  Frequency (%)
0  26532          90.97
1   2633           9.03
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has an email'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_167_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_pie_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Has an email'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_168_0.png" alt="png" /></p>

<h3>Account age</h3>

<p>Most accounts are 26 months old. The account age feature is not normally distributed; it is positively skewed. Another observation is that, on average, bad clients’ accounts are 34 months old vs 26 months old for good clients’ accounts.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Account age'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count   29165.00
mean       26.14
std        16.49
min         0.00
25%        12.00
50%        24.00
75%        39.00
max        60.00
Name: Account age, dtype: float64
**************************************************
Object type:float64
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_box_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Account age'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_172_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_hist_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Account age'</span><span class="p">,</span> <span class="n">the_bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_173_0.png" alt="png" /></p>

<ul>
  <li>bivariate analysis with target variable</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">low_high_risk_box_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Account age'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Is high risk
0   26.00
1   34.04
Name: Account age, dtype: float64
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_175_1.png" alt="png" /></p>

<h3>Is high risk (target variable)</h3>

<p>Most applicants are good clients (98% of applicants). We have imbalanced data that needs to be balanced using SMOTE before training on a model.</p>

<p>Note: Here, 0 is no and 1 is yes</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gen_info_feat</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Is high risk'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Description:
count     29165
unique        2
top           0
freq      28666
Name: Is high risk, dtype: int64
**************************************************
Object type:
object
**************************************************
Value count:
   Count  Frequency (%)
0  28666          98.29
1    499           1.71
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_bar_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Is high risk'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_179_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_pie_plot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="s">'Is high risk'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_180_0.png" alt="png" /></p>

<h3>Bivariate analysis</h3>

<p>Now that we have finished our univariate analysis let’s look into the bivariate analysis. Bivariate analysis, as the name implies, is the analysis of two features compared with each other. First, we will do a bivariate analysis of numerical features.</p>

<p>Looking at the pairplot (scatter plots of pairwise relationships in a dataset), we can see a positive linear correlation between the family member and the children’s count. It makes sense; the more children someone has, the larger the family member count. It is a multicollinearity problem (two highly correlated features) which is not ideal for training a model. We will need to drop one of them.</p>

<p>Another trend is the Employment length and age. It also makes sense; the longer the employment length, the older someone is.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># drop categorical features, do a pairplot of the remaining feature numerical feature
</span><span class="n">sns</span><span class="p">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">[</span><span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">].</span><span class="n">drop</span><span class="p">([</span><span class="s">'ID'</span><span class="p">,</span><span class="s">'Has a mobile phone'</span><span class="p">,</span> <span class="s">'Has a work phone'</span><span class="p">,</span> <span class="s">'Has a phone'</span><span class="p">,</span> <span class="s">'Has an email'</span><span class="p">,</span><span class="s">'Is high risk'</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="n">corner</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_184_0.png" alt="png" /></p>

<p>Now let’s look at the two interesting scatter plots.</p>

<p>We will start with the family member count vs children count. Of course, the more children a person has, the larger the family count. We added a line of best fit, also called the regression line, and you can read more about it in this blog post <a href="https://semasuka.github.io/blog/2021/04/04/demystify-machine-learning.html">here</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'Children count'</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s">'Family member count'</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">cc_train_copy</span><span class="p">,</span><span class="n">line_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'color'</span><span class="p">:</span> <span class="s">'red'</span><span class="p">})</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_186_0.png" alt="png" /></p>

<p>When we compare the employment length and age, the scatterplot shows a trend between the age and the length of employment.</p>

<p>It is shaped like a reversed triangle because the applicants’ age increases with the employment length. You can’t have an employment length that is superior to the age. Right?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_age</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Age'</span><span class="p">])</span><span class="o">/</span><span class="mf">365.25</span>
<span class="n">x_employ_length</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span>
    <span class="n">cc_train_copy</span><span class="p">[</span><span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">][</span><span class="s">'Employment length'</span><span class="p">])</span><span class="o">/</span><span class="mf">365.25</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x_employ_length</span><span class="p">,</span> <span class="n">y_age</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="p">.</span><span class="mi">05</span><span class="p">)</span>
<span class="c1"># change the frequency of the x-axis and y-axis labels
</span><span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x_employ_length</span><span class="p">.</span><span class="nb">max</span><span class="p">(),</span> <span class="mf">2.5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">y_age</span><span class="p">.</span><span class="nb">max</span><span class="p">(),</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/Users/sternsemasuka/opt/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
  warnings.warn(
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_188_1.png" alt="png" /></p>

<p>Now comparing account age and applicant age, we can see that most applicants are between 20 and 45 years old and have an account less than 25 months old. This information is deduced from darker blue hexagons (high-density area) between 22 and 43 on the Y axis and between 3 and 28 on the X axis.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="n">jointplot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Account age'</span><span class="p">]),</span><span class="n">y_age</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s">"hex"</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">y_age</span><span class="p">.</span><span class="nb">max</span><span class="p">(),</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">65</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Age'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/Users/sternsemasuka/opt/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
  warnings.warn(
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_190_1.png" alt="png" /></p>

<h3>Heatmap</h3>

<p>Time to do a correlation between all the numerical features using a heatmap. This heatmap shows the correlation between all the numerical features; the darker the cell, the more correlated the two features are, and the lighter the color, the less correlated the two features.</p>

<p>No feature is correlated with the target feature (Which is high risk). We see a strong correlation (0.89) between family member count and children count, as previously seen with the pairplot (The more children a person has, the larger the family count). Age has some positive correlation (0.30) with the family member count and children count. The older a person is, the most likely they will have a larger family and consequently more children.</p>

<p>Another positive correlation (0.31) is having a phone and having a work phone. We have a slightly positive correlation between age and work phone(0.18); younger people will be less likely to own a work phone. As previously discussed, we also have a negative  (-0.62) between employment length and age.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># change the datatype of the target feature to int
</span><span class="n">is_high_risk_int</span> <span class="o">=</span> <span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="s">'int32'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># correlation analysis with heatmap, after dropping the has a mobile phone with the target feature as int
</span><span class="n">cc_train_copy_corr_no_mobile</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">cc_train_copy</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'Has a mobile phone'</span><span class="p">,</span><span class="s">'Is high risk'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="n">is_high_risk_int</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">corr</span><span class="p">()</span>
<span class="c1"># Get the lower triangle of the correlation matrix
# Generate a mask for the upper triangle
</span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">cc_train_copy_corr_no_mobile</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'bool'</span><span class="p">)</span>
<span class="n">mask</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">triu_indices_from</span><span class="p">(</span><span class="n">mask</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">True</span>
<span class="c1"># Set up the matplotlib figure
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="c1"># seaborn heatmap
</span><span class="n">sns</span><span class="p">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cc_train_copy_corr_no_mobile</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'flare'</span><span class="p">,</span><span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">)</span>
<span class="c1"># plot the heatmap
</span><span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_194_0.png" alt="png" /></p>

<h3>ANOVA</h3>

<p>Now, let’s do an ANOVA (analysis of variance) between age and other categorical features.</p>

<p>But before we proceed, what is an ANOVA? ANOVA tells you if there are any statistical differences between the means of two or more independent features (categorical features).</p>

<p>Now, let’s use box plots to compare age’s mean and different categorical features. Female applicants are older than their male counterparts, and those who don’t own a car with property owners tend to be older. Of course, the pensioners are older than those working (We also see that some have pensioned at a young age, those are outliers).</p>

<p>It is also interesting to see that those with an academic degree are generally younger than the other groups. The widows tend to be much older, with some young outliers in their 30s. Unsurprisingly, those who live with their parents tend to be younger, and we also see some outliers here. Lastly, those who work as cleaning staff tend to be older, while those who work in IT tend to be younger.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">180</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
<span class="n">cat_features</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Gender'</span><span class="p">,</span> <span class="s">'Has a car'</span><span class="p">,</span> <span class="s">'Has a property'</span><span class="p">,</span> <span class="s">'Employment status'</span><span class="p">,</span> <span class="s">'Education level'</span><span class="p">,</span> <span class="s">'Marital status'</span><span class="p">,</span> <span class="s">'Dwelling'</span><span class="p">,</span> <span class="s">'Job title'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">cat_ft_count</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">row_count</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">feat_count</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
            <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">row_count</span><span class="p">,</span><span class="n">feat_count</span><span class="p">],</span><span class="n">x</span><span class="o">=</span><span class="n">cc_train_copy</span><span class="p">[</span><span class="n">cat_features</span><span class="p">[</span><span class="n">cat_ft_count</span><span class="p">]],</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Age'</span><span class="p">])</span><span class="o">/</span><span class="mf">365.25</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">row_count</span><span class="p">,</span><span class="n">feat_count</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="n">cat_features</span><span class="p">[</span><span class="n">cat_ft_count</span><span class="p">]</span> <span class="o">+</span> <span class="s">" vs age"</span><span class="p">)</span>
            <span class="n">plt</span><span class="p">.</span><span class="n">sca</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="n">row_count</span><span class="p">,</span><span class="n">feat_count</span><span class="p">])</span>
            <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s">'right'</span><span class="p">)</span>
            <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Age'</span><span class="p">)</span>
            <span class="n">cat_ft_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">break</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_197_0.png" alt="png" /></p>

<p>Now let’s turn our attention to employment length versus categorical features. The only interesting observation is that state-employed and medical staff applicants tend to have been employed longer than the rest.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">180</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">cat_ft_count</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">row_count</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">feat_count</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
            <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">row_count</span><span class="p">,</span><span class="n">feat_count</span><span class="p">],</span><span class="n">x</span><span class="o">=</span><span class="n">cc_train_copy</span><span class="p">[</span><span class="n">cat_features</span><span class="p">[</span><span class="n">cat_ft_count</span><span class="p">]],</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">[</span><span class="n">cc_train_copy</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">][</span><span class="s">'Employment length'</span><span class="p">])</span><span class="o">/</span><span class="mf">365.25</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">row_count</span><span class="p">,</span><span class="n">feat_count</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="n">cat_features</span><span class="p">[</span><span class="n">cat_ft_count</span><span class="p">]</span> <span class="o">+</span> <span class="s">" vs employment length"</span><span class="p">)</span>
            <span class="n">plt</span><span class="p">.</span><span class="n">sca</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="n">row_count</span><span class="p">,</span><span class="n">feat_count</span><span class="p">])</span>
            <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Employment length'</span><span class="p">)</span>
            <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span><span class="n">ha</span><span class="o">=</span><span class="s">'right'</span><span class="p">)</span>
            <span class="n">cat_ft_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">break</span>
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_199_0.png" alt="png" /></p>

<h3>Applicant general profile</h3>

<p>After analyzing each feature, we can create a typical credit card applicant profile. Here is the profile:</p>

<ul>
  <li><strong><em>Typical profile of an applicant is a Female in her early 40’s, married with a partner and no child. She has been employed for five years with a salary of 157500. She has completed her secondary education. She does not own a car but owns a property (a house/ apartment). Her account is 26 months old.</em></strong></li>
  <li><strong><em>Age and income do not have any effects on the target variable</em></strong></li>
  <li><strong><em>Those flagged as bad clients tend to have a shorter employment length and older accounts. They also constitute less than 2% of total applicants.</em></strong></li>
  <li><strong><em>Most applicants are 20 to 45 years old and have an account that is 30 months old or less.</em></strong></li>
</ul>

<h3>3. Prepare the data</h3>

<p>Using EDA, here is a list of all the transformations that need to be done on each feature:</p>

<p>ID:</p>
<ul>
  <li>Drop the feature</li>
</ul>

<p>Gender:</p>
<ul>
  <li>One hot encoding</li>
</ul>

<p>Age:</p>
<ul>
  <li>Min-max scaling</li>
  <li>Fix skewness</li>
  <li>Absolute values and divide by 365.25</li>
</ul>

<p>Marital status:</p>
<ul>
  <li>One hot encoding</li>
</ul>

<p>Family member count</p>
<ul>
  <li>Fix outliers</li>
</ul>

<p>Children count</p>
<ul>
  <li>Fix outliers</li>
  <li>Drop feature</li>
</ul>

<p>Dwelling type</p>
<ul>
  <li>One hot encoding</li>
</ul>

<p>Income</p>
<ul>
  <li>Remove outliers</li>
  <li>Fix skewness</li>
  <li>Min-max scaling</li>
</ul>

<p>Job title</p>
<ul>
  <li>One hot encoding</li>
  <li>Impute missing values</li>
</ul>

<p>Employment status:</p>
<ul>
  <li>One hot encoding</li>
</ul>

<p>Education level:</p>
<ul>
  <li>Ordinal encoding</li>
</ul>

<p>Employment length:</p>
<ul>
  <li>Remove outliers</li>
  <li>Min-max scaling</li>
  <li>Absolute values and divide by 365.25</li>
  <li>change days of employment of retirees to 0</li>
</ul>

<p>Has a car:</p>
<ul>
  <li>Change it to numerical</li>
  <li>One-hot encoding</li>
</ul>

<p>Has a property:</p>
<ul>
  <li>Change it to numerical</li>
  <li>One-hot encoding</li>
</ul>

<p>Has a mobile phone:</p>
<ul>
  <li>Drop feature</li>
</ul>

<p>Has a work phone:</p>
<ul>
  <li>One-hot encoding</li>
</ul>

<p>Has a phone:</p>
<ul>
  <li>One-hot encoding</li>
</ul>

<p>Has an email:</p>
<ul>
  <li>One-hot encoding</li>
</ul>

<p>Account age:</p>
<ul>
  <li>Drop feature</li>
</ul>

<p>Is high risk(Target):</p>
<ul>
  <li>Change the data type to numerical</li>
  <li>balance the data with SMOTE</li>
</ul>

<h3>Data Cleaning</h3>

<p>Here we are creating a class to handle outliers. But why do we have to remove the outliers?</p>

<p>Outliers are data points that differ significantly from other observations in the dataset. Outliers can spoil and mislead the training process resulting in longer training times, less accurate models and ultimately poorer results, which means that outliers must remove from the dataset.</p>

<p>This class will remove outliers more or less than 3 inter-quantile ranges away from the mean. This class will be the first class in the scikit-learn <code class="language-plaintext highlighter-rouge">Pipeline</code> to call.</p>

<p>Note: Refer to this picture below to understand IQR. In the image below, 1.5 IQR is used; in our case, we use 3 IQR, which is more sensitive to extreme outliers than 1.5 IQR.</p>

<p><img src="/blog/assets/post_cont_image/iqr.png" alt="iqr" /></p>

<p>Image credit: <a href="https://www.researchgate.net/figure/Interquartile-range-IQR-projection-on-a-normally-distributed-density-The-median-of-IQR_fig2_340969321">Research gate</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">OutlierRemover</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">feat_with_outliers</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Family member count'</span><span class="p">,</span><span class="s">'Income'</span><span class="p">,</span> <span class="s">'Employment length'</span><span class="p">]):</span>
        <span class="c1"># initializing the instance of the object
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">feat_with_outliers</span> <span class="o">=</span> <span class="n">feat_with_outliers</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="c1"># check if the feature in part of the dataset's features
</span>        <span class="k">if</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_outliers</span><span class="p">).</span><span class="n">issubset</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="c1"># 25% quantile
</span>            <span class="n">Q1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_outliers</span><span class="p">].</span><span class="n">quantile</span><span class="p">(.</span><span class="mi">25</span><span class="p">)</span>
            <span class="c1"># 75% quantile
</span>            <span class="n">Q3</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_outliers</span><span class="p">].</span><span class="n">quantile</span><span class="p">(.</span><span class="mi">75</span><span class="p">)</span>
            <span class="n">IQR</span> <span class="o">=</span> <span class="n">Q3</span> <span class="o">-</span> <span class="n">Q1</span>
            <span class="c1"># keep the data within 3 IQR only and discard the rest
</span>            <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="o">~</span><span class="p">((</span><span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_outliers</span><span class="p">]</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">Q1</span> <span class="o">-</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">IQR</span><span class="p">))</span> <span class="o">|</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_outliers</span><span class="p">]</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">Q3</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">IQR</span><span class="p">))).</span><span class="nb">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"One or more features are not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div>

<h3>Feature selection</h3>

<p>Next is feature selection; here, we will drop the features that we judge are not useful in our prediction. Note this is not a feature selection based on the model coefficients or feature importance; it is purely based on logic.</p>

<p>The features to be dropped are <code class="language-plaintext highlighter-rouge">ID</code>, <code class="language-plaintext highlighter-rouge">has a mobile phone</code>, <code class="language-plaintext highlighter-rouge">children count</code>, <code class="language-plaintext highlighter-rouge">job title</code>, <code class="language-plaintext highlighter-rouge">account age</code>.</p>

<p>Now the next question is, why are we dropping these features?</p>

<ul>
  <li>ID: ID is not helpful for prediction, it helped us when we were merging the two datasets, but after that, there is no need to keep it.</li>
  <li>Has a mobile phone: Since everyone has a mobile phone, this feature does not inform us about anything and is useless for the model.</li>
  <li>Children count: is highly correlated with Family member count, and to avoid multicollinearity, we will drop it.</li>
  <li>Job title: Has some missing values and the count of each category is not very different to justify using the mode to fill the missing values. So we drop it.</li>
  <li>Account age: Because Account age is used to create the target, reusing it will make our model overfit. Plus, this information is unknown while applying for a credit card and is not a predictor feature.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DropFeatures</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span><span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">feature_to_drop</span> <span class="o">=</span> <span class="p">[</span><span class="s">'ID'</span><span class="p">,</span><span class="s">'Has a mobile phone'</span><span class="p">,</span><span class="s">'Children count'</span><span class="p">,</span><span class="s">'Job title'</span><span class="p">,</span><span class="s">'Account age'</span><span class="p">]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">feature_to_drop</span> <span class="o">=</span> <span class="n">feature_to_drop</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">feature_to_drop</span><span class="p">).</span><span class="n">issubset</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="c1"># drop the list of features
</span>            <span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">feature_to_drop</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"One or more features are not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div>

<h3>Feature engineering</h3>

<p>This class will convert the features that use days (<code class="language-plaintext highlighter-rouge">Employment length</code>, <code class="language-plaintext highlighter-rouge">Age</code>) to absolute value because we can’t have negative days of employment.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TimeConversionHandler</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feat_with_days</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Employment length'</span><span class="p">,</span> <span class="s">'Age'</span><span class="p">]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">feat_with_days</span> <span class="o">=</span> <span class="n">feat_with_days</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_days</span><span class="p">).</span><span class="n">issubset</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="c1"># convert days to absolute value using NumPy
</span>            <span class="n">X</span><span class="p">[[</span><span class="s">'Employment length'</span><span class="p">,</span><span class="s">'Age'</span><span class="p">]]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">X</span><span class="p">[[</span><span class="s">'Employment length'</span><span class="p">,</span><span class="s">'Age'</span><span class="p">]])</span>
            <span class="k">return</span> <span class="n">X</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"One or more features are not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">X</span>
</code></pre></div></div>

<p>The following class will convert the employment length of retirees (set to 365243) to 0 so that it is not considered an outlier.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RetireeHandler</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="s">'Employment length'</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="c1"># select rows with an employment length is 365243, which corresponds to retirees
</span>            <span class="n">df_ret_idx</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">][</span><span class="n">df</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">365243</span><span class="p">].</span><span class="n">index</span>
            <span class="c1"># set those rows with value 365243 to 0
</span>            <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_ret_idx</span><span class="p">,</span><span class="s">'Employment length'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Employment length is not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div>

<p>Using the cubic root transformation, this class will reduce income and age distribution skewness. Skewed features negatively affect our predictive model’s performance, and machine learning models perform better with normally distributed data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SkewnessHandler</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">feat_with_skewness</span><span class="o">=</span><span class="p">[</span><span class="s">'Income'</span><span class="p">,</span><span class="s">'Age'</span><span class="p">]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">feat_with_skewness</span> <span class="o">=</span> <span class="n">feat_with_skewness</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_skewness</span><span class="p">).</span><span class="n">issubset</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="c1"># Handle skewness with cubic root transformation
</span>            <span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_skewness</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cbrt</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_skewness</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"One or more features are not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div>

<p>This class will change 1 to the character “Y” and 0 to “N,” which will be more comprehensive when we do a one-hot encoding for these features <code class="language-plaintext highlighter-rouge">Has a work phone</code>, <code class="language-plaintext highlighter-rouge">Has a phone</code>, <code class="language-plaintext highlighter-rouge">Has an email</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BinningNumToYN</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">feat_with_num_enc</span><span class="o">=</span><span class="p">[</span><span class="s">'Has a work phone'</span><span class="p">,</span><span class="s">'Has a phone'</span><span class="p">,</span><span class="s">'Has an email'</span><span class="p">]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">feat_with_num_enc</span> <span class="o">=</span> <span class="n">feat_with_num_enc</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_num_enc</span><span class="p">).</span><span class="n">issubset</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="c1"># Change 0 to N and 1 to Y for all the features in feat_with_num_enc
</span>            <span class="k">for</span> <span class="n">ft</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">feat_with_num_enc</span><span class="p">:</span>
                <span class="n">df</span><span class="p">[</span><span class="n">ft</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">ft</span><span class="p">].</span><span class="nb">map</span><span class="p">({</span><span class="mi">1</span><span class="p">:</span><span class="s">'Y'</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="s">'N'</span><span class="p">})</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"One or more features are not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div>

<p>This class will do one-hot encoding on the categorical features, but also this class will keep the names of the features. We want to keep the feature names instead of an array without names (default) because the feature names will be used for feature importance.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">OneHotWithFeatNames</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span><span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">one_hot_enc_ft</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Gender'</span><span class="p">,</span> <span class="s">'Marital status'</span><span class="p">,</span> <span class="s">'Dwelling'</span><span class="p">,</span> <span class="s">'Employment status'</span><span class="p">,</span> <span class="s">'Has a car'</span><span class="p">,</span> <span class="s">'Has a property'</span><span class="p">,</span> <span class="s">'Has a work phone'</span><span class="p">,</span> <span class="s">'Has a phone'</span><span class="p">,</span> <span class="s">'Has an email'</span><span class="p">]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">one_hot_enc_ft</span> <span class="o">=</span> <span class="n">one_hot_enc_ft</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">one_hot_enc_ft</span><span class="p">).</span><span class="n">issubset</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="c1"># function to one-hot encode the features
</span>            <span class="k">def</span> <span class="nf">one_hot_enc</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">one_hot_enc_ft</span><span class="p">):</span>
                <span class="c1"># instantiate the OneHotEncoder object
</span>                <span class="n">one_hot_enc</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
                <span class="c1"># fit the dataframe with the features we want to one-hot encode
</span>                <span class="n">one_hot_enc</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">one_hot_enc_ft</span><span class="p">])</span>
                <span class="c1"># get output feature names for transformation.
</span>                <span class="n">feat_names_one_hot_enc</span> <span class="o">=</span> <span class="n">one_hot_enc</span><span class="p">.</span><span class="n">get_feature_names_out</span><span class="p">(</span><span class="n">one_hot_enc_ft</span><span class="p">)</span>
                <span class="c1"># change the one hot encoding array to a dataframe with the column names
</span>                <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">one_hot_enc</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">one_hot_enc_ft</span><span class="p">]).</span><span class="n">toarray</span><span class="p">(),</span><span class="n">columns</span><span class="o">=</span><span class="n">feat_names_one_hot_enc</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">df</span>
            <span class="c1"># function to concatenate the one hot encoded features with the rest of the features that were not encoded
</span>            <span class="k">def</span> <span class="nf">concat_with_rest</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">one_hot_enc_df</span><span class="p">,</span><span class="n">one_hot_enc_ft</span><span class="p">):</span>
                <span class="c1"># get the rest of the features that are not encoded
</span>                <span class="n">rest_of_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">ft</span> <span class="k">for</span> <span class="n">ft</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">ft</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">one_hot_enc_ft</span><span class="p">]</span>
                <span class="c1"># concatenate the rest of the features with the one hot encoded features
</span>                <span class="n">df_concat</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">one_hot_enc_df</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="n">rest_of_features</span><span class="p">]],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">df_concat</span>
            <span class="c1"># call the one_hot_enc function and stores the dataframe in the one_hot_enc_df variable
</span>            <span class="n">one_hot_enc_df</span> <span class="o">=</span> <span class="n">one_hot_enc</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">one_hot_enc_ft</span><span class="p">)</span>
            <span class="c1"># returns the concatenated dataframe and stores it in the full_df_one_hot_enc variable
</span>            <span class="n">full_df_one_hot_enc</span> <span class="o">=</span> <span class="n">concat_with_rest</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">one_hot_enc_df</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">one_hot_enc_ft</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">full_df_one_hot_enc</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"One or more features are not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div>

<p>This class will convert the education level to an ordinal encoding. Here we use ordinal encoding instead of one-hot encoding because we know that the education level is ranked (University is higher than primary school).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">OrdinalFeatNames</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span><span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">ordinal_enc_ft</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Education level'</span><span class="p">]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ordinal_enc_ft</span> <span class="o">=</span> <span class="n">ordinal_enc_ft</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="s">'Education level'</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="c1"># instantiate the OrdinalEncoder object
</span>            <span class="n">ordinal_enc</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
            <span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">ordinal_enc_ft</span><span class="p">]</span> <span class="o">=</span> <span class="n">ordinal_enc</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">ordinal_enc_ft</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Education level is not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div>

<p>This class will scale the feature using min-max scaling while keeping the feature names. You may ask why we have to scale. Well, some of the numerical features range from 0 to 20 (Family member count) while others range from 27000 to 1575000 (Income), so this means that some machine learning algorithms will weight the features with big numbers more than the feature with smaller numbers which should not be the case. So scaling all the numerical feature on the same scale (0 to 1) solve this issue.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MinMaxWithFeatNames</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span><span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">min_max_scaler_ft</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Age'</span><span class="p">,</span> <span class="s">'Income'</span><span class="p">,</span> <span class="s">'Employment length'</span><span class="p">]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">min_max_scaler_ft</span> <span class="o">=</span> <span class="n">min_max_scaler_ft</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">min_max_scaler_ft</span><span class="p">).</span><span class="n">issubset</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="c1"># instantiate the MinMaxScaler object
</span>            <span class="n">min_max_enc</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
            <span class="c1"># fit and transform on a scale 0 to 1
</span>            <span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">min_max_scaler_ft</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_max_enc</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">min_max_scaler_ft</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"One or more features are not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div>

<p>This class will change the data type of the target variable to numerical as it is an object data type even though it is 0 and 1’s (0 and 1’s expressed as strings)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ChangeToNumTarget</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span><span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="c1"># check if the target is part of the dataframe
</span>        <span class="k">if</span> <span class="s">'Is high risk'</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="c1"># change to a numeric data type using Pandas
</span>            <span class="n">df</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Is high risk is not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div>

<p>This class will oversample the target variable using SMOTE because the minority class (Is high risk = 1) is scarce in the data, as we have seen while doing EDA of the target variable (<code class="language-plaintext highlighter-rouge">1</code> only accounts for about 1.71% of the total data while <code class="language-plaintext highlighter-rouge">0</code> represent 98.29%).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Oversample</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span><span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="s">'Is high risk'</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="c1"># smote function instantiation to oversample the minority class to fix the imbalance data
</span>            <span class="n">oversample</span> <span class="o">=</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="s">'minority'</span><span class="p">)</span>
            <span class="c1"># fit and resample the classes and assign them to X_bal, y_bal variable
</span>            <span class="n">X_bal</span><span class="p">,</span> <span class="n">y_bal</span> <span class="o">=</span> <span class="n">oversample</span><span class="p">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s">'Is high risk'</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">])</span>
            <span class="c1"># concatenate the balanced classes column-wise
</span>            <span class="n">df_bal</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_bal</span><span class="p">),</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_bal</span><span class="p">)],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df_bal</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Is high risk is not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>
</code></pre></div></div>

<h3>Data Preprocessing</h3>

<p>Now we are ready to create the data preprocessing pipeline using the built sklearn function <code class="language-plaintext highlighter-rouge">Pipeline</code>. This function calls each class in the pipeline sequentially, starting from the outlier remover to the oversample class. The dataset will be transformed consecutively from the first class to the next one till the end. The pipeline will be stored in a variable called pipeline and will call <code class="language-plaintext highlighter-rouge">fit_transform</code> on that variable, pass our dataframe we want to transform and return the result.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">full_pipeline</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="c1"># Create the pipeline that will call all the classes from OutlierRemoval() to Oversample() in one go
</span>    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s">'outlier_remover'</span><span class="p">,</span> <span class="n">OutlierRemover</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'feature_dropper'</span><span class="p">,</span> <span class="n">DropFeatures</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'time_conversion_handler'</span><span class="p">,</span> <span class="n">TimeConversionHandler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'retiree_handler'</span><span class="p">,</span> <span class="n">RetireeHandler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'skewness_handler'</span><span class="p">,</span> <span class="n">SkewnessHandler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'binning_num_to_yn'</span><span class="p">,</span> <span class="n">BinningNumToYN</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'one_hot_with_feat_names'</span><span class="p">,</span> <span class="n">OneHotWithFeatNames</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'ordinal_feat_names'</span><span class="p">,</span> <span class="n">OrdinalFeatNames</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'min_max_with_feat_names'</span><span class="p">,</span> <span class="n">MinMaxWithFeatNames</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'change_to_num_target'</span><span class="p">,</span> <span class="n">ChangeToNumTarget</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'oversample'</span><span class="p">,</span> <span class="n">Oversample</span><span class="p">())</span>
    <span class="p">])</span>
    <span class="n">df_pipe_prep</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df_pipe_prep</span>
</code></pre></div></div>

<p>Now we pass in the training dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd</span><span class="p">.</span><span class="n">options</span><span class="p">.</span><span class="n">mode</span><span class="p">.</span><span class="n">chained_assignment</span> <span class="o">=</span> <span class="bp">None</span>  <span class="c1"># Hide the warnings
</span><span class="n">cc_train_prep</span> <span class="o">=</span> <span class="n">full_pipeline</span><span class="p">(</span><span class="n">cc_train_copy</span><span class="p">)</span>
</code></pre></div></div>

<p>We check how many rows and columns we have after the transformation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cc_train_prep</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(46544, 34)
</code></pre></div></div>

<p>Let’s quickly look at the first few rows of the transformed dataframe. We can see that the columns’ names have been kept, and all the transformations have taken place.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd</span><span class="p">.</span><span class="n">set_option</span><span class="p">(</span><span class="s">'display.max_columns'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
<span class="n">cc_train_prep</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Gender_F</th>
      <th>Gender_M</th>
      <th>Marital status_Civil marriage</th>
      <th>Marital status_Married</th>
      <th>Marital status_Separated</th>
      <th>Marital status_Single / not married</th>
      <th>Marital status_Widow</th>
      <th>Dwelling_Co-op apartment</th>
      <th>Dwelling_House / apartment</th>
      <th>Dwelling_Municipal apartment</th>
      <th>Dwelling_Office apartment</th>
      <th>Dwelling_Rented apartment</th>
      <th>Dwelling_With parents</th>
      <th>Employment status_Commercial associate</th>
      <th>Employment status_Pensioner</th>
      <th>Employment status_State servant</th>
      <th>Employment status_Student</th>
      <th>Employment status_Working</th>
      <th>Has a car_N</th>
      <th>Has a car_Y</th>
      <th>Has a property_N</th>
      <th>Has a property_Y</th>
      <th>Has a work phone_N</th>
      <th>Has a work phone_Y</th>
      <th>Has a phone_N</th>
      <th>Has a phone_Y</th>
      <th>Has an email_N</th>
      <th>Has an email_Y</th>
      <th>Income</th>
      <th>Education level</th>
      <th>Age</th>
      <th>Employment length</th>
      <th>Family member count</th>
      <th>Is high risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>4.00</td>
      <td>0.60</td>
      <td>0.27</td>
      <td>2.00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>1.00</td>
      <td>0.20</td>
      <td>0.14</td>
      <td>2.00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.52</td>
      <td>4.00</td>
      <td>0.39</td>
      <td>0.50</td>
      <td>4.00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.81</td>
      <td>1.00</td>
      <td>0.84</td>
      <td>0.18</td>
      <td>1.00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.68</td>
      <td>4.00</td>
      <td>0.60</td>
      <td>0.04</td>
      <td>1.00</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>Now, we extract the target variable <code class="language-plaintext highlighter-rouge">Is high risk</code> from the dataframe and create a new dataframe composed of independent features (also called predictor, aka all the features except the target variable) as <code class="language-plaintext highlighter-rouge">X_cc_train_prep</code> and the target variable as  <code class="language-plaintext highlighter-rouge">y_cc_train_prep</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># split the train data into X and y (target)
</span><span class="n">X_cc_train_prep</span><span class="p">,</span> <span class="n">y_cc_train_prep</span> <span class="o">=</span> <span class="n">cc_train_prep</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">cc_train_prep</span><span class="p">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s">'Is high risk'</span><span class="p">],</span> <span class="n">cc_train_prep</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="s">'int64'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_cc_train_prep</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Gender_F</th>
      <th>Gender_M</th>
      <th>Marital status_Civil marriage</th>
      <th>Marital status_Married</th>
      <th>Marital status_Separated</th>
      <th>Marital status_Single / not married</th>
      <th>Marital status_Widow</th>
      <th>Dwelling_Co-op apartment</th>
      <th>Dwelling_House / apartment</th>
      <th>Dwelling_Municipal apartment</th>
      <th>Dwelling_Office apartment</th>
      <th>Dwelling_Rented apartment</th>
      <th>Dwelling_With parents</th>
      <th>Employment status_Commercial associate</th>
      <th>Employment status_Pensioner</th>
      <th>Employment status_State servant</th>
      <th>Employment status_Student</th>
      <th>Employment status_Working</th>
      <th>Has a car_N</th>
      <th>Has a car_Y</th>
      <th>Has a property_N</th>
      <th>Has a property_Y</th>
      <th>Has a work phone_N</th>
      <th>Has a work phone_Y</th>
      <th>Has a phone_N</th>
      <th>Has a phone_Y</th>
      <th>Has an email_N</th>
      <th>Has an email_Y</th>
      <th>Income</th>
      <th>Education level</th>
      <th>Age</th>
      <th>Employment length</th>
      <th>Family member count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>4.00</td>
      <td>0.60</td>
      <td>0.27</td>
      <td>2.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.42</td>
      <td>1.00</td>
      <td>0.20</td>
      <td>0.14</td>
      <td>2.00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.52</td>
      <td>4.00</td>
      <td>0.39</td>
      <td>0.50</td>
      <td>4.00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.81</td>
      <td>1.00</td>
      <td>0.84</td>
      <td>0.18</td>
      <td>1.00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>0.00</td>
      <td>0.68</td>
      <td>4.00</td>
      <td>0.60</td>
      <td>0.04</td>
      <td>1.00</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_cc_train_prep</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0    0
1    0
2    0
3    0
4    0
Name: Is high risk, dtype: int64
</code></pre></div></div>

<h3>Short-list promising models</h3>

<p>Alright! the moment we have been all waiting for has finally arrived; time to train our models. We first create a dictionary of models and their corresponding names. This dictionary will be used to loop through all the models and train them without having to write them over and over again.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">classifiers</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'sgd'</span><span class="p">:</span><span class="n">SGDClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="s">'perceptron'</span><span class="p">),</span>
    <span class="s">'logistic_regression'</span><span class="p">:</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
    <span class="s">'support_vector_machine'</span><span class="p">:</span><span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span><span class="n">probability</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="s">'decision_tree'</span><span class="p">:</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
    <span class="s">'random_forest'</span><span class="p">:</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
    <span class="s">'gaussian_naive_bayes'</span><span class="p">:</span><span class="n">GaussianNB</span><span class="p">(),</span>
    <span class="s">'k_nearest_neighbors'</span><span class="p">:</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span>
    <span class="s">'gradient_boosting'</span><span class="p">:</span><span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
    <span class="s">'linear_discriminant_analysis'</span><span class="p">:</span><span class="n">LinearDiscriminantAnalysis</span><span class="p">(),</span>
    <span class="s">'bagging'</span><span class="p">:</span><span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
    <span class="s">'neural_network'</span><span class="p">:</span><span class="n">MLPClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
    <span class="s">'adaboost'</span><span class="p">:</span><span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
    <span class="s">'extra_trees'</span><span class="p">:</span><span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
    <span class="p">}</span>
</code></pre></div></div>

<p>Now we will write some of the functions used for our training model. The first function is a function to plot the feature importance of the model. The feature importance is ranking features that contribute more(or less) than other features to the model prediction. The feature importance varies from one model to another.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">feat_importance_plot</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span> <span class="n">model_name</span><span class="p">):</span>
    <span class="s">'''
    Function to get the feature importance of the classifier and plot it
    '''</span>
    <span class="c1"># in order to get the feature importance, the model should not be 'sgd','support_vector_machine','gaussian_naive_bayes','k_nearest_neighbors','bagging','neural_network'
</span>    <span class="k">if</span> <span class="n">model_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'sgd'</span><span class="p">,</span><span class="s">'support_vector_machine'</span><span class="p">,</span><span class="s">'gaussian_naive_bayes'</span><span class="p">,</span><span class="s">'k_nearest_neighbors'</span><span class="p">,</span><span class="s">'bagging'</span><span class="p">,</span><span class="s">'neural_network'</span><span class="p">]:</span>
        <span class="c1"># change xtick font size
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'xtick.labelsize'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'ytick.labelsize'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span>
        <span class="c1"># top 10 most predictive features
</span>        <span class="n">top_10_feat</span> <span class="o">=</span> <span class="n">FeatureImportances</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span> <span class="n">relative</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="c1"># top 10 least predictive features
</span>        <span class="n">bottom_10_feat</span> <span class="o">=</span> <span class="n">FeatureImportances</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span> <span class="n">relative</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">topn</span><span class="o">=-</span><span class="mi">10</span><span class="p">)</span>
        <span class="c1"># change the figure size
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="c1"># change x label font size
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'xlabel'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="c1"># fit to get the feature importance
</span>        <span class="n">top_10_feat</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cc_train_prep</span><span class="p">,</span> <span class="n">y_cc_train_prep</span><span class="p">)</span>
        <span class="c1"># show the plot
</span>        <span class="n">top_10_feat</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'xlabel'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="c1"># fit to get the feature importance
</span>        <span class="n">bottom_10_feat</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cc_train_prep</span><span class="p">,</span> <span class="n">y_cc_train_prep</span><span class="p">)</span>
        <span class="c1"># show the plot
</span>        <span class="n">bottom_10_feat</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'No feature importance for {0}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<p>On the other hand, this function is used to get the y predictions of the model using cross-validation prediction with k fold equal to 10.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">y_prediction_func</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span><span class="n">model_name</span><span class="p">,</span><span class="n">final_model</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="s">'''
    Function to get the y prediction
    '''</span>
    <span class="k">if</span> <span class="n">final_model</span> <span class="o">==</span> <span class="bp">False</span><span class="p">:</span>
        <span class="c1"># check if y_train_copy_pred files exist; if not, create it
</span>        <span class="n">y_cc_train_pred_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s">'saved_models/{0}/y_train_copy_pred_{0}.sav'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">))</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">y_cc_train_pred_path</span><span class="p">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># if FileNotFoundError is raised
</span>        <span class="k">except</span> <span class="nb">FileNotFoundError</span><span class="p">:</span>
            <span class="c1"># cross-validation prediction with kfold = 10
</span>            <span class="n">y_cc_train_pred</span> <span class="o">=</span> <span class="n">cross_val_predict</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span><span class="n">X_cc_train_prep</span><span class="p">,</span><span class="n">y_cc_train_prep</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># save the predictions using joblib library
</span>            <span class="n">joblib</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">y_cc_train_pred</span><span class="p">,</span><span class="n">y_cc_train_pred_path</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">y_cc_train_pred</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># if the file exists, load the predictions
</span>            <span class="n">y_cc_train_pred</span> <span class="o">=</span> <span class="n">joblib</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">y_cc_train_pred_path</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">y_cc_train_pred</span>
    <span class="c1"># When we are dealing with the final model
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># check if y_train_copy_pred files exist; if not, create it
</span>        <span class="n">y_cc_train_pred_path_final</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s">'saved_models_final/{0}/y_train_copy_pred_{0}_final.sav'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">))</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">y_cc_train_pred_path_final</span><span class="p">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">except</span> <span class="nb">FileNotFoundError</span><span class="p">:</span>
            <span class="c1"># cross validation prediction with kfold = 10
</span>            <span class="n">y_cc_train_pred_final</span> <span class="o">=</span> <span class="n">cross_val_predict</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span><span class="n">X_cc_train_prep</span><span class="p">,</span><span class="n">y_cc_train_prep</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># save the predictions
</span>            <span class="n">joblib</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">y_cc_train_pred_final</span><span class="p">,</span><span class="n">y_cc_train_pred_path_final</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">y_cc_train_pred_final</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># if it exists load the predictions
</span>            <span class="n">y_cc_train_pred_final</span> <span class="o">=</span> <span class="n">joblib</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">y_cc_train_pred_path_final</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">y_cc_train_pred_final</span>
</code></pre></div></div>

<p>This function will plot the confusion matrix for each of the algorithms.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">confusion_matrix_func</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span><span class="n">model_name</span><span class="p">,</span><span class="n">final_model</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="s">'''
    Function to plot the confusion matrix
    '''</span>
    <span class="k">if</span> <span class="n">final_model</span> <span class="o">==</span> <span class="bp">False</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="c1"># plot confusion matrix
</span>        <span class="n">conf_matrix</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">.</span><span class="n">from_predictions</span><span class="p">(</span><span class="n">y_cc_train_prep</span><span class="p">,</span><span class="n">y_prediction_func</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span><span class="n">model_name</span><span class="p">),</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'Blues'</span><span class="p">,</span><span class="n">values_format</span><span class="o">=</span><span class="s">'d'</span><span class="p">)</span>
        <span class="c1"># remove the grid
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="n">visible</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="c1"># increase the font size of the X and Y labels
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted label'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True label'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="c1"># give a title to the plot using the model name
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Confusion Matrix'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="c1"># show the plot
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    <span class="c1"># When we are dealing with the final model
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="c1"># plot confusion matrix
</span>        <span class="n">conf_matrix_final</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">.</span><span class="n">from_predictions</span><span class="p">(</span><span class="n">y_cc_train_prep</span><span class="p">,</span><span class="n">y_prediction_func</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span><span class="n">model_name</span><span class="p">,</span><span class="n">final_model</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'Blues'</span><span class="p">,</span><span class="n">values_format</span><span class="o">=</span><span class="s">'d'</span><span class="p">)</span>
        <span class="c1"># remove the grid
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="n">visible</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="c1"># increase the font size of the X and Y labels
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted label'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True label'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="c1"># give a title to the plot using the model name
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Confusion Matrix'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
        <span class="c1"># show the plot
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<p>The following function will plot the ROC curve of each model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">roc_curve_func</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span><span class="n">model_name</span><span class="p">,</span><span class="n">final_model</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="s">'''
    Function to plot the roc curve
    '''</span>
    <span class="k">if</span> <span class="n">final_model</span> <span class="o">==</span> <span class="bp">False</span><span class="p">:</span>
        <span class="c1"># check if the y probabilities file exists; if not create it
</span>        <span class="n">y_proba_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s">'saved_models/{0}/y_cc_train_proba_{0}.sav'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">))</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">y_proba_path</span><span class="p">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># if the FileNotFoundError is raised
</span>        <span class="k">except</span> <span class="nb">FileNotFoundError</span><span class="p">:</span>
            <span class="c1"># calculate the y probability
</span>            <span class="n">y_cc_train_proba</span> <span class="o">=</span> <span class="n">model_trn</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_cc_train_prep</span><span class="p">)</span>
            <span class="c1"># save y_cc_train_proba file at y_proba_path
</span>            <span class="n">joblib</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">y_cc_train_proba</span><span class="p">,</span><span class="n">y_proba_path</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># if path exist load the y probabilities file
</span>            <span class="n">y_cc_train_proba</span> <span class="o">=</span> <span class="n">joblib</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">y_proba_path</span><span class="p">)</span>
        <span class="c1"># plot the roc curve
</span>        <span class="n">skplt</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="n">plot_roc</span><span class="p">(</span><span class="n">y_cc_train_prep</span><span class="p">,</span> <span class="n">y_cc_train_proba</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s">'ROC curve for {0}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'cool'</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span> <span class="n">text_fontsize</span><span class="o">=</span><span class="s">'large'</span><span class="p">)</span>
        <span class="c1"># remove the grid
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="n">visible</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    <span class="c1"># When we are dealing with the final model
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># check if y probabilities file exists, if not create it
</span>        <span class="n">y_proba_path_final</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s">'saved_models_final/{0}/y_cc_train_proba_{0}_final.sav'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">))</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">y_proba_path_final</span><span class="p">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">except</span> <span class="nb">FileNotFoundError</span><span class="p">:</span>
            <span class="n">y_cc_train_proba_final</span> <span class="o">=</span> <span class="n">model_trn</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_cc_train_prep</span><span class="p">)</span>
            <span class="n">joblib</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">y_cc_train_proba_final</span><span class="p">,</span><span class="n">y_proba_path_final</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># if path exist load the y probabilities file
</span>            <span class="n">y_cc_train_proba_final</span> <span class="o">=</span> <span class="n">joblib</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">y_proba_path_final</span><span class="p">)</span>
        <span class="c1"># plot the roc curve
</span>        <span class="n">skplt</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="n">plot_roc</span><span class="p">(</span><span class="n">y_cc_train_prep</span><span class="p">,</span> <span class="n">y_cc_train_proba_final</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s">'ROC curve for {0}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'cool'</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span> <span class="n">text_fontsize</span><span class="o">=</span><span class="s">'large'</span><span class="p">)</span>
        <span class="c1"># remove the grid
</span>        <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="n">visible</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<p>This other function will print the classification report. A classification report is a table that describes the performance of a classification model and has information like precision, recall, f1-score, support, accuracy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">score_func</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">final_model</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="s">'''
    Function to display the classification report
    '''</span>
    <span class="k">if</span> <span class="n">final_model</span> <span class="o">==</span> <span class="bp">False</span><span class="p">:</span>
        <span class="n">class_report</span> <span class="o">=</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_cc_train_prep</span><span class="p">,</span><span class="n">y_prediction_func</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span><span class="n">model_name</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="n">class_report</span><span class="p">)</span>
    <span class="c1"># When we are dealing with the final model
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="n">class_report_final</span> <span class="o">=</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_cc_train_prep</span><span class="p">,</span><span class="n">y_prediction_func</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span><span class="n">model_name</span><span class="p">,</span><span class="n">final_model</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="n">class_report_final</span><span class="p">)</span>
</code></pre></div></div>

<p>This function will train the models and save them in the <code class="language-plaintext highlighter-rouge">saved_models</code> and <code class="language-plaintext highlighter-rouge">saved_models_final</code> folders.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">model_name</span><span class="p">,</span><span class="n">final_model</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="s">'''
    Function to train and save the model
    '''</span>
    <span class="c1"># If we are not training the final model
</span>    <span class="k">if</span> <span class="n">final_model</span> <span class="o">==</span> <span class="bp">False</span><span class="p">:</span>
        <span class="c1"># Check if the model file exists and if not, create, train and save it
</span>        <span class="n">model_file_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s">'saved_models/{0}/{0}_model.sav'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">))</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">model_file_path</span><span class="p">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">except</span> <span class="nb">FileNotFoundError</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s">'sgd'</span><span class="p">:</span>
                <span class="c1"># for sgd, loss = 'hinge' does not have a predict_proba method. Therefore, we use a calibrated model
</span>                <span class="n">calibrated_model</span> <span class="o">=</span> <span class="n">CalibratedClassifierCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">)</span>
                <span class="c1"># train the model
</span>                <span class="n">model_trn</span> <span class="o">=</span> <span class="n">calibrated_model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cc_train_prep</span><span class="p">,</span><span class="n">y_cc_train_prep</span><span class="p">)</span>
            <span class="c1"># For the rest of the models
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="n">model_trn</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cc_train_prep</span><span class="p">,</span><span class="n">y_cc_train_prep</span><span class="p">)</span>
            <span class="c1"># save the model
</span>            <span class="n">joblib</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span><span class="n">model_file_path</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">model_trn</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># if path exist load the model
</span>            <span class="n">model_trn</span> <span class="o">=</span> <span class="n">joblib</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_file_path</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">model_trn</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># check if the final model file exist and if not create, train and save it
</span>        <span class="n">final_model_file_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s">'saved_models_final/{0}/{0}_model.sav'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">))</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">final_model_file_path</span><span class="p">.</span><span class="n">resolve</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">except</span> <span class="nb">FileNotFoundError</span><span class="p">:</span>
            <span class="c1"># train the model
</span>            <span class="n">model_trn</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cc_train_prep</span><span class="p">,</span><span class="n">y_cc_train_prep</span><span class="p">)</span>
            <span class="n">joblib</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span><span class="n">final_model_file_path</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">model_trn</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># if path exist load the model
</span>            <span class="n">model_trn</span> <span class="o">=</span> <span class="n">joblib</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">final_model_file_path</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">model_trn</span>
</code></pre></div></div>

<p>This function below will look at the <code class="language-plaintext highlighter-rouge">folder_check_model</code> which will check if <code class="language-plaintext highlighter-rouge">saved_models</code> folder exists; if not, it will create it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">folder_check_model</span><span class="p">():</span>
    <span class="c1"># check if the folder for saving the model exists, if not create it
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="s">'saved_models/{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">)):</span>
        <span class="n">os</span><span class="p">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s">'saved_models/{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># loop over all the models
</span><span class="k">for</span> <span class="n">model_name</span><span class="p">,</span><span class="n">model</span> <span class="ow">in</span> <span class="n">classifiers</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="c1"># title formatting
</span>    <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'  {}  '</span><span class="p">.</span><span class="n">center</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="s">'-'</span><span class="p">).</span><span class="nb">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    <span class="c1"># check if the folder for saving the model exists; if not create it
</span>    <span class="n">folder_check_model</span><span class="p">()</span>
    <span class="c1"># train the model
</span>    <span class="n">model_trn</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">model_name</span><span class="p">)</span>
    <span class="c1"># print the scores from the classification report
</span>    <span class="n">score_func</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
    <span class="c1"># plot the ROC curve
</span>    <span class="n">roc_curve_func</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span><span class="n">model_name</span><span class="p">)</span>
    <span class="c1"># plot the confusion matrix
</span>    <span class="n">confusion_matrix_func</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span><span class="n">model_name</span><span class="p">)</span>
    <span class="c1"># plot feature importance
</span>    <span class="n">feat_importance_plot</span><span class="p">(</span><span class="n">model_trn</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
    <span class="n">warnings</span><span class="p">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s">"ignore"</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----------------------  sgd  ----------------------


              precision    recall  f1-score   support

           0       0.57      0.61      0.59     23272
           1       0.58      0.54      0.56     23272

    accuracy                           0.58     46544
   macro avg       0.58      0.58      0.58     46544
weighted avg       0.58      0.58      0.58     46544
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_264_1.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_3.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No feature importance for sgd






----------------------  logistic_regression  ----------------------


              precision    recall  f1-score   support

           0       0.59      0.57      0.58     23272
           1       0.59      0.61      0.60     23272

    accuracy                           0.59     46544
   macro avg       0.59      0.59      0.59     46544
weighted avg       0.59      0.59      0.59     46544
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_264_5.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_7.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_9.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_11.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----------------------  support_vector_machine  ----------------------


              precision    recall  f1-score   support

           0       0.87      0.81      0.84     23272
           1       0.83      0.88      0.85     23272

    accuracy                           0.85     46544
   macro avg       0.85      0.85      0.85     46544
weighted avg       0.85      0.85      0.85     46544
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_264_13.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_15.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No feature importance for support_vector_machine






----------------------  decision_tree  ----------------------


              precision    recall  f1-score   support

           0       0.98      0.98      0.98     23272
           1       0.98      0.98      0.98     23272

    accuracy                           0.98     46544
   macro avg       0.98      0.98      0.98     46544
weighted avg       0.98      0.98      0.98     46544
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_264_17.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_19.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_21.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_23.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----------------------  random_forest  ----------------------


              precision    recall  f1-score   support

           0       0.99      0.99      0.99     23272
           1       0.99      0.99      0.99     23272

    accuracy                           0.99     46544
   macro avg       0.99      0.99      0.99     46544
weighted avg       0.99      0.99      0.99     46544
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_264_25.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_27.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_29.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_31.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----------------------  gaussian_naive_bayes  ----------------------


              precision    recall  f1-score   support

           0       0.60      0.50      0.55     23272
           1       0.57      0.66      0.61     23272

    accuracy                           0.58     46544
   macro avg       0.58      0.58      0.58     46544
weighted avg       0.58      0.58      0.58     46544
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_264_33.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_35.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No feature importance for gaussian_naive_bayes






----------------------  k_nearest_neighbors  ----------------------


              precision    recall  f1-score   support

           0       0.98      0.96      0.97     23272
           1       0.96      0.98      0.97     23272

    accuracy                           0.97     46544
   macro avg       0.97      0.97      0.97     46544
weighted avg       0.97      0.97      0.97     46544
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_264_37.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_39.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No feature importance for k_nearest_neighbors






----------------------  gradient_boosting  ----------------------


              precision    recall  f1-score   support

           0       0.90      0.90      0.90     23272
           1       0.90      0.90      0.90     23272

    accuracy                           0.90     46544
   macro avg       0.90      0.90      0.90     46544
weighted avg       0.90      0.90      0.90     46544
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_264_41.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_43.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_45.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_47.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----------------------  linear_discriminant_analysis  ----------------------


              precision    recall  f1-score   support

           0       0.60      0.56      0.58     23272
           1       0.59      0.62      0.60     23272

    accuracy                           0.59     46544
   macro avg       0.59      0.59      0.59     46544
weighted avg       0.59      0.59      0.59     46544
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_264_49.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_51.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_53.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_55.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----------------------  bagging  ----------------------


              precision    recall  f1-score   support

           0       0.99      0.99      0.99     23272
           1       0.99      0.99      0.99     23272

    accuracy                           0.99     46544
   macro avg       0.99      0.99      0.99     46544
weighted avg       0.99      0.99      0.99     46544
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_264_57.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_59.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No feature importance for bagging






----------------------  neural_network  ----------------------


              precision    recall  f1-score   support

           0       0.97      0.94      0.96     23272
           1       0.94      0.97      0.96     23272

    accuracy                           0.96     46544
   macro avg       0.96      0.96      0.96     46544
weighted avg       0.96      0.96      0.96     46544
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_264_61.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_63.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No feature importance for neural_network






----------------------  adaboost  ----------------------


              precision    recall  f1-score   support

           0       0.78      0.76      0.77     23272
           1       0.77      0.79      0.78     23272

    accuracy                           0.77     46544
   macro avg       0.77      0.77      0.77     46544
weighted avg       0.77      0.77      0.77     46544
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_264_65.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_67.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_69.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_71.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----------------------  extra_trees  ----------------------


              precision    recall  f1-score   support

           0       0.99      0.99      0.99     23272
           1       0.99      0.99      0.99     23272

    accuracy                           0.99     46544
   macro avg       0.99      0.99      0.99     46544
weighted avg       0.99      0.99      0.99     46544
</code></pre></div></div>

<p><img src="/blog/assets/post_cont_image/output_264_73.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_75.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_77.png" alt="png" /></p>

<p><img src="/blog/assets/post_cont_image/output_264_79.png" alt="png" /></p>

<h3>What metrics to use in order to choose the best model for this problem?</h3>

<p>Lastly, we create a for loop function that will go through the dictionary of models and call all the functions that we have defined above.</p>

<p>Since the objective of this problem is to minimize the risk of a credit default, the metrics to use depends on the current economic situation:</p>

<ul>
  <li>
    <p>During a bull market (when the economy is expanding), people feel wealthy and are employed. Money is usually cheap, and the risk of default is low because of economic stability and low unemployment. The financial institution can handle the risk of default; therefore, it is not very strict about giving credit. The financial institution can handle some bad clients as long as most credit card owners are good clients (aka those who pay back their credit in time and in total).In this case, having a good recall (sensitivity) is ideal.</p>
  </li>
  <li>
    <p>During a bear market (when the economy is contracting), people lose their jobs and money through the stock market and other investment venues. Many people struggle to meet their financial obligations. The financial institution, therefore, tends to be more conservative in giving out credit or loans. The financial institution can’t afford to give out credit to many clients who won’t be able to pay back their credit. The financial institution would rather have a smaller number of good clients, even if it means that some good clients are denied credit. In this case, having good precision (specificity) is desirable.</p>

    <p><strong><em>Note</em></strong>: There is always a trade-off between precision and recall. Choosing the right metrics depends on the problem you are solving.</p>

    <p><strong><em>Conclusion</em></strong>: Since the time I worked on this project (beginning in 2022), we have been in the longest bull market (excluding March 2020 flash crash) ever recorded; we will use recall as our metric.</p>
  </li>
</ul>

<h3>Top model</h3>

<p>Using the ROC curve and recall, we can conclude that the best model is:</p>
<ul>
  <li>Gradient boosting classifier</li>
</ul>

<p>Let’s look at the picture below to understand how to interpret a ROC curve.</p>

<p><img src="/blog/assets/post_cont_image/roc_curve.svg" alt="heatmap" /></p>

<p>Source: <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#/media/File:Roc_curve.svg">Wikipedia</a></p>

<p>With this ROC curve, we can compare the performance of different classifiers. The closer the curve is to the top left corner of the plot without actually reaching the far end of the corner, the better the model</p>

<ul>
  <li>Any classifier’s ROC below the dashed red line performs worst than random chance. Random chance is a 50% chance of being correct for a binary classifier.</li>
  <li>Any classifier with the ROC curve blended with the dashed red line is no better than tossing a fair coin.</li>
  <li>The orange curve is slightly better than the dashed red line, but that would not be considered a good classifier.</li>
  <li>The green curve is much better than the orange one but could be better.</li>
  <li>The blue curve is the best classifier here; this curve gets closer to the top left without touching the top left corner.</li>
  <li>Lastly, the “perfect” curve that touches the top left corner is not a good classifier. You might be asked why; well, a classifier with this curve is overfitting, meaning it has learned so well on the training data but can’t generalize well on the test data (unseen data).</li>
</ul>

<p>So what to do when a classifier is overfitting? Well, these are the options to deal with this issue:</p>
<ul>
  <li>Use a simplified model by selecting fewer parameters or constraining the model (also called regularization).</li>
  <li>Gather more training data.</li>
  <li>Discard outliers and fix missing data.</li>
</ul>

<h3>Test the final model on the test set</h3>

<p>Now that we have our model trained, we can use it to predict the classes on the test set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cc_test_copy</span><span class="p">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>Gender</th>
      <th>Has a car</th>
      <th>Has a property</th>
      <th>Children count</th>
      <th>Income</th>
      <th>Employment status</th>
      <th>Education level</th>
      <th>Marital status</th>
      <th>Dwelling</th>
      <th>Age</th>
      <th>Employment length</th>
      <th>Has a mobile phone</th>
      <th>Has a work phone</th>
      <th>Has a phone</th>
      <th>Has an email</th>
      <th>Job title</th>
      <th>Family member count</th>
      <th>Account age</th>
      <th>Is high risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5091261</td>
      <td>F</td>
      <td>N</td>
      <td>Y</td>
      <td>0</td>
      <td>202500.00</td>
      <td>State servant</td>
      <td>Secondary / secondary special</td>
      <td>Separated</td>
      <td>House / apartment</td>
      <td>-16834</td>
      <td>-1692</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Medicine staff</td>
      <td>1.00</td>
      <td>-6.00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5096963</td>
      <td>M</td>
      <td>Y</td>
      <td>N</td>
      <td>0</td>
      <td>675000.00</td>
      <td>Commercial associate</td>
      <td>Higher education</td>
      <td>Married</td>
      <td>House / apartment</td>
      <td>-18126</td>
      <td>-948</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>Managers</td>
      <td>2.00</td>
      <td>-16.00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5087880</td>
      <td>F</td>
      <td>N</td>
      <td>N</td>
      <td>0</td>
      <td>234000.00</td>
      <td>State servant</td>
      <td>Higher education</td>
      <td>Civil marriage</td>
      <td>House / apartment</td>
      <td>-21967</td>
      <td>-5215</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>Core staff</td>
      <td>2.00</td>
      <td>-52.00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5021949</td>
      <td>F</td>
      <td>Y</td>
      <td>Y</td>
      <td>0</td>
      <td>445500.00</td>
      <td>Commercial associate</td>
      <td>Higher education</td>
      <td>Married</td>
      <td>House / apartment</td>
      <td>-12477</td>
      <td>-456</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Managers</td>
      <td>2.00</td>
      <td>-54.00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5105705</td>
      <td>F</td>
      <td>Y</td>
      <td>N</td>
      <td>0</td>
      <td>225000.00</td>
      <td>Working</td>
      <td>Secondary / secondary special</td>
      <td>Married</td>
      <td>Municipal apartment</td>
      <td>-12155</td>
      <td>-667</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Laborers</td>
      <td>2.00</td>
      <td>-48.00</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>We pass to the scikit-learn pipeline the test set as we did before for the training set to obtain a preprocessed dataset ready for our model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cc_test_prep</span> <span class="o">=</span> <span class="n">full_pipeline</span><span class="p">(</span><span class="n">cc_test_copy</span><span class="p">)</span>
</code></pre></div></div>

<p>We extract the independent variables/features and the target variable and store them into variables <code class="language-plaintext highlighter-rouge">X_cc_test_prep</code> and <code class="language-plaintext highlighter-rouge">y_cc_test_prep</code> respectively.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># split the train data into X and y (target)
</span><span class="n">X_cc_test_prep</span><span class="p">,</span> <span class="n">y_cc_test_prep</span> <span class="o">=</span> <span class="n">cc_test_prep</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">cc_test_prep</span><span class="p">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s">'Is high risk'</span><span class="p">],</span> <span class="n">cc_test_prep</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="s">'int64'</span><span class="p">)</span>
</code></pre></div></div>

<p>Next, we train the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># train the model
</span><span class="n">model_trn</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">classifiers</span><span class="p">[</span><span class="s">'gradient_boosting'</span><span class="p">],</span><span class="s">'gradient_boosting'</span><span class="p">)</span>
</code></pre></div></div>

<p>Then predict the dependent variable (predicted target) and store the prediction in the <code class="language-plaintext highlighter-rouge">final_prediction</code> variable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">final_predictions</span> <span class="o">=</span> <span class="n">model_trn</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_cc_test_prep</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we use the <code class="language-plaintext highlighter-rouge">shape</code> method to get the number of rows and columns.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">final_predictions</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(11654,)
</code></pre></div></div>

<p>We use the <code class="language-plaintext highlighter-rouge">sum</code> function to compare the predictions and actual target values. We store the count of the correct predictions in <code class="language-plaintext highlighter-rouge">n_correct</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_correct</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">final_predictions</span> <span class="o">==</span> <span class="n">y_cc_test_prep</span><span class="p">)</span>
</code></pre></div></div>

<p>We divide the number of correct predictions by the total number of predictions to get the accuracy. We achieved 85% accuracy on the testing set, which is very good! :)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">n_correct</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">final_predictions</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.8579028659687661
</code></pre></div></div>

<h3>Deploying the model on AWS S3</h3>

<p>Now we will deploy the gradient boosting model we previously saved on our local machine to AWS S3, but what is an AWS S3 bucket, we may ask?</p>

<p>AWS S3 (S3 stands for Simple Storage Service) is a cloud storage service that provides access to affordable data storage in the cloud. Our trained gradient boosting model stored on S3 can be accessed with access and secret access keys.</p>

<p>Now, let’s store the gradient boosting model on AWS S3, but you must create an AWS account first. AWS has a free tier subscription, and hosting this model on an S3 bucket is free of charge; also, remember to create an account as a root user. After creating an account on AWS, sign in as a root user and type on the search bar s3.</p>

<p><img src="/blog/assets/post_cont_image/search_bar_s3.png" alt="search bar s3" /></p>

<p>You should see a dropdown menu; click on the first option with a green bucket logo.</p>

<p>It will take you to the Amazon s3 landing page, and click the Create bucket button.</p>

<p><img src="/blog/assets/post_cont_image/create_bucket.png" alt="create a bucket" /></p>

<p>You will be prompted with this page.</p>

<p><img src="/blog/assets/post_cont_image/create_bucket_page.png" alt="create bucket page" /></p>

<p>Give the bucket a name; in this case, we can call our bucket name creditcardapproval; in one word, select an AWS region close to your location for better latency. We will keep the default option for the rest, then hit the create bucket button.</p>

<p>We see the bucket we just created in the list of buckets on the S3 landing page. Click on that bucket name, and you shall see the page below.</p>

<p><img src="/blog/assets/post_cont_image/upload_bucket.png" alt="upload to bucket" /></p>

<p>Click on the upload button, which will prompt you to another page. Click on add files, locate our saved model, hit the upload button and wait for it to upload to the bucket.</p>

<p><img src="/blog/assets/post_cont_image/add_bucket.png" alt="add the file to the bucket" /></p>

<p>Our model is uploaded on AWS. The status should be successful if everything goes well, like the image below.</p>

<p><img src="/blog/assets/post_cont_image/succeeded_bucket.png" alt="succeeded uploaded bucket" /></p>

<p>We have our model uploaded on the S3; we can now access it and make a prediction using access and secret access keys. So how do we get those two keys? We use IAM user and we need to create one.</p>

<p>Search for iam and click on users.</p>

<p><img src="/blog/assets/post_cont_image/users_iam.png" alt="search iam" /></p>

<p>Assuming you don’t have any IAM users, you must create one by clicking on the add users.</p>

<p>Note: I already have mine created, so I will add a new IAM user to show you how to get the keys because we can only access the secret access key once after creating a new user. Once it is created, you can no longer access the secret access key. So keep it private and store it in a safe place.</p>

<p><img src="/blog/assets/post_cont_image/add_iam_user.png" alt="add IAM user" /></p>

<p>Give it a name, let’s say <code class="language-plaintext highlighter-rouge">stern-test</code> or whatever you want. Check the access key checkbox so we can access our s3 blob storage API; now let’s go to the next step, which is the permissions.</p>

<p><img src="/blog/assets/post_cont_image/user_details.png" alt="IAM user detail" /></p>

<p>We will attach existing policies directly for the permission page. The existing permission we will be using are <code class="language-plaintext highlighter-rouge">AmazonS3FullAccess</code> and <code class="language-plaintext highlighter-rouge">AWSCompromisedKeyQuarantineV2</code> and check the corresponding checkbox. We will set the user without the permission boundary.</p>

<p><img src="/blog/assets/post_cont_image/permissions_iam.png" alt="permission iam" /></p>

<p>The next page is the tags page. IAM tags are key-value pairs you can add to your user. Tags can include user information, such as an email address, or can be descriptive, such as a job title. You can use the tags to organize, track, or control access for this user. Tags are optional, so it is up to you if you want to set them or not. I did not use them on my end since it is not helpful for this project. Press next for the review of the IAM user.</p>

<p>The review page is just a summary of the previous pages. Once you have reviewed it and satisfy with it, create the user.</p>

<p>Now comes the most crucial page; once the user is created, you will be prompted with the user name, the access and the secret access key. These keys will be used when linking our Streamlit web app with the hosted model on AWS. You can download the two keys as CSV files or copy them on your clipboard.</p>

<p>Note: This is the only time AWS will give you access to the secret access key for security purposes. You must create a new IAM user if you lose the secret access key. Please don’t share the keys; copy/save them in a safe place.</p>

<p>Now that you have saved your access and secret access key, you can close the page, and if you go back to the IAM welcome page, you can see the user you just created.</p>

<p><img src="/blog/assets/post_cont_image/final_iam.png" alt="final page iam user" /></p>

<p><img src="/blog/assets/post_cont_image/confirmation-iam.png" alt="user creation confirmation" /></p>

<p>With our model stored on S3 and the two keys in our possession. We are good to go to our last two sections with Streamlit.</p>

<h3>Streamlit Web Interface</h3>

<p>So we have our trained model stored on AWS S3. We need an interface for the model where someone can input their information in a sort of form (which is the profile to predict) and see if they will be approved for a credit card or not.</p>

<p>While working on this project, I encountered an issue with how to prepare the applicant data (feature selection, engineering and data preprocessing). I encountered errors that I could not figure out how to solve. It got exacerbated due to the fact Streamlit does not support jupyter notebooks (.ipynb), only support python files (.py)</p>

<p>To overcome this issue, I appended the applicant’s profile to the training data and did all the data preprocessing with the training data in one python script (with the Streamlit interface code), then extracted the last row, which corresponds to our applicant.</p>

<p>Note: I appended the applicant profile to the training dataset but did not retrain the model (which may result in overfitting the model). I only did the data preprocessing, as we will see shortly.</p>

<p>The following code is part of a python script (saved as .py) used for the Streamlit web interface deployment. We will go through what each session does but won’t explain the data preprocessing part in detail because most of the functions are the same as those from the sections above.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># libraries we have already seen
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">MinMaxScaler</span><span class="p">,</span> <span class="n">OrdinalEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">SMOTE</span>
<span class="kn">import</span> <span class="nn">joblib</span>
<span class="c1"># new libraries we have not seen
</span><span class="kn">import</span> <span class="nn">streamlit</span> <span class="k">as</span> <span class="n">st</span>
<span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">streamlit_lottie</span> <span class="kn">import</span> <span class="n">st_lottie_spinner</span>

</code></pre></div></div>

<p>We have already seen the first libraries in the script above; they are all the same. The second parts are libraries we have not seen yet.</p>

<ul>
  <li>Streamlit is a fantastic library that creates an interface for our model, and very easy to deploy using the streamlit share free service</li>
  <li>Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, allowing Python developers to write software that uses services like Amazon S3 and Amazon EC2. In this project, we will use it to connect our interface to the trained model on AWS S3 through the access and secret access key.</li>
  <li>tempfile is a module that creates temporary files and directories. In this project, it is used to store our trained model temporally in this python script</li>
  <li>json is used here for the streamlit hand animation while the model is predicting (This library is optional since it is for the animation and does not affect any way our predictions)</li>
  <li>request is used to get the animation from the server using HTTP request (This library is optional too)</li>
  <li>streamlit_lottie is the animation library for streamlit (This library is optional as well)</li>
</ul>

<p>We will quickly skim through the next section; if you forgot what each function does, feel free to refer to the sessions above.</p>

<p>So we will import the training and testing data directly as a raw file from Github.</p>

<p>Note: This data already has the target feature.</p>

<p>So now, we concatenate the training and testing on the row axis, do a resampling(reshuffling), and split the data (80% for the training data and 20% for the testing data). We make a copy of them and store them in <code class="language-plaintext highlighter-rouge">train_copy</code> and <code class="language-plaintext highlighter-rouge">test_copy</code>variables.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_original</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/semasuka/Credit-card-approval-prediction-classification/main/datasets/train.csv'</span><span class="p">)</span>

<span class="n">test_original</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/semasuka/Credit-card-approval-prediction-classification/main/datasets/test.csv'</span><span class="p">)</span>

<span class="n">full_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">train_original</span><span class="p">,</span> <span class="n">test_original</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">full_data</span> <span class="o">=</span> <span class="n">full_data</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">data_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="p">):</span>
    <span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_df</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">test_df</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>


<span class="n">train_original</span><span class="p">,</span> <span class="n">test_original</span> <span class="o">=</span> <span class="n">data_split</span><span class="p">(</span><span class="n">full_data</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>

<span class="n">train_copy</span> <span class="o">=</span> <span class="n">train_original</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">test_copy</span> <span class="o">=</span> <span class="n">test_original</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
</code></pre></div></div>

<p>After this, we reuse the same functions and classes we used for the data preprocessing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">value_cnt_norm_cal</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">feature</span><span class="p">):</span>
    <span class="s">'''Function that will return the value count and frequency of each observation within a feature'''</span>
    <span class="c1"># get the value counts of each feature
</span>    <span class="n">ftr_value_cnt</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">].</span><span class="n">value_counts</span><span class="p">()</span>
    <span class="c1"># normalize the value counts on a scale of 100
</span>    <span class="n">ftr_value_cnt_norm</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">feature</span><span class="p">].</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="c1"># concatenate the value counts with normalized value count column wise
</span>    <span class="n">ftr_value_cnt_concat</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">ftr_value_cnt</span><span class="p">,</span> <span class="n">ftr_value_cnt_norm</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># give it a column name
</span>    <span class="n">ftr_value_cnt_concat</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Count'</span><span class="p">,</span> <span class="s">'Frequency (%)'</span><span class="p">]</span>
    <span class="c1"># return the dataframe
</span>    <span class="k">return</span> <span class="n">ftr_value_cnt_concat</span>


<span class="k">class</span> <span class="nc">OutlierRemover</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feat_with_outliers</span><span class="o">=</span><span class="p">[</span><span class="s">'Family member count'</span><span class="p">,</span> <span class="s">'Income'</span><span class="p">,</span> <span class="s">'Employment length'</span><span class="p">]):</span>
        <span class="c1"># initializing the instance of the object
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">feat_with_outliers</span> <span class="o">=</span> <span class="n">feat_with_outliers</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="c1"># check if the feature in part of the dataset's features
</span>        <span class="k">if</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_outliers</span><span class="p">).</span><span class="n">issubset</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="c1"># 25% quantile
</span>            <span class="n">Q1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_outliers</span><span class="p">].</span><span class="n">quantile</span><span class="p">(.</span><span class="mi">25</span><span class="p">)</span>
            <span class="c1"># 75% quantile
</span>            <span class="n">Q3</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_outliers</span><span class="p">].</span><span class="n">quantile</span><span class="p">(.</span><span class="mi">75</span><span class="p">)</span>
            <span class="n">IQR</span> <span class="o">=</span> <span class="n">Q3</span> <span class="o">-</span> <span class="n">Q1</span>
            <span class="c1"># keep the data within 3 IQR only and discard the rest
</span>            <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="o">~</span><span class="p">((</span><span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_outliers</span><span class="p">]</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">Q1</span> <span class="o">-</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">IQR</span><span class="p">))</span> <span class="o">|</span>
                      <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_outliers</span><span class="p">]</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">Q3</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">IQR</span><span class="p">))).</span><span class="nb">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"One or more features are not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>


<span class="k">class</span> <span class="nc">DropFeatures</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_to_drop</span><span class="o">=</span><span class="p">[</span><span class="s">'ID'</span><span class="p">,</span> <span class="s">'Has a mobile phone'</span><span class="p">,</span> <span class="s">'Children count'</span><span class="p">,</span> <span class="s">'Job title'</span><span class="p">,</span> <span class="s">'Account age'</span><span class="p">]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">feature_to_drop</span> <span class="o">=</span> <span class="n">feature_to_drop</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">feature_to_drop</span><span class="p">).</span><span class="n">issubset</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="c1"># drop the list of features
</span>            <span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">feature_to_drop</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"One or more features are not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>


<span class="k">class</span> <span class="nc">TimeConversionHandler</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feat_with_days</span><span class="o">=</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">,</span> <span class="s">'Age'</span><span class="p">]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">feat_with_days</span> <span class="o">=</span> <span class="n">feat_with_days</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_days</span><span class="p">).</span><span class="n">issubset</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="c1"># convert days to absolute value using NumPy
</span>            <span class="n">X</span><span class="p">[[</span><span class="s">'Employment length'</span><span class="p">,</span> <span class="s">'Age'</span><span class="p">]]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span>
                <span class="n">X</span><span class="p">[[</span><span class="s">'Employment length'</span><span class="p">,</span> <span class="s">'Age'</span><span class="p">]])</span>
            <span class="k">return</span> <span class="n">X</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"One or more features are not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">X</span>


<span class="k">class</span> <span class="nc">RetireeHandler</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="s">'Employment length'</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="c1"># select rows with an employment length is 365243, which corresponds to retirees
</span>            <span class="n">df_ret_idx</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">][</span><span class="n">df</span><span class="p">[</span><span class="s">'Employment length'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">365243</span><span class="p">].</span><span class="n">index</span>
            <span class="c1"># set those rows with value 365243 to 0
</span>            <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_ret_idx</span><span class="p">,</span> <span class="s">'Employment length'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Employment length is not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>


<span class="k">class</span> <span class="nc">SkewnessHandler</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feat_with_skewness</span><span class="o">=</span><span class="p">[</span><span class="s">'Income'</span><span class="p">,</span> <span class="s">'Age'</span><span class="p">]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">feat_with_skewness</span> <span class="o">=</span> <span class="n">feat_with_skewness</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_skewness</span><span class="p">).</span><span class="n">issubset</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="c1"># Handle skewness with cubic root transformation
</span>            <span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_skewness</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cbrt</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_skewness</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"One or more features are not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>


<span class="k">class</span> <span class="nc">BinningNumToYN</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feat_with_num_enc</span><span class="o">=</span><span class="p">[</span><span class="s">'Has a work phone'</span><span class="p">,</span> <span class="s">'Has a phone'</span><span class="p">,</span> <span class="s">'Has an email'</span><span class="p">]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">feat_with_num_enc</span> <span class="o">=</span> <span class="n">feat_with_num_enc</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">feat_with_num_enc</span><span class="p">).</span><span class="n">issubset</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="c1"># Change 0 to N and 1 to Y for all the features in feat_with_num_enc
</span>            <span class="k">for</span> <span class="n">ft</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">feat_with_num_enc</span><span class="p">:</span>
                <span class="n">df</span><span class="p">[</span><span class="n">ft</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">ft</span><span class="p">].</span><span class="nb">map</span><span class="p">({</span><span class="mi">1</span><span class="p">:</span> <span class="s">'Y'</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span> <span class="s">'N'</span><span class="p">})</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"One or more features are not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>


<span class="k">class</span> <span class="nc">OneHotWithFeatNames</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">one_hot_enc_ft</span><span class="o">=</span><span class="p">[</span><span class="s">'Gender'</span><span class="p">,</span> <span class="s">'Marital status'</span><span class="p">,</span> <span class="s">'Dwelling'</span><span class="p">,</span> <span class="s">'Employment status'</span><span class="p">,</span> <span class="s">'Has a car'</span><span class="p">,</span> <span class="s">'Has a property'</span><span class="p">,</span> <span class="s">'Has a work phone'</span><span class="p">,</span> <span class="s">'Has a phone'</span><span class="p">,</span> <span class="s">'Has an email'</span><span class="p">]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">one_hot_enc_ft</span> <span class="o">=</span> <span class="n">one_hot_enc_ft</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">one_hot_enc_ft</span><span class="p">).</span><span class="n">issubset</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="c1"># function to one-hot encode the features
</span>            <span class="k">def</span> <span class="nf">one_hot_enc</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">one_hot_enc_ft</span><span class="p">):</span>
                <span class="c1"># instantiate the OneHotEncoder object
</span>                <span class="n">one_hot_enc</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
                <span class="c1"># fit the dataframe with the features we want to one-hot encode
</span>                <span class="n">one_hot_enc</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">one_hot_enc_ft</span><span class="p">])</span>
                <span class="c1"># get output feature names for transformation.
</span>                <span class="n">feat_names_one_hot_enc</span> <span class="o">=</span> <span class="n">one_hot_enc</span><span class="p">.</span><span class="n">get_feature_names_out</span><span class="p">(</span>
                    <span class="n">one_hot_enc_ft</span><span class="p">)</span>
                <span class="c1"># change the one hot encoding array to a dataframe with the column names
</span>                <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">one_hot_enc</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">one_hot_enc_ft</span><span class="p">]).</span><span class="n">toarray</span><span class="p">(</span>
                <span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">feat_names_one_hot_enc</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">df</span>
            <span class="c1"># function to concatenate the one hot encoded features with the rest of the features that were not encoded
</span>
            <span class="k">def</span> <span class="nf">concat_with_rest</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">one_hot_enc_df</span><span class="p">,</span> <span class="n">one_hot_enc_ft</span><span class="p">):</span>
                <span class="c1"># get the rest of the features that are not encoded
</span>                <span class="n">rest_of_features</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">ft</span> <span class="k">for</span> <span class="n">ft</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">ft</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">one_hot_enc_ft</span><span class="p">]</span>
                <span class="c1"># concatenate the rest of the features with the one hot encoded features
</span>                <span class="n">df_concat</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">one_hot_enc_df</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="n">rest_of_features</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">df_concat</span>
            <span class="c1"># call the one_hot_enc function and stores the dataframe in the one_hot_enc_df variable
</span>            <span class="n">one_hot_enc_df</span> <span class="o">=</span> <span class="n">one_hot_enc</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">one_hot_enc_ft</span><span class="p">)</span>
            <span class="c1"># returns the concatenated dataframe and stores it in the full_df_one_hot_enc variable
</span>            <span class="n">full_df_one_hot_enc</span> <span class="o">=</span> <span class="n">concat_with_rest</span><span class="p">(</span>
                <span class="n">df</span><span class="p">,</span> <span class="n">one_hot_enc_df</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">one_hot_enc_ft</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">full_df_one_hot_enc</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"One or more features are not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>


<span class="k">class</span> <span class="nc">OrdinalFeatNames</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ordinal_enc_ft</span><span class="o">=</span><span class="p">[</span><span class="s">'Education level'</span><span class="p">]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ordinal_enc_ft</span> <span class="o">=</span> <span class="n">ordinal_enc_ft</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="s">'Education level'</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="c1"># instantiate the OrdinalEncoder object
</span>            <span class="n">ordinal_enc</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
            <span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">ordinal_enc_ft</span><span class="p">]</span> <span class="o">=</span> <span class="n">ordinal_enc</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span>
                <span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">ordinal_enc_ft</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Education level is not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>


<span class="k">class</span> <span class="nc">MinMaxWithFeatNames</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_max_scaler_ft</span><span class="o">=</span><span class="p">[</span><span class="s">'Age'</span><span class="p">,</span> <span class="s">'Income'</span><span class="p">,</span> <span class="s">'Employment length'</span><span class="p">]):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">min_max_scaler_ft</span> <span class="o">=</span> <span class="n">min_max_scaler_ft</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">min_max_scaler_ft</span><span class="p">).</span><span class="n">issubset</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)):</span>
            <span class="c1"># instantiate the MinMaxScaler object
</span>            <span class="n">min_max_enc</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
            <span class="c1"># fit and transform on a scale 0 to 1
</span>            <span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">min_max_scaler_ft</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_max_enc</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span>
                <span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">min_max_scaler_ft</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"One or more features are not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>


<span class="k">class</span> <span class="nc">ChangeToNumTarget</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="c1"># check if the target is part of the dataframe
</span>        <span class="k">if</span> <span class="s">'Is high risk'</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="c1"># change to a numeric data type using Pandas
</span>            <span class="n">df</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">df</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Is high risk is not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>


<span class="k">class</span> <span class="nc">Oversample</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="k">if</span> <span class="s">'Is high risk'</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="c1"># smote function instantiation to oversample the minority class to fix the imbalance data
</span>            <span class="n">oversample</span> <span class="o">=</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="s">'minority'</span><span class="p">)</span>
            <span class="c1"># fit and resample the classes and assign them to X_bal, y_bal variable
</span>            <span class="n">X_bal</span><span class="p">,</span> <span class="n">y_bal</span> <span class="o">=</span> <span class="n">oversample</span><span class="p">.</span><span class="n">fit_resample</span><span class="p">(</span>
                <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s">'Is high risk'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'Is high risk'</span><span class="p">])</span>
            <span class="c1"># concatenate the balanced classes column-wise
</span>            <span class="n">df_bal</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span>
                <span class="p">[</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_bal</span><span class="p">),</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_bal</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df_bal</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Is high risk is not in the dataframe"</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">df</span>


<span class="k">def</span> <span class="nf">full_pipeline</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="c1"># Create the pipeline that will call all the classes from OutlierRemoval() to Oversample() in one go
</span>    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s">'outlier_remover'</span><span class="p">,</span> <span class="n">OutlierRemover</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'feature_dropper'</span><span class="p">,</span> <span class="n">DropFeatures</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'time_conversion_handler'</span><span class="p">,</span> <span class="n">TimeConversionHandler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'retiree_handler'</span><span class="p">,</span> <span class="n">RetireeHandler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'skewness_handler'</span><span class="p">,</span> <span class="n">SkewnessHandler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'binning_num_to_yn'</span><span class="p">,</span> <span class="n">BinningNumToYN</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'one_hot_with_feat_names'</span><span class="p">,</span> <span class="n">OneHotWithFeatNames</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'ordinal_feat_names'</span><span class="p">,</span> <span class="n">OrdinalFeatNames</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'min_max_with_feat_names'</span><span class="p">,</span> <span class="n">MinMaxWithFeatNames</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'change_to_num_target'</span><span class="p">,</span> <span class="n">ChangeToNumTarget</span><span class="p">()),</span>
        <span class="p">(</span><span class="s">'oversample'</span><span class="p">,</span> <span class="n">Oversample</span><span class="p">())</span>
    <span class="p">])</span>
    <span class="n">df_pipe_prep</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df_pipe_prep</span>

</code></pre></div></div>

<p>Now let’s work on the Streamlit interface/dashboard.</p>

<p>We start by creating a title and a brief description of our interface and what it does. The streamlit function <code class="language-plaintext highlighter-rouge">st.write</code> will accept within the parentheses markdown markup language. So that first line that starts with <code class="language-plaintext highlighter-rouge">#</code> is equivalent to HTML heading H1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
# Credit card approval prediction
This app predicts if an applicant will be approved for a credit card or not. Just fill in the following information and click on the Predict button.
"""</span><span class="p">)</span>

</code></pre></div></div>

<p>The first input from the applicant is gender, and use the streamlit radio button function to choose between two options. We store the output in the <code class="language-plaintext highlighter-rouge">input_gender</code> variable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Gender input
</span><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
## Gender
"""</span><span class="p">)</span>
<span class="n">input_gender</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="n">radio</span><span class="p">(</span><span class="s">'Select you gender'</span><span class="p">,[</span><span class="s">'Male'</span><span class="p">,</span><span class="s">'Female'</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>For age, we will use a slider instead, with a maximum value of 70 and a minimum value of 18, with one step at a time. We are then changing the age to days by multiplying it with 365.25, as we did in the sessions above.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Age input slider
</span><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
## Age
"""</span><span class="p">)</span>
<span class="n">input_age</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">negative</span><span class="p">(</span><span class="n">st</span><span class="p">.</span><span class="n">slider</span><span class="p">(</span>
    <span class="s">'Select your age'</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">365.25</span><span class="p">)</span>

</code></pre></div></div>

<p>We use a drop-down for marital status. Each marital status string value is mapped to an index to create a dictionary to return that string value.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Marital status input dropdown
</span><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
## Marital status
"""</span><span class="p">)</span>
<span class="c1"># get the index from value_cnt_norm_cal function
</span><span class="n">marital_status_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
    <span class="n">value_cnt_norm_cal</span><span class="p">(</span><span class="n">full_data</span><span class="p">,</span> <span class="s">'Marital status'</span><span class="p">).</span><span class="n">index</span><span class="p">)</span>
<span class="n">marital_status_key</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Married'</span><span class="p">,</span> <span class="s">'Single/not married'</span><span class="p">,</span> <span class="s">'Civil marriage'</span><span class="p">,</span> <span class="s">'Separated'</span><span class="p">,</span> <span class="s">'Widowed'</span><span class="p">]</span>
<span class="c1"># mapping of the values and keys
</span><span class="n">marital_status_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">marital_status_key</span><span class="p">,</span> <span class="n">marital_status_values</span><span class="p">))</span>
<span class="c1"># streamlit dropdown menu function, value stored in input_marital_status_key
</span><span class="n">input_marital_status_key</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="n">selectbox</span><span class="p">(</span>
    <span class="s">'Select your marital status'</span><span class="p">,</span> <span class="n">marital_status_key</span><span class="p">)</span>

<span class="c1"># get the corresponding value
</span><span class="n">input_marital_status_val</span> <span class="o">=</span> <span class="n">marital_status_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">input_marital_status_key</span><span class="p">)</span>

</code></pre></div></div>

<p>We again get the family count using streamlit dropdown menu.</p>

<p>Note: since we have removed outliers from our training model, we will only have the family count up to 6, which encompass most scenario.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Family member count
</span><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
## Family member count
"""</span><span class="p">)</span>
<span class="n">fam_member_count</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">st</span><span class="p">.</span><span class="n">selectbox</span><span class="p">(</span><span class="s">'Select your family member count'</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]))</span>
</code></pre></div></div>

<p>We use a dropdown menu for dwelling type just like we did for Marital status.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Dwelling type dropdown
</span><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
## Dwelling type
"""</span><span class="p">)</span>
<span class="n">dwelling_type_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">value_cnt_norm_cal</span><span class="p">(</span><span class="n">full_data</span><span class="p">,</span> <span class="s">'Dwelling'</span><span class="p">).</span><span class="n">index</span><span class="p">)</span>
<span class="n">dwelling_type_key</span> <span class="o">=</span> <span class="p">[</span><span class="s">'House / apartment'</span><span class="p">,</span> <span class="s">'Live with parents'</span><span class="p">,</span> <span class="s">'Municipal apartment '</span><span class="p">,</span> <span class="s">'Rented apartment'</span><span class="p">,</span> <span class="s">'Office apartment'</span><span class="p">,</span> <span class="s">'Co-op apartment'</span><span class="p">]</span>
<span class="n">dwelling_type_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dwelling_type_key</span><span class="p">,</span> <span class="n">dwelling_type_values</span><span class="p">))</span>
<span class="n">input_dwelling_type_key</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="n">selectbox</span><span class="p">(</span>
    <span class="s">'Select the type of dwelling you reside in'</span><span class="p">,</span> <span class="n">dwelling_type_key</span><span class="p">)</span>
<span class="n">input_dwelling_type_val</span> <span class="o">=</span> <span class="n">dwelling_type_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">input_dwelling_type_key</span><span class="p">)</span>

</code></pre></div></div>

<p>For income, we will input income value in a text field.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Income
</span><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
## Income
"""</span><span class="p">)</span>
<span class="n">input_income</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">int</span><span class="p">(</span><span class="n">st</span><span class="p">.</span><span class="n">text_input</span><span class="p">(</span><span class="s">'Enter your income (in USD)'</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>

<p>We will proceed the same way for employment status as marital status.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Employment status dropdown
</span><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
## Employment status
"""</span><span class="p">)</span>
<span class="n">employment_status_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
    <span class="n">value_cnt_norm_cal</span><span class="p">(</span><span class="n">full_data</span><span class="p">,</span> <span class="s">'Employment status'</span><span class="p">).</span><span class="n">index</span><span class="p">)</span>
<span class="n">employment_status_key</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">'Working'</span><span class="p">,</span> <span class="s">'Commercial associate'</span><span class="p">,</span> <span class="s">'Pensioner'</span><span class="p">,</span> <span class="s">'State servant'</span><span class="p">,</span> <span class="s">'Student'</span><span class="p">]</span>
<span class="n">employment_status_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="nb">zip</span><span class="p">(</span><span class="n">employment_status_key</span><span class="p">,</span> <span class="n">employment_status_values</span><span class="p">))</span>
<span class="n">input_employment_status_key</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="n">selectbox</span><span class="p">(</span>
    <span class="s">'Select your employment status'</span><span class="p">,</span> <span class="n">employment_status_key</span><span class="p">)</span>
<span class="n">input_employment_status_val</span> <span class="o">=</span> <span class="n">employment_status_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span>
    <span class="n">input_employment_status_key</span><span class="p">)</span>

</code></pre></div></div>

<p>We use a slider for the employment length.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Employment length input slider
</span><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
## Employment length
"""</span><span class="p">)</span>
<span class="n">input_employment_length</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">negative</span><span class="p">(</span><span class="n">st</span><span class="p">.</span><span class="n">slider</span><span class="p">(</span>
    <span class="s">'Select your employment length'</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">365.25</span><span class="p">)</span>

</code></pre></div></div>

<p>Again, we use a dropdown for the education level.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Education level dropdown
</span><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
## Education level
"""</span><span class="p">)</span>
<span class="n">edu_level_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">value_cnt_norm_cal</span><span class="p">(</span><span class="n">full_data</span><span class="p">,</span> <span class="s">'Education level'</span><span class="p">).</span><span class="n">index</span><span class="p">)</span>
<span class="n">edu_level_key</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Secondary school'</span><span class="p">,</span> <span class="s">'Higher education'</span><span class="p">,</span> <span class="s">'Incomplete higher'</span><span class="p">,</span> <span class="s">'Lower secondary'</span><span class="p">,</span> <span class="s">'Academic degree'</span><span class="p">]</span>
<span class="n">edu_level_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">edu_level_key</span><span class="p">,</span> <span class="n">edu_level_values</span><span class="p">))</span>
<span class="n">input_edu_level_key</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="n">selectbox</span><span class="p">(</span>
    <span class="s">'Select your education status'</span><span class="p">,</span> <span class="n">edu_level_key</span><span class="p">)</span>
<span class="n">input_edu_level_val</span> <span class="o">=</span> <span class="n">edu_level_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">input_edu_level_key</span><span class="p">)</span>

</code></pre></div></div>

<p>We use the <code class="language-plaintext highlighter-rouge">st.radio</code> streamlit function (radio button select only one input between two choices) for car ownership feature, property ownership, work phone input, phone input, and email input.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Car ownship input
</span><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
## Car ownship
"""</span><span class="p">)</span>
<span class="n">input_car_ownship</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="n">radio</span><span class="p">(</span><span class="s">'Do you own a car?'</span><span class="p">,</span> <span class="p">[</span><span class="s">'Yes'</span><span class="p">,</span> <span class="s">'No'</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Property ownship input
</span><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
## Property ownship
"""</span><span class="p">)</span>
<span class="n">input_prop_ownship</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="n">radio</span><span class="p">(</span><span class="s">'Do you own a property?'</span><span class="p">,</span> <span class="p">[</span><span class="s">'Yes'</span><span class="p">,</span> <span class="s">'No'</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>


<span class="c1"># Work phone input
</span><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
## Work phone
"""</span><span class="p">)</span>
<span class="n">input_work_phone</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="n">radio</span><span class="p">(</span>
    <span class="s">'Do you have a work phone?'</span><span class="p">,</span> <span class="p">[</span><span class="s">'Yes'</span><span class="p">,</span> <span class="s">'No'</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">work_phone_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s">'Yes'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'No'</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="n">work_phone_val</span> <span class="o">=</span> <span class="n">work_phone_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">input_work_phone</span><span class="p">)</span>

<span class="c1"># Phone input
</span><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
## Phone
"""</span><span class="p">)</span>
<span class="n">input_phone</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="n">radio</span><span class="p">(</span><span class="s">'Do you have a phone?'</span><span class="p">,</span> <span class="p">[</span><span class="s">'Yes'</span><span class="p">,</span> <span class="s">'No'</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">work_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s">'Yes'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'No'</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="n">phone_val</span> <span class="o">=</span> <span class="n">work_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">input_phone</span><span class="p">)</span>

<span class="c1"># Email input
</span><span class="n">st</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"""
## Email
"""</span><span class="p">)</span>
<span class="n">input_email</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="n">radio</span><span class="p">(</span><span class="s">'Do you have an email?'</span><span class="p">,</span> <span class="p">[</span><span class="s">'Yes'</span><span class="p">,</span> <span class="s">'No'</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">email_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s">'Yes'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'No'</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="n">email_val</span> <span class="o">=</span> <span class="n">email_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">input_email</span><span class="p">)</span>

</code></pre></div></div>

<p>The final element on the interface is the predict button.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Predict button
</span><span class="n">predict_bt</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="n">button</span><span class="p">(</span><span class="s">'Predict'</span><span class="p">)</span>
</code></pre></div></div>

<p>So now that we have the interface ready and all the input variables, we can store those input variables in a list which will be the profile we are predicting.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># list of all the input variables
</span><span class="n">profile_to_predict</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># ID (which will be dropped in the pipeline)
</span>                    <span class="n">input_gender</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># get the first element in gender
</span>                    <span class="n">input_car_ownship</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># get the first element in car ownership
</span>                    <span class="n">input_prop_ownship</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># get the first element in property ownership
</span>                    <span class="mi">0</span><span class="p">,</span> <span class="c1"># Children count (which will be dropped in the pipeline)
</span>                    <span class="n">input_income</span><span class="p">,</span>  <span class="c1"># Income
</span>                    <span class="n">input_employment_status_val</span><span class="p">,</span>  <span class="c1"># Employment status
</span>                    <span class="n">input_edu_level_val</span><span class="p">,</span>  <span class="c1"># Education level
</span>                    <span class="n">input_marital_status_val</span><span class="p">,</span>  <span class="c1"># Marital status
</span>                    <span class="n">input_dwelling_type_val</span><span class="p">,</span>  <span class="c1"># Dwelling type
</span>                    <span class="n">input_age</span><span class="p">,</span>  <span class="c1"># Age
</span>                    <span class="n">input_employment_length</span><span class="p">,</span>    <span class="c1"># Employment length
</span>                    <span class="mi">1</span><span class="p">,</span> <span class="c1"># Has a mobile phone (which will be dropped in the pipeline)
</span>                    <span class="n">work_phone_val</span><span class="p">,</span>  <span class="c1"># Work phone
</span>                    <span class="n">phone_val</span><span class="p">,</span>  <span class="c1"># Phone
</span>                    <span class="n">email_val</span><span class="p">,</span>  <span class="c1"># Email
</span>                    <span class="s">'to_be_droped'</span><span class="p">,</span> <span class="c1"># Job title (which will be dropped in the pipeline)
</span>                    <span class="n">fam_member_count</span><span class="p">,</span>  <span class="c1"># Family member count
</span>                    <span class="mf">0.00</span><span class="p">,</span> <span class="c1"># Account age (which will be dropped in the pipeline)
</span>                    <span class="mi">0</span>  <span class="c1"># target set to 0 as a placeholder
</span>                    <span class="p">]</span>

</code></pre></div></div>

<p>We will change the list into a one row dataframe.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">profile_to_predict_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">profile_to_predict</span><span class="p">],</span><span class="n">columns</span><span class="o">=</span><span class="n">train_copy</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
</code></pre></div></div>

<p>We will add the profile to predict as the last row in the train data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_copy_with_profile_to_pred</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">train_copy</span><span class="p">,</span><span class="n">profile_to_predict_df</span><span class="p">],</span><span class="n">ignore_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>We will prepare the whole dataset (profile to predict with the training dataset) with the <code class="language-plaintext highlighter-rouge">full_pipeline</code> function we have defined above.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># whole dataset prepared
</span><span class="n">train_copy_with_profile_to_pred_prep</span> <span class="o">=</span> <span class="n">full_pipeline</span><span class="p">(</span><span class="n">train_copy_with_profile_to_pred</span><span class="p">)</span>
</code></pre></div></div>

<p>To get our applicant profile observation, we first get the row with the ID = 0 and then drop the ID with the target (which was added as a placeholder) column.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">profile_to_pred_prep</span> <span class="o">=</span> <span class="n">train_copy_with_profile_to_pred_prep</span><span class="p">[</span><span class="n">train_copy_with_profile_to_pred_prep</span><span class="p">[</span><span class="s">'ID'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">].</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'ID'</span><span class="p">,</span><span class="s">'Is high risk'</span><span class="p">])</span>
</code></pre></div></div>

<p>Now we will add an optional but cool animation of an impatient hand that will be displayed when the model makes the prediction; here is what it looks like.</p>

<p><img src="/blog/assets/post_cont_image/hand_ani.png" alt="hand animation" /></p>

<p>Check out the animation in action <a href="https://lottiefiles.com/89308-loading-hand-green">here</a></p>

<p>And here is its function</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Animation function
</span><span class="o">@</span><span class="n">st</span><span class="p">.</span><span class="n">experimental_memo</span>
<span class="k">def</span> <span class="nf">load_lottieurl</span><span class="p">(</span><span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">r</span><span class="p">.</span><span class="n">status_code</span> <span class="o">!=</span> <span class="mi">200</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">None</span>
    <span class="k">return</span> <span class="n">r</span><span class="p">.</span><span class="n">json</span><span class="p">()</span>


<span class="n">lottie_loading_an</span> <span class="o">=</span> <span class="n">load_lottieurl</span><span class="p">(</span>
    <span class="s">'https://assets3.lottiefiles.com/packages/lf20_szlepvdh.json'</span><span class="p">)</span>

</code></pre></div></div>

<p>Last but not least, we will finally create a function to make predictions. We first get the client from AWS S3 using the <code class="language-plaintext highlighter-rouge">boto3.client</code> function and store it in the <code class="language-plaintext highlighter-rouge">client</code> variable.</p>

<p>Now you might ask, how are we passing the keys to this function, yet there is nowhere we pasted our access and secret access key. It will be done when we deploy to streamlit share in the sections below.</p>

<p>we declare our bucket and model name stored on AWS, then load the model from AWS into a temporally file using the <code class="language-plaintext highlighter-rouge">tempfile</code> library, download, load and return a prediction as <code class="language-plaintext highlighter-rouge">0</code> (is not high risk) or <code class="language-plaintext highlighter-rouge">1</code> (is high risk).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_prediction</span><span class="p">():</span>
    <span class="c1"># connect to s3 bucket with the access and secret access key
</span>    <span class="n">client</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">client</span><span class="p">(</span>
        <span class="s">'s3'</span><span class="p">,</span> <span class="n">aws_access_key_id</span><span class="o">=</span><span class="n">st</span><span class="p">.</span><span class="n">secrets</span><span class="p">[</span><span class="s">"access_key"</span><span class="p">],</span> <span class="n">aws_secret_access_key</span><span class="o">=</span><span class="n">st</span><span class="p">.</span><span class="n">secrets</span><span class="p">[</span><span class="s">"secret_access_key"</span><span class="p">])</span>

    <span class="n">bucket_name</span> <span class="o">=</span> <span class="s">"creditapplipred"</span>
    <span class="n">key</span> <span class="o">=</span> <span class="s">"gradient_boosting_model.sav"</span>

    <span class="c1"># load the model from s3 in a temporary file
</span>    <span class="k">with</span> <span class="n">tempfile</span><span class="p">.</span><span class="n">TemporaryFile</span><span class="p">()</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
        <span class="c1"># download our model from AWS
</span>        <span class="n">client</span><span class="p">.</span><span class="n">download_fileobj</span><span class="p">(</span><span class="n">Fileobj</span><span class="o">=</span><span class="n">fp</span><span class="p">,</span> <span class="n">Bucket</span><span class="o">=</span><span class="n">bucket_name</span><span class="p">,</span> <span class="n">Key</span><span class="o">=</span><span class="n">key</span><span class="p">)</span>
        <span class="c1"># change the position of the File Handle to the beginning of the file
</span>        <span class="n">fp</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># load the model using joblib library
</span>        <span class="n">model</span> <span class="o">=</span> <span class="n">joblib</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>

    <span class="c1"># prediction from the model, returns 0 or 1
</span>    <span class="k">return</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">profile_to_pred_prep</span><span class="p">)</span>

</code></pre></div></div>

<p>Let’s create an if statement that will call the function above only when someone clicks on the predict button. The following code will be executed only when <code class="language-plaintext highlighter-rouge">predict_bt</code> is = <code class="language-plaintext highlighter-rouge">1</code>, meaning when someone clicks the predict button.</p>

<p>The animation will run as long as the <code class="language-plaintext highlighter-rouge">make_prediction</code> function is running and will stop once the function has finished executing. If the result from the prediction is <code class="language-plaintext highlighter-rouge">0</code>, a green banner for success will be displayed with text that the applicant has been approved for a credit card; it is <code class="language-plaintext highlighter-rouge">1</code>, and a red banner will be displayed with the appropriate text.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">predict_bt</span><span class="p">:</span>
    <span class="c1"># will run the animation as long as the function is running, if final_pred exit, then stop displaying the loading animation
</span>    <span class="k">with</span> <span class="n">st_lottie_spinner</span><span class="p">(</span><span class="n">lottie_loading_an</span><span class="p">,</span> <span class="n">quality</span><span class="o">=</span><span class="s">'high'</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="s">'200px'</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="s">'200px'</span><span class="p">):</span>
        <span class="n">final_pred</span> <span class="o">=</span> <span class="n">make_prediction</span><span class="p">()</span>
    <span class="c1"># the prediction is 0
</span>    <span class="k">if</span> <span class="n">final_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># display a green banner for success
</span>        <span class="n">st</span><span class="p">.</span><span class="n">success</span><span class="p">(</span><span class="s">'## You have been approved for a credit card'</span><span class="p">)</span>
        <span class="c1"># display the streamlit ballon
</span>        <span class="n">st</span><span class="p">.</span><span class="n">balloons</span><span class="p">()</span>
    <span class="c1"># if prediction is 1
</span>    <span class="k">elif</span> <span class="n">final_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># display a red banner for error/failure
</span>        <span class="n">st</span><span class="p">.</span><span class="n">error</span><span class="p">(</span><span class="s">'## Unfortunately, you have not been approved for a credit card'</span><span class="p">)</span>

</code></pre></div></div>

<p>That is it, guys!! We have our Streamlit interface ready to go; now we need to deploy it on Streamlit share and share it with the world. What an exciting moment! :)</p>

<h3>Deployment to Streamlit share (free web hosting for the Streamlit Web interface)</h3>

<p>In this last session, we will deploy our web interface to Streamlit share. In other words, we are creating a front-end interface for our model through which the applicant can interact with our trained model.</p>

<p>Before deployment, we first need to store our Streamlit file on Github, where Streamlit can pick up the files from the Github repository.</p>

<p>Head on to Github, sign up for an account if you don’t already have one and create a new repository just like this.</p>

<p><img src="/blog/assets/post_cont_image/github_repo.png" alt="GitHub New Repo" /></p>

<p>We get the following page, give it a name and description (optional) and set the repository to be public so that Streamlit can read the file. And hit the create repository button.</p>

<p><img src="/blog/assets/post_cont_image/github_create_repo.png" alt="Github create repo" /></p>

<p>Drag and drop our streamlit python file in the area below.</p>

<p><img src="/blog/assets/post_cont_image/github_drag.png" alt="GitHub drag" /></p>

<p>Streamlit also needs a <code class="language-plaintext highlighter-rouge">requirements.txt</code> file to be added to the repo. This simple text file will inform Streamlit which version of the python libraries to install on the Streamlit servers.</p>

<p>Create a text file locally, copy past the information below, and then upload it to GitHub like the python file.</p>

<p>Note: it is crucial to name the file <code class="language-plaintext highlighter-rouge">requirements.txt</code> so that Streamlit can read it; otherwise, it won’t deploy.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>numpy==1.22.0
pandas==1.3.5
scikit-learn==1.0.2
imbalanced-learn==0.9.0
streamlit&gt;=1.8.1
boto3==1.20.34
joblib&gt;=0.11,&lt;=1.0.1
streamlit-lottie==0.0.3
</code></pre></div></div>

<p>So now we can commit the two files after giving them a brief description.</p>

<p><img src="/blog/assets/post_cont_image/Github_commit_change.png" alt="Github commit saved" /></p>

<p>Now let’s head to Streamlit share, <a href="https://share.streamlit.io/">here</a> is the link. You can sign up with your Google account.</p>

<p>After login in, you will land on this page. I already have three apps deployed here.</p>

<p><img src="/blog/assets/post_cont_image/streamlit_new_app.png" alt="New app" /></p>

<p>So on the deployment page, fill in the information below.</p>

<p>For the repository, you give it the username of your GitHub account, separated from the repository name by /
For the branch, it should default, which is the main branch
For main file path, it should be the Streamlit Python file name</p>

<p>Then click on the Advanced settings.</p>

<p><img src="/blog/assets/post_cont_image/deploy_app.png" alt="Deploy an app on Streamlit" /></p>

<p>A new pop window will come; select the latest Python version and past the access and secret access key from AWS S3.</p>

<p><img src="/blog/assets/post_cont_image/save_deployed_app.png" alt="Save deployed app" /></p>

<p>Now relax and give it a minute while the app is deploying on Streamlit.</p>

<p><img src="/blog/assets/post_cont_image/deploying.png" alt="deploying" /></p>

<p>Tadaaaa! the app should be up and running on Streamlit.</p>

<p><img src="/blog/assets/post_cont_image/running.png" alt="App running" /></p>

<p><a href="https://share.streamlit.io/semasuka/credit-card-approval-prediction-classification/main/cc_approval_pred.py">Here is the link</a> of the app deployed on Streamlit.</p>

<h3>Conclusion</h3>

<p>It was a long and fascinating project. We have come a long way, and you are still with me; you deserve a pad on your shoulder. By now, you should have a good grasp of what an end-to-end Machine Learning project is all about.</p>

<p>In this project, we touched at pretty much the main processes into carring an end-to-end Machine learning project, which are:</p>
<ul>
  <li>Exploratory data analysis</li>
  <li>Data preparation</li>
  <li>Training the model</li>
  <li>Model selection</li>
  <li>Testing the model</li>
  <li>Building a web interface for the model</li>
  <li>Deploying the model</li>
</ul>

<p>The only process I would say is missing is Web Scrapping the data because, in the real world, data is not found on Kaggle or clean. Data is received either through source data (could be a file, database, or API), but sometimes we might need to scrap it from a website. Hey! this is an excellent idea for you to apply the knowledge you gained from this project to your project. Try to scrap a website for your next project and create an end-to-end machine learning project as we did in this post.</p>

<p>I also want to mention some of the limitations of this project and what could be improved:</p>
<ul>
  <li>This model only predicts if an applicant is approved or not for a credit card, we could combine this model with a regression model to predict how much of a credit limit an applicant will be approved for.</li>
  <li>We could do a hyperparameter tuning with grid search or random search.</li>
  <li>We could do a chi-square test.</li>
  <li>We could also retrain the model without the least predictive features.</li>
</ul>

<p>I hope you enjoyed this project as much as I did. Find the codes of this project on my GitHub profile <a href="https://github.com/semasuka/Credit-card-approval-prediction-classification">here</a></p>

<p>Thank you again for going through this project with me. I hope you have learned one or two things. If you like this post, please subscribe to stay updated with new posts, and if you have a thought, correction or a question, I would love to hear it by commenting below. Remember, practice makes perfect! Keep on learning every day! Cheers!</p>


        <div class="share-page">
    Share this post on
    <a href="https://twitter.com/intent/tweet?text=Credit Card Approval Prediction (End-To-End Machine Learning Project)&url=http://localhost:4000/2022/10/12/credit-card-approval-prediction.html" rel="nofollow" target="_blank" title="Share on Twitter"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M16 3.038c-.59.26-1.22.437-1.885.517.677-.407 1.198-1.05 1.443-1.816-.634.37-1.337.64-2.085.79-.598-.64-1.45-1.04-2.396-1.04-1.812 0-3.282 1.47-3.282 3.28 0 .26.03.51.085.75-2.728-.13-5.147-1.44-6.766-3.42C.83 2.58.67 3.14.67 3.75c0 1.14.58 2.143 1.46 2.732-.538-.017-1.045-.165-1.487-.41v.04c0 1.59 1.13 2.918 2.633 3.22-.276.074-.566.114-.865.114-.21 0-.41-.02-.61-.058.42 1.304 1.63 2.253 3.07 2.28-1.12.88-2.54 1.404-4.07 1.404-.26 0-.52-.015-.78-.045 1.46.93 3.18 1.474 5.04 1.474 6.04 0 9.34-5 9.34-9.33 0-.14 0-.28-.01-.42.64-.46 1.2-1.04 1.64-1.7z" fill-rule="nonzero"/></svg></span></a>
    <a href="https://facebook.com/sharer.php?u=http://localhost:4000/2022/10/12/credit-card-approval-prediction.html" rel="nofollow" target="_blank" title="Share on Facebook"><span class="icon icon--facebook"><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M15.117 0H.883C.395 0 0 .395 0 .883v14.234c0 .488.395.883.883.883h7.663V9.804H6.46V7.39h2.086V5.607c0-2.066 1.262-3.19 3.106-3.19.883 0 1.642.064 1.863.094v2.16h-1.28c-1 0-1.195.48-1.195 1.18v1.54h2.39l-.31 2.42h-2.08V16h4.077c.488 0 .883-.395.883-.883V.883C16 .395 15.605 0 15.117 0" fill-rule="nonzero"/></svg>
</span></a>
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/2022/10/12/credit-card-approval-prediction.html&title=Credit Card Approval Prediction (End-To-End Machine Learning Project)&summary=&source=MIB" rel="nofollow" target="_blank" title="Share on LinkedIn"><span class="icon icon--linkedin"><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M13.632 13.635h-2.37V9.922c0-.886-.018-2.025-1.234-2.025-1.235 0-1.424.964-1.424 1.96v3.778h-2.37V6H8.51v1.04h.03c.318-.6 1.092-1.233 2.247-1.233 2.4 0 2.845 1.58 2.845 3.637v4.188zM3.558 4.955c-.762 0-1.376-.617-1.376-1.377 0-.758.614-1.375 1.376-1.375.76 0 1.376.617 1.376 1.375 0 .76-.617 1.377-1.376 1.377zm1.188 8.68H2.37V6h2.376v7.635zM14.816 0H1.18C.528 0 0 .516 0 1.153v13.694C0 15.484.528 16 1.18 16h13.635c.652 0 1.185-.516 1.185-1.153V1.153C16 .516 15.467 0 14.815 0z" fill-rule="nonzero"/></svg></span></a>
    <a href="http://www.reddit.com/submit?url=http://localhost:4000/2022/10/12/credit-card-approval-prediction.html&title=Credit Card Approval Prediction (End-To-End Machine Learning Project)" rel="nofollow" target="_blank" title="Share on Reddit"><span class="icon icon--reddit"><svg viewBox="-6 2 34 20" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M24 11.779c0-1.459-1.192-2.645-2.657-2.645-.715 0-1.363.286-1.84.746-1.81-1.191-4.259-1.949-6.971-2.046l1.483-4.669 4.016.941-.006.058c0 1.193.975 2.163 2.174 2.163 1.198 0 2.172-.97 2.172-2.163s-.975-2.164-2.172-2.164c-.92 0-1.704.574-2.021 1.379l-4.329-1.015c-.189-.046-.381.063-.44.249l-1.654 5.207c-2.838.034-5.409.798-7.3 2.025-.474-.438-1.103-.712-1.799-.712-1.465 0-2.656 1.187-2.656 2.646 0 .97.533 1.811 1.317 2.271-.052.282-.086.567-.086.857 0 3.911 4.808 7.093 10.719 7.093s10.72-3.182 10.72-7.093c0-.274-.029-.544-.075-.81.832-.447 1.405-1.312 1.405-2.318zm-17.224 1.816c0-.868.71-1.575 1.582-1.575.872 0 1.581.707 1.581 1.575s-.709 1.574-1.581 1.574-1.582-.706-1.582-1.574zm9.061 4.669c-.797.793-2.048 1.179-3.824 1.179l-.013-.003-.013.003c-1.777 0-3.028-.386-3.824-1.179-.145-.144-.145-.379 0-.523.145-.145.381-.145.526 0 .65.647 1.729.961 3.298.961l.013.003.013-.003c1.569 0 2.648-.315 3.298-.962.145-.145.381-.144.526 0 .145.145.145.379 0 .524zm-.189-3.095c-.872 0-1.581-.706-1.581-1.574 0-.868.709-1.575 1.581-1.575s1.581.707 1.581 1.575-.709 1.574-1.581 1.574z"/></svg>
</span></a>
    <a href="mailto:?subject=Credit Card Approval Prediction (End-To-End Machine Learning Project)&amp;body=Check out this post http://localhost:4000/blog/2022/10/12/credit-card-approval-prediction.html" target="_blank" title="Share on Email"><span class="icon icon--email"><svg xmlns="http://www.w3.org/2000/svg" viewBox="313.1 3.7 16 16"><path d="M318.5 8.9c0-.2.2-.4.4-.4h4.5c.2 0 .4.2.4.4s-.2.4-.4.4h-4.5c-.3 0-.4-.2-.4-.4zm.4 2.1h4.5c.2 0 .4-.2.4-.4s-.2-.4-.4-.4h-4.5c-.2 0-.4.2-.4.4s.1.4.4.4zm3.5 1.2c0-.2-.2-.4-.4-.4h-3.1c-.2 0-.4.2-.4.4s.2.4.4.4h3.1c.2.1.4-.1.4-.4zm-1.5-8.4l-1.7 1.4c-.2.1-.2.4 0 .6s.4.2.6 0l1.4-1.2 1.4 1.2c.2.1.4.1.6 0s.1-.4 0-.6l-1.7-1.4c-.3-.1-.5-.1-.6 0zm7.8 6.2c.1.1.1.2.1.3v7.9c0 .8-.7 1.5-1.5 1.5h-12.5c-.8 0-1.5-.7-1.5-1.5v-7.9c0-.1.1-.2.1-.3l1.6-1.3c.2-.1.4-.1.6 0s.1.4 0 .6l-1.2 1 1.8 1.3v-4c0-.6.5-1.1 1.1-1.1h7.5c.6 0 1.1.5 1.1 1.1v4l1.8-1.3-1.2-1c-.2-.1-.2-.4 0-.6s.4-.2.6 0l1.6 1.3zm-11.6 2.2l4 2.8 4-2.8V7.6c0-.1-.1-.2-.2-.2h-7.5c-.1 0-.2.1-.2.2v4.6zm10.9-1l-4.7 3.4 3.4 2.6c.2.1.2.4.1.6-.1.2-.4.2-.6.1l-3.6-2.8-1.2.8c-.1.1-.3.1-.5 0l-1.2-.8-3.6 2.8c-.2.1-.4.1-.6-.1-.1-.2-.1-.4.1-.6l3.4-2.6-4.7-3.4v7.1c0 .4.3.6.6.6h12.5c.4 0 .6-.3.6-.6v-7.1z"/></svg></span></a>
</div>

        
          
  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/blog/2022/10/12/credit-card-approval-prediction.html';
      this.page.identifier = 'http://localhost:4000/blog/2022/10/12/credit-card-approval-prediction.html';
    };

    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://semasuka-blog.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

        
      </div>
    </article>
  </div>
</main>
<script src="https://code.jquery.com/jquery-3.1.1.min.js"></script>
<script src="https://raw.githack.com/mburakerman/prognroll/master/src/prognroll.min.js"></script>
<script>
$(function() {
  $("body").prognroll({
    height: 5, //Progress bar height
    color: "#539F30", //Progress bar background color
    custom: false //If you make it true, you can add your custom div and see it's scroll progress on the page
  });
});
</script>



        </div>

        <div class="search-content">
          <div class="inner">
  <div tabindex="-1" class="search-searchbar"></div>
        <div class="search-hits"></div>
</div>

        </div>
      </div>
    </div>

    <footer id="footer" class="site-footer">
    <div class="inner">
      <div class="copyright">
          <!-- <p></a>Copyright &copy; 2018-<script type="text/javascript">
            document.write(new Date().getFullYear());
          </script></a>&#160;MIB. -->
          </br>Powered by <a href="https://jekyllrb.com/">Jekyll</a> & hosted on <a href="https://pages.github.com/">GitHub pages</a></p>
      </div>
    </div>
</footer>
    

<script async src="/blog/assets/javascripts/main.js"></script>

<!-- Including InstantSearch.js library and styling -->
<script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.3.3/dist/instantsearch.min.js"></script>
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.3.3/dist/instantsearch.min.css">
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.3.3/dist/instantsearch-theme-algolia.min.css">

<script>
// Instanciating InstantSearch.js with Algolia credentials
const search = instantsearch({
  appId: '2G0E5ME37Q',
  apiKey: 'afde2e91a28f543ce7c4afdefa8ecfbf',
  indexName: 'Machine Learning Blog',
  searchParameters: {
    restrictSearchableAttributes: [
      'title',
      'content'
    ]
  }
});

const hitTemplate = function(hit) {
  const url = hit.url;
  const title = hit._highlightResult.title.value;
  const content = hit._highlightResult.html.value;

  return `
    <article class="entry">
      <h3 class="entry-title"><a href="/blog${url}">${title}</a></h3>
      <div class="entry-excerpt">${content}</div>
    </article>
  `;
}

// Adding searchbar and results widgets
search.addWidget(
  instantsearch.widgets.searchBox({
    container: '.search-searchbar',
    
    placeholder: 'Enter your search term...'
  })
);
search.addWidget(
  instantsearch.widgets.hits({
    container: '.search-hits',
    templates: {
      item: hitTemplate
    }
  })
);

// Starting the search
search.start();
</script>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script>
$(document).ready(function(){
  // Add smooth scrolling to all links
  $("a").on('click', function(event) {

    // Make sure this.hash has a value before overriding default behavior
    if (this.hash !== "") {
      // Prevent default anchor click behavior
      event.preventDefault();

      // Store hash
      var hash = this.hash;

      // Using jQuery's animate() method to add smooth page scroll
      // The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
      $('html, body').animate({
        scrollTop: $(hash).offset().top
      }, 800, function(){

        // Add hash (#) to URL when done scrolling (default click behavior)
        window.location.hash = hash;
      });
    } // End if
  });
});
</script>

  </body>

</html>
