{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4e28b9",
   "metadata": {},
   "source": [
    "Welcome back, forks! After a long period of not posting here, I am happy to share that I am back again on MIB. In this post, we will work on an end-to-end machine learning project. I firmly believe this is one of the most detailed and comprehensive end-to-end ML project blog post on the internet. This project is perfect for the beginner in Machine Learning and seasoned ML engineers who could still learn one or two things from this post.\n",
    "\n",
    "Here is the roadmap we will follow:\n",
    "- We will start with exploratory data analysis(EDA)\n",
    "- Feature engineering\n",
    "- Feature selection\n",
    "- Data preprocessing\n",
    "- Model training\n",
    "- Model selection\n",
    "- Model storage on AWS blob storage\n",
    "- Build a web app interface for the model using Streamlit.\n",
    "- Finally, deploy the model.\n",
    "\n",
    "\n",
    "The goal is to predict whether an application for a credit card will be approved or not, using the applicant data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438e42e",
   "metadata": {},
   "source": [
    "I chose this project because when applying for a loan, credit card, or any other type of credit at any financial institution, there is a hard inquiry that affects your credit score negatively. This app predicts the probability of being approved without affecting your credit score. This app can be used by applicants who want to find out if they will be approved for a credit card without affecting their credit score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d454a643",
   "metadata": {},
   "source": [
    "***For those who are in a hurry, here is the key insights results from the analysis of this project:***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8ae320",
   "metadata": {},
   "source": [
    "Correlation between the features.\n",
    "\n",
    "![heatmap](../assets/post_cont_image/heatmap_cc_approval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f4456",
   "metadata": {},
   "source": [
    "Confusion matrix of gradient boosting classifier.\n",
    "\n",
    "![Confusion matrix](../assets/post_cont_image/cm_cc_approval.png)\n",
    "\n",
    "ROC curve of gradient boosting classifier.\n",
    "\n",
    "![ROC curve](../assets/post_cont_image/roc_cc_approval.png)\n",
    "\n",
    "Top 3 models (with default parameters)\n",
    "\n",
    "| Model     \t                | Recall score \t|\n",
    "|-------------------\t        |------------------\t|\n",
    "| Support vector machine     \t| 88% \t            |\n",
    "| Gradient boosting    \t        | 90% \t            |\n",
    "| Adaboost               \t    | 79% \t            |\n",
    "\n",
    "\n",
    "- **The final model used for this project: Gradient boosting**\n",
    "- **Metrics used: Recall**\n",
    "- **Why choose recall as metrics**:\n",
    "  Since the objective of this problem is to minimize the risk of a credit default, the metrics to use depends on the current economic situation:\n",
    "\n",
    "  - During a bull market (when the economy is expanding), people feel wealthy and are employed. Money is usually cheap, and the risk of default is low because of economic stability and low unemployment. The financial institution can handle the risk of default; therefore, it is not very strict about giving credit. The financial institution can handle some bad clients as long as most credit card owners are good clients (aka those who pay back their credit in time and in total).In this case, having a good recall (sensitivity) is ideal.\n",
    "\n",
    "  - During a bear market (when the economy is contracting), people lose their jobs and money through the stock market and other investment venues. Many people struggle to meet their financial obligations. The financial institution, therefore, tends to be more conservative in giving out credit or loans. The financial institution can't afford to give out credit to many clients who won't be able to pay back their credit. The financial institution would rather have a smaller number of good clients, even if it means that some good clients are denied credit. In this case, having a good precision (specificity) is desirable.\n",
    "\n",
    "    ***Note***: There is always a trade-off between precision and recall. Choosing the right metrics depends on the problem you are solving.\n",
    "\n",
    "    ***Conclusion***: Since the time I worked on this project (beginning 2022), we were in the longest bull market (excluding March 2020 flash crash) ever recorded; we will use recall as our metric.\n",
    "\n",
    "\n",
    " **Lessons learned and recommendation**\n",
    "\n",
    "- Based on this project's analysis, income, family member headcount, and employment length are the three most predictive features in determining whether an applicant will be approved for a credit card. Other features like age and working employment status are also helpful. The least useful features are the type of dwelling and car ownership.\n",
    "- The recommendation would be to focus more on the most predictive features when looking at the applicant profile and pay less attention to the least predictive features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526fe63d",
   "metadata": {},
   "source": [
    "***For the rest of my nerdy friends, let's get started from scratch***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52738d5",
   "metadata": {},
   "source": [
    "### Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d425d1",
   "metadata": {},
   "source": [
    "Wait! no, so fast! Before we start writing code, we need to have our python/jupyter environment ready, and Ken Jee has a fantastic video on this; click [here](https://www.youtube.com/watch?v=C4OPn58BLaU) to watch it.\n",
    "\n",
    "***Note:*** make sure that you install Python 3.10+ as we'll be using the switch statement, a new feature in Python 3.10 and above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a87b086",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be34778",
   "metadata": {},
   "source": [
    "Now we can import all the required libraries. Feel free to visit my [other post](https://semasuka.github.io/blog/2019/01/06/introduction-to-jupyter-notebook.html), where I talk about installing these libraries in the jupyter environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46deb3b2",
   "metadata": {
    "hidden": true,
    "id": "46deb3b2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from pandas_profiling import ProfileReport\n",
    "from pathlib import Path\n",
    "from scipy.stats import probplot, chi2_contingency, chi2, stats\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, roc_curve, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import scikitplot as skplt\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "import joblib\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da97a67",
   "metadata": {},
   "source": [
    "I will briefly explain what each library does and why we need it for this project.\n",
    "\n",
    "- NumPy is a library for manipulating multidimensional arrays and matrices. In this project, we will use NumPy to change the sequences of the elements in a list and also transform an array with negative values into absolute ones.\n",
    "- Pandas is a library to manipulate tabular data stored as dataframes (More than two columns) and Series(when dealing with one column); we will use it in this project to import the data into our notebook, create dataframes, merge and concatenate dataframes.\n",
    "- MissingNo is a great library to visualize at a glance missing value in a Pandas dataframe.\n",
    "- Scipy is a library that contains mathematical modules like statistics, optimization, linear algebra, etc\n",
    "- Pathlib is a built-in python library with useful path functionalities. Pathlib will use it in the project to check if a file exists at a specific path, then use the joblib to save it.\n",
    "- Matplotlib is a data visualization library to plot different types of plots like histograms, line plots, scatter plots, contour plots, etc. It is built on top of NumPy.\n",
    "- Seaborn is another data visualization library built on top of Matplotlib with added features and simpler syntax than Matplotlib. We will mainly use this library for our exploratory data analysis.\n",
    "- Warnings is a python builtin library to control the warnings at the execution time\n",
    "- Scikit-learn, also called sklearn, is the industry standard machine learning library from which all the machine learning algorithms are imported. It is built on NumPy, Scipy, and Matplotlib.\n",
    "- Imbalance learn is a library based on sklearn, which provides tools when dealing with classification with imbalanced classes. Here classes mean the prediction results, which in this case, are approved or denied for a credit card. In this project, we have two outcomes (we have a binary classification), and one of the outcomes is less likely to happen, which is reflected in the data. So we use the SMOTE technique to balance the outcomes because we don't want to train on unbalanced data as we try to avoid bias.\n",
    "- Scikit-plot is a helpful library that plots scikit-learn objects; for this project, Scikit-plot will use to plot the ROC curve.\n",
    "- Yellowbrick extends the scikit-learn API library to make a model selection. In this project, we have used it to plot the feature importance.\n",
    "- Joblib is a builtin python library to save models as files; those models will deploy on AWS S3\n",
    "- os is a builtin library to access some of the operating system functionality\n",
    "- Finally, magic command ```%matplotlib inline``` will make your plot outputs appear and be stored within the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bed837",
   "metadata": {},
   "source": [
    "### Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9878175d",
   "metadata": {},
   "source": [
    "After importing the libraries, we will now import the datasets. The datasets are from Kaggle. Here is the [link](https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eabba0b",
   "metadata": {},
   "source": [
    "There are two ways to import the CSV, we can download the file and pass the local machine path to the ```read_csv``` pandas function, or we can host the data on GitHub and directly read the hosted CSV file as a raw data. In this case, we went with the latter method.\n",
    "\n",
    "The first dataset is the application record with all the information about the applicants like gender, age, income, etc. The second dataset is the credit record which holds information about the credit status and balance. we will store those two dataset in ```cc_data_full_data``` and ```credit_status``` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b89053",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08b89053",
    "outputId": "00881095-43c8-4dbb-c660-f031ef0c5873"
   },
   "outputs": [],
   "source": [
    "cc_data_full_data = pd.read_csv('https://raw.githubusercontent.com/semasuka/Credit-card-approval-prediction-classification/main/datasets/application_record.csv')\n",
    "credit_status = pd.read_csv('https://raw.githubusercontent.com/semasuka/Credit-card-approval-prediction-classification/main/datasets/credit_record.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d0dc7",
   "metadata": {},
   "source": [
    "Let's glance at the first five rows using each Pandas' ``head``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data_full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c71a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_status.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c153a",
   "metadata": {},
   "source": [
    "Now let's look at the metadata of the datasets to understand the data better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b95f65",
   "metadata": {},
   "source": [
    "For the application record dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff3646",
   "metadata": {},
   "source": [
    "![appli_rec_metadata](../assets/post_cont_image/cc_app_meta.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c145c56",
   "metadata": {},
   "source": [
    "And for the credit record dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f0574",
   "metadata": {},
   "source": [
    "![appli_rec_metadata](../assets/post_cont_image/credit_meta.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54576333",
   "metadata": {},
   "source": [
    "### Creating a target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea57b124",
   "metadata": {},
   "source": [
    "As you may have noticed from our first dataset, we don't have a target variable that states whether the client is good or not (a client who will not default on their credit card would be called a good client). We will use the credit record to come up with the target variable. We use the [vintage analysis](https://www.listendata.com/2019/09/credit-risk-vintage-analysis.html) for this.\n",
    "\n",
    "For simplicity purposes, we will say that the applicants over 60 days overdue are considered bad clients. When the target variable is 1, that means a bad client, and when it is 0, that represents a good client. That is what the following script does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efea0f1",
   "metadata": {
    "id": "1efea0f1"
   },
   "outputs": [],
   "source": [
    "begin_month=pd.DataFrame(credit_status.groupby(['ID'])['MONTHS_BALANCE'].agg(min))\n",
    "begin_month=begin_month.rename(columns={'MONTHS_BALANCE':'Account age'})\n",
    "cc_data_full_data=pd.merge(cc_data_full_data,begin_month,how='left',on='ID')\n",
    "credit_status['dep_value'] = None\n",
    "credit_status['dep_value'][credit_status['STATUS'] =='2']='Yes'\n",
    "credit_status['dep_value'][credit_status['STATUS'] =='3']='Yes'\n",
    "credit_status['dep_value'][credit_status['STATUS'] =='4']='Yes'\n",
    "credit_status['dep_value'][credit_status['STATUS'] =='5']='Yes'\n",
    "cpunt=credit_status.groupby('ID').count()\n",
    "cpunt['dep_value'][cpunt['dep_value'] > 0]='Yes'\n",
    "cpunt['dep_value'][cpunt['dep_value'] == 0]='No'\n",
    "cpunt = cpunt[['dep_value']]\n",
    "cc_data_full_data = pd.merge(cc_data_full_data,cpunt,how='inner',on='ID')\n",
    "cc_data_full_data['Is high risk']=cc_data_full_data['dep_value']\n",
    "cc_data_full_data.loc[cc_data_full_data['Is high risk']=='Yes','Is high risk']=1\n",
    "cc_data_full_data.loc[cc_data_full_data['Is high risk']=='No','Is high risk']=0\n",
    "cc_data_full_data.drop('dep_value',axis=1,inplace=True)\n",
    "pd.options.mode.chained_assignment = None # hide warning SettingWithCopyWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf384fa",
   "metadata": {},
   "source": [
    "Let's print the first 5 rows of the dataframe, with the newly created target column ```Is high risk``` at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0793e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data_full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81151168",
   "metadata": {},
   "source": [
    "Since the features (columns) names are not very descriptive, we will change them to make them more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f2df32",
   "metadata": {
    "id": "f0f2df32"
   },
   "outputs": [],
   "source": [
    "# rename the features to more readable feature names\n",
    "cc_data_full_data = cc_data_full_data.rename(columns={\n",
    "    'CODE_GENDER':'Gender',\n",
    "    'FLAG_OWN_CAR':'Has a car',\n",
    "    'FLAG_OWN_REALTY':'Has a property',\n",
    "    'CNT_CHILDREN':'Children count',\n",
    "    'AMT_INCOME_TOTAL':'Income',\n",
    "    'NAME_INCOME_TYPE':'Employment status',\n",
    "    'NAME_EDUCATION_TYPE':'Education level',\n",
    "    'NAME_FAMILY_STATUS':'Marital status',\n",
    "    'NAME_HOUSING_TYPE':'Dwelling',\n",
    "    'DAYS_BIRTH':'Age',\n",
    "    'DAYS_EMPLOYED': 'Employment length',\n",
    "    'FLAG_MOBIL': 'Has a mobile phone',\n",
    "    'FLAG_WORK_PHONE': 'Has a work phone',\n",
    "    'FLAG_PHONE': 'Has a phone',\n",
    "    'FLAG_EMAIL': 'Has an email',\n",
    "    'OCCUPATION_TYPE': 'Job title',\n",
    "    'CNT_FAM_MEMBERS': 'Family member count',\n",
    "    'Account age': 'Account age'\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15295a4d",
   "metadata": {},
   "source": [
    "Now we will split the ```cc_data_full_data``` into a training and testing set. We will use 80% of the data for training and 20% for testing and store them respectively in ```cc_train_original``` and ```cc_test_original``` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed66b5",
   "metadata": {
    "id": "d9ed66b5"
   },
   "outputs": [],
   "source": [
    "# split the data into train and test dataset\n",
    "def data_split(df, test_size):\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
    "    # reset the indexes\n",
    "    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3579a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set the test_size to 0.2, which means that the train_size will be 0.8\n",
    "cc_train_original, cc_test_original = data_split(cc_data_full_data, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416babde",
   "metadata": {},
   "source": [
    "Dataframe's ``shape `` function helps us know the dimension of the dataframe. Here we have 20 features(columns) and 29165 observations(rows) for the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba7729",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_train_original.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e1565d",
   "metadata": {},
   "source": [
    "And 20 features(columns) and 7292 observations(rows) for the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94df2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_test_original.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6be8aa",
   "metadata": {},
   "source": [
    "Finally, we will export the data as a CSV file on our local machine and create a copy of the dataset. Please note that these steps are optional. It is best practice to keep the original dataset untouched as a backup and work with the copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd5177",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_train_original.to_csv('dataset/train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_test_original.to_csv('dataset/test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1857ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the dataset so that the original stays untouched\n",
    "cc_train_copy = cc_train_original.copy()\n",
    "cc_test_copy = cc_test_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1457a76",
   "metadata": {},
   "source": [
    "### Data at a glance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf179a",
   "metadata": {},
   "source": [
    "Now that we have split the dataset into training and testing datasets, we will focus on the training dataset for now and use the test dataset toward the end of this post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656be5d9",
   "metadata": {},
   "source": [
    "Let's review the first 5 rows again with the ```head()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0NDJvJIk4DnC",
   "metadata": {
    "id": "0NDJvJIk4DnC"
   },
   "outputs": [],
   "source": [
    "cc_data_full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc3a202",
   "metadata": {},
   "source": [
    "Now let's see the data types of each of the features with the ```info()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Tsy-_XPMz4dm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tsy-_XPMz4dm",
    "outputId": "06a46cef-223d-42a5-f344-014512c5897c"
   },
   "outputs": [],
   "source": [
    "cc_data_full_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30cb5c0",
   "metadata": {},
   "source": [
    "Let's digest the information above. The first column is the indexes of the features; the second is the names; the third is the count of non-null values(only the job title has missing values); and the fourth is datatypes (objects which mean strings datatype, float or integer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc74c85",
   "metadata": {},
   "source": [
    "The ```describe()``` function gives us statistics about the numerical features in the dataset. These statistics include each numerical feature's count, mean, standard deviation, interquartile range(25%, 50%, 75%), and minimum and maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf6713",
   "metadata": {
    "id": "8baf6713"
   },
   "outputs": [],
   "source": [
    "cc_data_full_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e356f42",
   "metadata": {},
   "source": [
    "We will use the [Missingno](https://github.com/ResidentMario/missingno) to visualize the missing values per feature using its ```matrix``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a38813",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(cc_data_full_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd1dbb",
   "metadata": {},
   "source": [
    "Here we can see that the Job title is the only feature with missing values. Slim white lines represent missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a1338",
   "metadata": {},
   "source": [
    "To see a clear representation of the missing values count, we can use its ```bar()``` function to have a barplot with the count of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(cc_data_full_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202da9da",
   "metadata": {},
   "source": [
    "Now we will create functions to analyze each feature(Univariate analysis). Don't worry too much about understanding these functions, as we will see how they are used during the exploratory data analysis section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95865fa4",
   "metadata": {},
   "source": [
    "Our first function ```value_cnt_norm_cal``` is used to calculate the count of each class in a feature with its frequency (normalized on a scale of 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X2DnChyC8DmH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2DnChyC8DmH",
    "outputId": "94024136-79d7-4d2b-b771-c0e52a51f734"
   },
   "outputs": [],
   "source": [
    "def value_cnt_norm_cal(df,feature):\n",
    "    '''Function that will return the value count and frequency of each observation within a feature'''\n",
    "    # get the value counts of each feature\n",
    "    ftr_value_cnt = df[feature].value_counts()\n",
    "    # normalize the value counts on a scale of 100\n",
    "    ftr_value_cnt_norm = df[feature].value_counts(normalize=True) * 100\n",
    "    # concatenate the value counts with normalized value count column wise\n",
    "    ftr_value_cnt_concat = pd.concat([ftr_value_cnt, ftr_value_cnt_norm], axis=1)\n",
    "    # give it a column name\n",
    "    ftr_value_cnt_concat.columns = ['Count', 'Frequency (%)']\n",
    "    # return the dataframe\n",
    "    return ftr_value_cnt_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0693166a",
   "metadata": {},
   "source": [
    "```gen_info_feat``` returned the description, the datatype, statistics, the value counts and frequencies\n",
    "\n",
    "Note: I have used the switch statement to handle features differently depending on their data type and characteristics. For example, I divided age by 365.25 and changed it to a positive value because it is expressed in days instead of years. Same as employment length; however, we did not print the value count for account age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f8e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_info_feat(df,feature):\n",
    "    '''function to display general information about the feature'''\n",
    "    match feature:\n",
    "        # if the feature is Age\n",
    "        case 'Age':\n",
    "            # change the feature to be expressed in positive numbers of days and divide by 365.25 to be expressed in years and get the description\n",
    "            print('Description:\\n{}'.format((np.abs(df[feature])/365.25).describe()))\n",
    "            # print separators\n",
    "            print('*'*50)\n",
    "            # print the datatype\n",
    "            print('Object type:{}'.format(df[feature].dtype))\n",
    "        # if the feature is employment length\n",
    "        case 'Employment length':\n",
    "            # select only the rows where the rows are negative values to ignore those who have retired or are unemployed\n",
    "            employment_len_no_ret = cc_train_copy['Employment length'][cc_train_copy['Employment length'] < 0]\n",
    "            # change the negative values to positive values\n",
    "            employment_len_no_ret_yrs = np.abs(employment_len_no_ret)/365.25\n",
    "            # print the descriptions\n",
    "            print('Description:\\n{}'.format((employment_len_no_ret_yrs).describe()))\n",
    "            # print separators\n",
    "            print('*'*50)\n",
    "            # print the datatype\n",
    "            print('Object type:{}'.format(employment_len_no_ret.dtype))\n",
    "        # if the feature is account age\n",
    "        case 'Account age' | 'Income':\n",
    "            # change the account age to a positive number of months and get the description\n",
    "            print('Description:\\n{}'.format((np.abs(df[feature])).describe()))\n",
    "            # print separators\n",
    "            print('*'*50)\n",
    "            # print the datatype\n",
    "            print('Object type:{}'.format(df[feature].dtype))\n",
    "        # if it is any other feature\n",
    "        case _:\n",
    "            # get the description\n",
    "            print('Description:\\n{}'.format(df[feature].describe()))\n",
    "            # print separators\n",
    "            print('*'*50)\n",
    "            # print the datatype\n",
    "            print('Object type:\\n{}'.format(df[feature].dtype))\n",
    "            # print separators\n",
    "            print('*'*50)\n",
    "            # calling the value_cnt_norm_cal function previously seen\n",
    "            value_cnt = value_cnt_norm_cal(df,feature)\n",
    "            # print the result\n",
    "            print('Value count:\\n{}'.format(value_cnt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49543582",
   "metadata": {},
   "source": [
    "The following function prints a pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399707b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pie_plot(df,feature):\n",
    "    '''function to create a pie chart plot'''\n",
    "    match feature:\n",
    "        # if the feature is dwelling or education level\n",
    "        case 'Dwelling' | 'Education level':\n",
    "            # calling the value_cnt_norm_cal function previously seen\n",
    "            ratio_size = value_cnt_norm_cal(df, feature)\n",
    "            # get how many classes we have\n",
    "            ratio_size_len = len(ratio_size.index)\n",
    "            ratio_list = []\n",
    "            # loop till the max range\n",
    "            for i in range(ratio_size_len):\n",
    "                #append the ratio of each feature to the list\n",
    "                ratio_list.append(ratio_size.iloc[i]['Frequency (%)'])\n",
    "            # create a subplot\n",
    "            fig, ax = plt.subplots(figsize=(8,8))\n",
    "            # %1.2f%% display decimals in the pie chart with 2 decimal places\n",
    "            plt.pie(ratio_list, startangle=90, wedgeprops={'edgecolor' :'black'})\n",
    "            # add a title to the chart\n",
    "            plt.title('Pie chart of {}'.format(feature))\n",
    "            # add a legend to the chart\n",
    "            plt.legend(loc='best',labels=ratio_size.index)\n",
    "            # center the plot in the subplot\n",
    "            plt.axis('equal')\n",
    "\n",
    "            # return the plot\n",
    "            return plt.show()\n",
    "        # for other features\n",
    "        case _:\n",
    "            ratio_size = value_cnt_norm_cal(df, feature)\n",
    "            ratio_size_len = len(ratio_size.index)\n",
    "            ratio_list = []\n",
    "            for i in range(ratio_size_len):\n",
    "                ratio_list.append(ratio_size.iloc[i]['Frequency (%)'])\n",
    "            fig, ax = plt.subplots(figsize=(8,8))\n",
    "            # %1.2f%% display decimals in the pie chart with 2 decimal places\n",
    "            plt.pie(ratio_list, labels=ratio_size.index, autopct='%1.2f%%', startangle=90, wedgeprops={'edgecolor' :'black'})\n",
    "            plt.title('Pie chart of {}'.format(feature))\n",
    "            plt.legend(loc='best')\n",
    "            plt.axis('equal')\n",
    "            return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851ac9f",
   "metadata": {},
   "source": [
    "The next function create a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bar_plot(df,feature):\n",
    "    '''function to create a bar chart plot'''\n",
    "    match feature:\n",
    "        case 'Marital status' | 'Dwelling' | 'Job title' | 'Employment status' | 'Education level':\n",
    "            fig, ax = plt.subplots(figsize=(6,10))\n",
    "            # create a barplot using seaborn with X-axis the indexes from value_cnt_norm_cal function and Y axis we use the value counts from the same function\n",
    "            sns.barplot(x=value_cnt_norm_cal(df,feature).index,y=value_cnt_norm_cal(df,feature).values[:,0])\n",
    "            # set the plot's tick labels to the index from the value_cnt_norm_cal function, rotate those ticks by 45 degrees\n",
    "            ax.set_xticklabels(labels=value_cnt_norm_cal(df,feature).index,rotation=45,ha='right')\n",
    "            # Give the X-axis the same label as the feature name\n",
    "            plt.xlabel('{}'.format(feature))\n",
    "            # Give the Y-axis the label \"Count\"\n",
    "            plt.ylabel('Count')\n",
    "            # Give the plot a title\n",
    "            plt.title('{} count'.format(feature))\n",
    "            # Return the title\n",
    "            return plt.show()\n",
    "        case _:\n",
    "            fig, ax = plt.subplots(figsize=(6,10))\n",
    "            sns.barplot(x=value_cnt_norm_cal(df,feature).index,y=value_cnt_norm_cal(df,feature).values[:,0])\n",
    "            plt.xlabel('{}'.format(feature))\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('{} count'.format(feature))\n",
    "            return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da68e2",
   "metadata": {},
   "source": [
    "This function will create a box plot for continuous variables.\n",
    "\n",
    "\n",
    "Note: Depending on which transformation needs to be done on each feature, we have used a switch statement to handle the different feature that requires different handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_box_plot(df,feature):\n",
    "    '''function to create a box plot'''\n",
    "    match feature:\n",
    "        case 'Age':\n",
    "            fig, ax = plt.subplots(figsize=(2,8))\n",
    "            # change the feature to be expressed in positive numbers days\n",
    "            sns.boxplot(y=np.abs(df[feature])/365.25)\n",
    "            plt.title('{} distribution(Boxplot)'.format(feature))\n",
    "            return plt.show()\n",
    "        case 'Children count':\n",
    "            fig, ax = plt.subplots(figsize=(2,8))\n",
    "            sns.boxplot(y=df[feature])\n",
    "            plt.title('{} distribution(Boxplot)'.format(feature))\n",
    "            # use the numpy arrange to populate the Y ticks starting from 0 till the max count of children with an interval of 1 as follows np.arange(start, stop, step)\n",
    "            plt.yticks(np.arange(0,df[feature].max(),1))\n",
    "            return plt.show()\n",
    "        case 'Employment length':\n",
    "            fig, ax = plt.subplots(figsize=(2,8))\n",
    "            employment_len_no_ret = cc_train_copy['Employment length'][cc_train_copy['Employment length'] < 0]\n",
    "            # employment length in days is a negative number, so we need to change it to positive and change it to years\n",
    "            employment_len_no_ret_yrs = np.abs(employment_len_no_ret)/365.25\n",
    "            # create a boxplot with seaborn\n",
    "            sns.boxplot(y=employment_len_no_ret_yrs)\n",
    "            plt.title('{} distribution(Boxplot)'.format(feature))\n",
    "            plt.yticks(np.arange(0,employment_len_no_ret_yrs.max(),2))\n",
    "            return plt.show()\n",
    "        case 'Income':\n",
    "            fig, ax = plt.subplots(figsize=(2,8))\n",
    "            sns.boxplot(y=df[feature])\n",
    "            plt.title('{} distribution(Boxplot)'.format(feature))\n",
    "            # suppress the scientific notation\n",
    "            ax.get_yaxis().set_major_formatter(\n",
    "                matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "            return plt.show()\n",
    "        case 'Account age':\n",
    "            fig, ax = plt.subplots(figsize=(2,8))\n",
    "            sns.boxplot(y=np.abs(df[feature]))\n",
    "            plt.title('{} distribution(Boxplot)'.format(feature))\n",
    "            return plt.show()\n",
    "        case _:\n",
    "            fig, ax = plt.subplots(figsize=(2,8))\n",
    "            sns.boxplot(y=df[feature])\n",
    "            plt.title('{} distribution(Boxplot)'.format(feature))\n",
    "            return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b0917",
   "metadata": {},
   "source": [
    "This function will plot a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hist_plot(df,feature, the_bins=50):\n",
    "    '''function to create a histogram plot'''\n",
    "    match feature:\n",
    "        case 'Age':\n",
    "            fig, ax = plt.subplots(figsize=(18,10))\n",
    "            # change the feature to be expressed in positive numbers days\n",
    "            sns.histplot(np.abs(df[feature])/365.25,bins=the_bins,kde=True)\n",
    "            plt.title('{} distribution'.format(feature))\n",
    "            return plt.show()\n",
    "        case 'Income':\n",
    "            fig, ax = plt.subplots(figsize=(18,10))\n",
    "            sns.histplot(df[feature],bins=the_bins,kde=True)\n",
    "            # suppress scientific notation\n",
    "            ax.get_xaxis().set_major_formatter(\n",
    "                matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "            plt.title('{} distribution'.format(feature))\n",
    "            return plt.show()\n",
    "        case 'Employment length':\n",
    "            employment_len_no_ret = cc_train_copy['Employment length'][cc_train_copy['Employment length'] < 0]\n",
    "            # change the feature to be expressed in positive numbers days\n",
    "            employment_len_no_ret_yrs = np.abs(employment_len_no_ret)/365.25\n",
    "            fig, ax = plt.subplots(figsize=(18,10))\n",
    "            sns.histplot(employment_len_no_ret_yrs,bins=the_bins,kde=True)\n",
    "            plt.title('{} distribution'.format(feature))\n",
    "            return plt.show()\n",
    "        case 'Account age':\n",
    "            fig, ax = plt.subplots(figsize=(18,10))\n",
    "            sns.histplot(np.abs(df[feature]),bins=the_bins,kde=True)\n",
    "            plt.title('{} distribution'.format(feature))\n",
    "            return plt.show()\n",
    "        case _:\n",
    "            fig, ax = plt.subplots(figsize=(18,10))\n",
    "            sns.histplot(df[feature],bins=the_bins,kde=True)\n",
    "            plt.title('{} distribution'.format(feature))\n",
    "            return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26ae60",
   "metadata": {},
   "source": [
    "This function will plot two box plots, one is for low-risk (good client), and the other is for high-risk (bad client) applicants. On the Y axis, we have the continuous features we are studying. Again don't worry too much, as we will see these functions in action in the sections below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_high_risk_box_plot(df,feature):\n",
    "    '''High risk vs low risk applicants compared on a box plot'''\n",
    "    match feature:\n",
    "        case 'Age':\n",
    "            print(np.abs(df.groupby('Is high risk')[feature].mean()/365.25))\n",
    "            fig, ax = plt.subplots(figsize=(5,8))\n",
    "            # Place on the Y-axis age and X-axis the two box plot (is high risk: No and Yes)\n",
    "            sns.boxplot(y=np.abs(df[feature])/365.25,x=df['Is high risk'])\n",
    "            # add ticks to the X axis\n",
    "            plt.xticks(ticks=[0,1],labels=['no','yes'])\n",
    "            plt.title('High risk individuals grouped by age')\n",
    "            return plt.show()\n",
    "        case 'Income':\n",
    "            print(np.abs(df.groupby('Is high risk')[feature].mean()))\n",
    "            fig, ax = plt.subplots(figsize=(5,8))\n",
    "            sns.boxplot(y=np.abs(df[feature]),x=df['Is high risk'])\n",
    "            plt.xticks(ticks=[0,1],labels=['no','yes'])\n",
    "            # suppress the scientific notation\n",
    "            ax.get_yaxis().set_major_formatter(\n",
    "                matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "            plt.title('High risk individuals grouped by {}'.format(feature))\n",
    "            return plt.show()\n",
    "        case 'Employment length':\n",
    "            # checking is an applicant is high risk or not (for those who have negative employment length mean only those who are employed)\n",
    "            employment_no_ret = cc_train_copy['Employment length'][cc_train_copy['Employment length'] <0]\n",
    "            employment_no_ret_idx = employment_no_ret.index\n",
    "            employment_len_no_ret_yrs = np.abs(employment_no_ret)/365.25\n",
    "            # extract those who are employed from the original dataframe and return only the employment length and Is high risk columns\n",
    "            employment_no_ret_df = cc_train_copy.iloc[employment_no_ret_idx][['Employment length','Is high risk']]\n",
    "            # return the mean employment length group by how risky is the applicant\n",
    "            employment_no_ret_is_high_risk = employment_no_ret_df.groupby('Is high risk')['Employment length'].mean()\n",
    "            print(np.abs(employment_no_ret_is_high_risk)/365.25)\n",
    "            fig, ax = plt.subplots(figsize=(5,8))\n",
    "            sns.boxplot(y=employment_len_no_ret_yrs,x=df['Is high risk'])\n",
    "            plt.xticks(ticks=[0,1],labels=['no','yes'])\n",
    "            plt.title('High vs low risk individuals grouped by {}'.format(feature))\n",
    "            return plt.show()\n",
    "        case _:\n",
    "            print(np.abs(df.groupby('Is high risk')[feature].mean()))\n",
    "            fig, ax = plt.subplots(figsize=(5,8))\n",
    "            sns.boxplot(y=np.abs(df[feature]),x=df['Is high risk'])\n",
    "            plt.xticks(ticks=[0,1],labels=['no','yes'])\n",
    "            plt.title('High risk individuals grouped by {}'.format(feature))\n",
    "            return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a95bab",
   "metadata": {},
   "source": [
    "This function is similar to the previous one; the only difference is that it uses a bar plot which is a count of classes for comparison purposes between high risk and low risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce57246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_high_risk_bar_plot(df,feature):\n",
    "    '''High risk vs low risk applicants compared on a bar plot'''\n",
    "    # get the sum of high-risk clients grouped by a specific feature\n",
    "    is_high_risk_grp = df.groupby(feature)['Is high risk'].sum()\n",
    "    # sort is a descending order\n",
    "    is_high_risk_grp_srt = is_high_risk_grp.sort_values(ascending=False)\n",
    "    print(dict(is_high_risk_grp_srt))\n",
    "    fig, ax = plt.subplots(figsize=(6,10))\n",
    "    # plot on the X axis the indexes which correspond to classes, and on the Y axis, the count\n",
    "    sns.barplot(x=is_high_risk_grp_srt.index,y=is_high_risk_grp_srt.values)\n",
    "    # add the labels to the plot\n",
    "    ax.set_xticklabels(labels=is_high_risk_grp_srt.index,rotation=45, ha='right')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('High risk applicants count grouped by {}'.format(feature))\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623fb99",
   "metadata": {},
   "source": [
    "Now let's properly start our exploratory data analysis with a univariate analysis. Univariate analysis is an analysis of each feature individually in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2399c72c",
   "metadata": {},
   "source": [
    "### Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ce011",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a192aab1",
   "metadata": {
    "id": "a192aab1"
   },
   "source": [
    "We start with ```Gender```. We call ```gen_info_feat``` and see that we have two unique classes ```F``` (for female) and ```M``` (for male), with 19549 and 9616 occurrences, respectively. Percentage-wise we have 67.02% females and 32.97% males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a96d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "579a96d1",
    "outputId": "e6c0e80f-38f9-4309-b435-9b821a159000"
   },
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot(cc_train_copy,'Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69982c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pie_plot(cc_train_copy,'Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f7b77",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23e10e",
   "metadata": {
    "id": "8c23e10e"
   },
   "source": [
    "Now let's look at ```Age```; since age is a continuous variable, we will process it differently than ```Gender```. Using the ```gen_info_feat``` function, we look at the mean, standard deviation, minimum, maximum and interquartile range. Then we plot that information on a box plot by calling the ```create_box_plot``` function. With that, we can see that the youngest applicant(s) is 21 years old while the oldest is 68. With an average of 43.7 and a median of 42.6 (outliers insensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96236415",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927742c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_box_plot(cc_train_copy,'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c835f0",
   "metadata": {},
   "source": [
    "After that, we plot its histogram with the kernel density estimator. ``` Age `` is not normally distributed; it is slightly positively skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ea2d2",
   "metadata": {
    "id": "330ea2d2",
    "outputId": "6cdc63ea-0af2-4580-aa11-9928c5443818"
   },
   "outputs": [],
   "source": [
    "create_hist_plot(cc_train_copy,'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13d8ab",
   "metadata": {},
   "source": [
    "Now we perform a quick bivariate analysis (comparison of two features) of ```Age``` and the target variable ```Is high risk```. The blue box plot represents a good client (is high risk = No), and the green box plot represents a bad client (is high risk = Yes). We can see no significant difference between the age of those who are high risk and those who are not. The mean age for both groups is around 43 years old, and there is no correlation between the age and risk factors of the applicant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66e5ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "6a66e5ac",
    "outputId": "b20f3c79-fa32-40bc-f0b9-2e4e49c7e59b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "low_high_risk_box_plot(cc_train_copy,'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb462915",
   "metadata": {
    "id": "cb462915"
   },
   "source": [
    "### Marital status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a4c11",
   "metadata": {},
   "source": [
    "There are 5 unique classes for this feature. Married constitutes the most significant proportion of marital status, with 68% far ahead of single, as seen on the pie chart and bar charts. Another interesting observation is that even though we have a higher number of applicants who are separated than widows, it seems that widow applicants are bad clients than those who are separated by a small margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Marital status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6732bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pie_plot(cc_train_copy,'Marital status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot(cc_train_copy,'Marital status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39587a19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39587a19",
    "outputId": "3fb1f38d-bb38-4adc-e619-e5c00691288a"
   },
   "outputs": [],
   "source": [
    "low_high_risk_bar_plot(cc_train_copy,'Marital status')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a21789",
   "metadata": {
    "id": "79a21789"
   },
   "source": [
    "### Family member count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4785a3d",
   "metadata": {},
   "source": [
    "Family member count is a numerical feature, with the median of 2 family members representing 53% (count = 15552) of all the counts, followed by a single family member with 19% (count = 5613). Looking at the box plot, we have 6 outliers; 2 are extreme, with 20 and 15 members in their household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a70321",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Family member count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_box_plot(cc_train_copy,'Family member count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b1655",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot(cc_train_copy,'Family member count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255ed45",
   "metadata": {
    "id": "8255ed45"
   },
   "source": [
    "### Children count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912fc557",
   "metadata": {},
   "source": [
    "From the chart below, we can see that most applicants don't have any children. Again, we have 6 outliers, most probably the same seen from the family member count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccff033",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Children count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_box_plot(cc_train_copy,'Children count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot(cc_train_copy,'Children count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f676d23",
   "metadata": {
    "id": "5f676d23"
   },
   "source": [
    "### Dwelling type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b8cd7",
   "metadata": {},
   "source": [
    "89% of applicants live in houses/apartments by a substantial margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7e7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Dwelling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d09850",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pie_plot(cc_train_copy,'Dwelling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184bca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot(cc_train_copy,'Dwelling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0022c0",
   "metadata": {
    "id": "6d0022c0"
   },
   "source": [
    "### Income"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6a0464",
   "metadata": {},
   "source": [
    "Looking at the results from the ```gen_info_feat``` function, we can see that the average mean income is 186890, but this amount factors in outliers. Most people make 157500 (median income) if we ignore the outliers. We have 3 applicants who make more than 1000000.\n",
    "\n",
    "This feature is also positively skewed. Focusing on the income box plot of good and bad clients, they all have roughly similar incomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "gen_info_feat(cc_train_copy,'Income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf87d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_box_plot(cc_train_copy,'Income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68155550",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hist_plot(cc_train_copy,'Income')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c65285",
   "metadata": {},
   "source": [
    "* bivariate analysis with target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc275de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_high_risk_box_plot(cc_train_copy,'Income')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530b925",
   "metadata": {
    "id": "9530b925"
   },
   "source": [
    "### Job title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e63bc4",
   "metadata": {},
   "source": [
    "The most common Job title is laborers by a large margin (24.85%), followed by core staff (14.23%), sales staff (13.77%) and managers (12.03%). We also have 30.95% of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd92861",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fd92861",
    "outputId": "cf041d66-374e-47c1-cca5-238e30295c21"
   },
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Job title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08dd90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title_nan_count = cc_train_copy['Job title'].isna().sum()\n",
    "job_title_nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1636d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_total_count = cc_train_copy.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The percentage of missing rows is {:.2f} %'.format(job_title_nan_count * 100 / rows_total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot(cc_train_copy,'Job title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b67bdf",
   "metadata": {
    "id": "88b67bdf"
   },
   "source": [
    "### Employment status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ac2c42",
   "metadata": {},
   "source": [
    "Most applicants are working (51.62%); the next most represented status is commercial associate, followed by the pensioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c282165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Employment status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dce8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot(cc_train_copy,'Employment status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pie_plot(cc_train_copy,'Employment status')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef4551",
   "metadata": {
    "id": "0bef4551"
   },
   "source": [
    "### Education level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d431a851",
   "metadata": {},
   "source": [
    "Most applicants have completed their secondary degree (67.90%) ¼ completed their higher education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1602108",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1602108",
    "outputId": "cc712302-2785-4db1-aab5-035303c9abcd"
   },
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Education level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pie_plot(cc_train_copy,'Education level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eae247",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot(cc_train_copy,'Education level')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641315f",
   "metadata": {
    "id": "3641315f"
   },
   "source": [
    "### Employment length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e467a1b",
   "metadata": {},
   "source": [
    "Most applicants have been working between 5 to 7 years on average, and we also have many outliers who have been working for more than 20 years+. The employment length histogram is positively skewed. Finally, bad clients have a low employment length of 5 versus 7 years for good clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Employment length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c635ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_box_plot(cc_train_copy,'Employment length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d6be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hist_plot(cc_train_copy,'Employment length')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc141c87",
   "metadata": {},
   "source": [
    "* bivariate analysis with target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ffa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of employment length for good vs bad client\n",
    "# Here 0 means No and 1 means Yes\n",
    "low_high_risk_box_plot(cc_train_copy,'Employment length')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea91a4b6",
   "metadata": {
    "id": "ea91a4b6"
   },
   "source": [
    "### Has a car"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8543a6cb",
   "metadata": {},
   "source": [
    "Most applicants don't own a car (62% of applicants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32107b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Has a car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot(cc_train_copy,'Has a car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5bb5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16c5bb5f",
    "outputId": "e75f6536-9194-4e72-cde1-eb90c7115395"
   },
   "outputs": [],
   "source": [
    "create_pie_plot(cc_train_copy,'Has a car')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e3e40",
   "metadata": {
    "id": "a53e3e40"
   },
   "source": [
    "### Has a property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1432d3b9",
   "metadata": {},
   "source": [
    "Most applicants own a property (67% of applicants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6eea9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75d6eea9",
    "outputId": "66b67fc4-cd04-48a9-bb49-29b40a01368d"
   },
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Has a property')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e544e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45e544e6",
    "outputId": "762d7968-1c6c-466a-cd9f-08019d99e40b"
   },
   "outputs": [],
   "source": [
    "create_bar_plot(cc_train_copy,'Has a property')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27106ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a27106ab",
    "outputId": "9e962898-910e-43e2-cfa7-e8e3154646ef"
   },
   "outputs": [],
   "source": [
    "create_pie_plot(cc_train_copy,'Has a property')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba5bd2",
   "metadata": {
    "id": "6fba5bd2"
   },
   "source": [
    "### Has a work phone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eacedad",
   "metadata": {},
   "source": [
    "More than ¾ of applicants don’t have a work phone\n",
    "\n",
    "Note: Here, 0 represent no and 1 represents yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144512b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "144512b4",
    "outputId": "1c66967c-ae61-4d8b-b166-4b390cec2d33"
   },
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Has a work phone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5cc2a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d5cc2a6",
    "outputId": "d47a2d7f-2de9-45ee-e758-3345b4930838"
   },
   "outputs": [],
   "source": [
    "create_bar_plot(cc_train_copy,'Has a work phone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0e15e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aa0e15e",
    "outputId": "e2da754b-6bba-4b64-d3d9-349d91c85690"
   },
   "outputs": [],
   "source": [
    "create_pie_plot(cc_train_copy,'Has a work phone')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1645b2",
   "metadata": {
    "id": "cb1645b2"
   },
   "source": [
    "### Has a mobile phone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ccf24",
   "metadata": {},
   "source": [
    "All the applicants, without exception, have a mobile phone.\n",
    "\n",
    "Note: Here, 0 is no and 1 is yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47945ebf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47945ebf",
    "outputId": "8e82dbcc-b5cc-4c9a-c9fb-3c553e9eef94"
   },
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Has a mobile phone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fba1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pie_plot(cc_train_copy,'Has a mobile phone')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f450bf4",
   "metadata": {
    "id": "9f450bf4"
   },
   "source": [
    "### Has a phone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f918a66",
   "metadata": {},
   "source": [
    "70% of applicants don’t have a phone (probably a home phone)\n",
    "\n",
    "Note: Here, 0 is no and 1 is yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38bb77d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e38bb77d",
    "outputId": "0b91a895-290f-4368-e3b2-ac8eac93bc14"
   },
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Has a phone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cfd34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot(cc_train_copy,'Has a phone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d3508",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pie_plot(cc_train_copy,'Has a phone')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1748834",
   "metadata": {
    "id": "d1748834",
    "scrolled": false
   },
   "source": [
    "### Has an email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f9c7c",
   "metadata": {},
   "source": [
    "Interestingly, more than 90 % of applicants don’t have an email\n",
    "\n",
    "Note: Here, 0 is no and 1 is yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109a1de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d109a1de",
    "outputId": "aec84aa2-8029-4fca-ba8b-0c9f22e3c7e0"
   },
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Has an email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf64ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63cf64ee",
    "outputId": "d4a1112c-9414-4220-a69b-80023f8eb7c8"
   },
   "outputs": [],
   "source": [
    "create_bar_plot(cc_train_copy,'Has an email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2798a6e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2798a6e",
    "outputId": "4e3928db-5b7e-444f-cf3e-79957a88c0df"
   },
   "outputs": [],
   "source": [
    "create_pie_plot(cc_train_copy,'Has an email')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074ec44",
   "metadata": {},
   "source": [
    "### Account age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ac339",
   "metadata": {},
   "source": [
    "Most accounts are 26 months old. The account age feature is not normally distributed; it is positively skewed. Another observation is that, on average, bad clients' accounts are 34 months old vs 26 months old for good clients' accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9bef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Account age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_box_plot(cc_train_copy,'Account age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f30df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hist_plot(cc_train_copy,'Account age', the_bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f4fc4",
   "metadata": {},
   "source": [
    "* bivariate analysis with target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_high_risk_box_plot(cc_train_copy,'Account age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a35e7",
   "metadata": {},
   "source": [
    "### Is high risk (target variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f4b28d",
   "metadata": {},
   "source": [
    "Most applicants are good clients (98% of applicants). We have imbalanced data that needs to be balanced using SMOTE before training on a model.\n",
    "\n",
    "Note: Here, 0 is no and 1 is yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_info_feat(cc_train_copy,'Is high risk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1791863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot(cc_train_copy,'Is high risk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b556f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pie_plot(cc_train_copy,'Is high risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03bc549",
   "metadata": {},
   "source": [
    "### Bivariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd66bf",
   "metadata": {},
   "source": [
    "Now that we have finished our univariate analysis let's look into the bivariate analysis. Bivariate analysis, as the name implies, is the analysis of two features compared with each other. First, we will do a bivariate analysis of numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d11417",
   "metadata": {},
   "source": [
    "Looking at the pairplot (scatter plots of pairwise relationships in a dataset), we can see a positive linear correlation between the family member and the children's count. It makes sense; the more children someone has, the larger the family member count. It is a multicollinearity problem (two highly correlated features) which is not ideal for training a model. We will need to drop one of them.\n",
    "\n",
    "Another trend is the Employment length and age. It also makes sense; the longer the employment length, the older someone is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop categorical features, do a pairplot of the remaining feature numerical feature\n",
    "sns.pairplot(cc_train_copy[cc_train_copy['Employment length'] < 0].drop(['ID','Has a mobile phone', 'Has a work phone', 'Has a phone', 'Has an email','Is high risk'],axis=1),corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4d2fb",
   "metadata": {},
   "source": [
    "Now let's look at the two interesting scatter plots.\n",
    "\n",
    "We will start with the family member count vs children count. Of course, the more children a person has, the larger the family count. We added a line of best fit, also called the regression line, and you can read more about it in this blog post [here](https://semasuka.github.io/blog/2021/04/04/demystify-machine-learning.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa9b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='Children count',y='Family member count',data=cc_train_copy,line_kws={'color': 'red'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8004c513",
   "metadata": {},
   "source": [
    "When we compare the employment length and age, the scatterplot shows a trend between the age and the length of employment.\n",
    "\n",
    "It is shaped like a reversed triangle because the applicants' age increases with the employment length. You can't have an employment length that is superior to the age. Right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf82e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_age = np.abs(cc_train_copy['Age'])/365.25\n",
    "x_employ_length = np.abs(\n",
    "    cc_train_copy[cc_train_copy['Employment length'] < 0]['Employment length'])/365.25\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.scatterplot(x_employ_length, y_age, alpha=.05)\n",
    "# change the frequency of the x-axis and y-axis labels\n",
    "plt.xticks(np.arange(0, x_employ_length.max(), 2.5))\n",
    "plt.yticks(np.arange(20, y_age.max(), 5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ea803",
   "metadata": {},
   "source": [
    "Now comparing account age and applicant age, we can see that most applicants are between 20 and 45 years old and have an account less than 25 months old. This information is deduced from darker blue hexagons (high-density area) between 22 and 43 on the Y axis and between 3 and 28 on the X axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15661d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(np.abs(cc_train_copy['Account age']),y_age, kind=\"hex\", height=12)\n",
    "plt.yticks(np.arange(20, y_age.max(), 5))\n",
    "plt.xticks(np.arange(0, 65, 5))\n",
    "plt.ylabel('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7717c8",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc63991",
   "metadata": {},
   "source": [
    "Time to do a correlation between all the numerical features using a heatmap. This heatmap shows the correlation between all the numerical features; the darker the cell, the more correlated the two features are, and the lighter the color, the less correlated the two features.\n",
    "\n",
    "No feature is correlated with the target feature (Which is high risk). We see a strong correlation (0.89) between family member count and children count, as previously seen with the pairplot (The more children a person has, the larger the family count). Age has some positive correlation (0.30) with the family member count and children count. The older a person is, the most likely they will have a larger family and consequently more children.\n",
    "\n",
    "Another positive correlation (0.31) is having a phone and having a work phone. We have a slightly positive correlation between age and work phone(0.18); younger people will be less likely to own a work phone. As previously discussed, we also have a negative  (-0.62) between employment length and age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476473e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the datatype of the target feature to int\n",
    "is_high_risk_int = cc_train_copy['Is high risk'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation analysis with heatmap, after dropping the has a mobile phone with the target feature as int\n",
    "cc_train_copy_corr_no_mobile = pd.concat([cc_train_copy.drop(['Has a mobile phone','Is high risk'], axis=1),is_high_risk_int],axis=1).corr()\n",
    "# Get the lower triangle of the correlation matrix\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(cc_train_copy_corr_no_mobile, dtype='bool')\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(18,10))\n",
    "# seaborn heatmap\n",
    "sns.heatmap(cc_train_copy_corr_no_mobile, annot=True, cmap='flare',mask=mask, linewidths=.5)\n",
    "# plot the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1247d63",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c930a65",
   "metadata": {},
   "source": [
    "Now, let's do an ANOVA (analysis of variance) between age and other categorical features.\n",
    "\n",
    "But before we proceed, what is an ANOVA? ANOVA tells you if there are any statistical differences between the means of two or more independent features (categorical features).\n",
    "\n",
    "Now, let's use box plots to compare age's mean and different categorical features. Female applicants are older than their male counterparts, and those who don't own a car with property owners tend to be older. Of course, the pensioners are older than those working (We also see that some have pensioned at a young age, those are outliers).\n",
    "\n",
    "It is also interesting to see that those with an academic degree are generally younger than the other groups. The widows tend to be much older, with some young outliers in their 30s. Unsurprisingly, those who live with their parents tend to be younger, and we also see some outliers here. Lastly, those who work as cleaning staff tend to be older, while those who work in IT tend to be younger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dff1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4,2,figsize=(15,20),dpi=180)\n",
    "fig.tight_layout(pad=5.0)\n",
    "cat_features = ['Gender', 'Has a car', 'Has a property', 'Employment status', 'Education level', 'Marital status', 'Dwelling', 'Job title']\n",
    "for cat_ft_count, ax in enumerate(axes):\n",
    "    for row_count in range(4):\n",
    "        for feat_count in range(2):\n",
    "            sns.boxplot(ax=axes[row_count,feat_count],x=cc_train_copy[cat_features[cat_ft_count]],y=np.abs(cc_train_copy['Age'])/365.25)\n",
    "            axes[row_count,feat_count].set_title(cat_features[cat_ft_count] + \" vs age\")\n",
    "            plt.sca(axes[row_count,feat_count])\n",
    "            plt.xticks(rotation=45,ha='right')\n",
    "            plt.ylabel('Age')\n",
    "            cat_ft_count += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cf847",
   "metadata": {},
   "source": [
    "Now let's turn our attention to employment length versus categorical features. The only interesting observation is that state-employed and medical staff applicants tend to have been employed longer than the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99426f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4,2,figsize=(15,20),dpi=180)\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "for cat_ft_count, ax in enumerate(axes):\n",
    "    for row_count in range(4):\n",
    "        for feat_count in range(2):\n",
    "            sns.boxplot(ax=axes[row_count,feat_count],x=cc_train_copy[cat_features[cat_ft_count]],y=np.abs(cc_train_copy[cc_train_copy['Employment length'] < 0]['Employment length'])/365.25)\n",
    "            axes[row_count,feat_count].set_title(cat_features[cat_ft_count] + \" vs employment length\")\n",
    "            plt.sca(axes[row_count,feat_count])\n",
    "            plt.ylabel('Employment length')\n",
    "            plt.xticks(rotation=45,ha='right')\n",
    "            cat_ft_count += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a70de5d",
   "metadata": {},
   "source": [
    "### Applicant general profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0186619",
   "metadata": {},
   "source": [
    "After analyzing each feature, we can create a typical credit card applicant profile. Here is the profile:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf8d334",
   "metadata": {},
   "source": [
    "- ***Typical profile of an applicant is a Female in her early 40’s, married with a partner and no child. She has been employed for five years with a salary of 157500. She has completed her secondary education. She does not own a car but owns a property (a house/ apartment). Her account is 26 months old.***\n",
    "- ***Age and income do not have any effects on the target variable***\n",
    "- ***Those flagged as bad clients tend to have a shorter employment length and older accounts. They also constitute less than 2% of total applicants.***\n",
    "- ***Most applicants are 20 to 45 years old and have an account that is 30 months old or less.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a8a68",
   "metadata": {},
   "source": [
    "### 3. Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0a1d3",
   "metadata": {},
   "source": [
    "Using EDA, here is a list of all the transformations that need to be done on each feature:\n",
    "\n",
    "ID:\n",
    "* Drop the feature\n",
    "\n",
    "Gender:\n",
    "* One hot encoding\n",
    "\n",
    "Age:\n",
    "* Min-max scaling\n",
    "* Fix skewness\n",
    "* Absolute values and divide by 365.25\n",
    "\n",
    "Marital status:\n",
    "* One hot encoding\n",
    "\n",
    "Family member count\n",
    "* Fix outliers\n",
    "\n",
    "Children count\n",
    "* Fix outliers\n",
    "* Drop feature\n",
    "\n",
    "Dwelling type\n",
    "* One hot encoding\n",
    "\n",
    "Income\n",
    "* Remove outliers\n",
    "* Fix skewness\n",
    "* Min-max scaling\n",
    "\n",
    "Job title\n",
    "* One hot encoding\n",
    "* Impute missing values\n",
    "\n",
    "Employment status:\n",
    "* One hot encoding\n",
    "\n",
    "Education level:\n",
    "* Ordinal encoding\n",
    "\n",
    "Employment length:\n",
    "* Remove outliers\n",
    "* Min-max scaling\n",
    "* Absolute values and divide by 365.25\n",
    "* change days of employment of retirees to 0\n",
    "\n",
    "Has a car:\n",
    "* Change it to numerical\n",
    "* One-hot encoding\n",
    "\n",
    "Has a property:\n",
    "* Change it to numerical\n",
    "* One-hot encoding\n",
    "\n",
    "Has a mobile phone:\n",
    "* Drop feature\n",
    "\n",
    "Has a work phone:\n",
    "* One-hot encoding\n",
    "\n",
    "Has a phone:\n",
    "* One-hot encoding\n",
    "\n",
    "Has an email:\n",
    "* One-hot encoding\n",
    "\n",
    "Account age:\n",
    "* Drop feature\n",
    "\n",
    "Is high risk(Target):\n",
    "* Change the data type to numerical\n",
    "* balance the data with SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820631d",
   "metadata": {
    "id": "1820631d"
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa853213",
   "metadata": {
    "id": "fa853213"
   },
   "source": [
    "Here we are creating a class to handle outliers. But why do we have to remove the outliers?\n",
    "\n",
    "Outliers are data points that differ significantly from other observations in the dataset. Outliers can spoil and mislead the training process resulting in longer training times, less accurate models and ultimately poorer results, which means that outliers must remove from the dataset.\n",
    "\n",
    "This class will remove outliers more or less than 3 inter-quantile ranges away from the mean. This class will be the first class in the scikit-learn ```Pipeline``` to call.\n",
    "\n",
    "Note: Refer to this picture below to understand IQR. In the image below, 1.5 IQR is used; in our case, we use 3 IQR, which is more sensitive to extreme outliers than 1.5 IQR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aeceb6",
   "metadata": {},
   "source": [
    "![iqr](../assets/post_cont_image/iqr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffc3ea0",
   "metadata": {},
   "source": [
    "Image credit: [Research gate](https://www.researchgate.net/figure/Interquartile-range-IQR-projection-on-a-normally-distributed-density-The-median-of-IQR_fig2_340969321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d1e14",
   "metadata": {
    "id": "0e2d1e14"
   },
   "outputs": [],
   "source": [
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,feat_with_outliers = ['Family member count','Income', 'Employment length']):\n",
    "        # initializing the instance of the object\n",
    "        self.feat_with_outliers = feat_with_outliers\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        # check if the feature in part of the dataset's features\n",
    "        if (set(self.feat_with_outliers).issubset(df.columns)):\n",
    "            # 25% quantile\n",
    "            Q1 = df[self.feat_with_outliers].quantile(.25)\n",
    "            # 75% quantile\n",
    "            Q3 = df[self.feat_with_outliers].quantile(.75)\n",
    "            IQR = Q3 - Q1\n",
    "            # keep the data within 3 IQR only and discard the rest\n",
    "            df = df[~((df[self.feat_with_outliers] < (Q1 - 3 * IQR)) |(df[self.feat_with_outliers] > (Q3 + 3 * IQR))).any(axis=1)]\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23637fe",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87653fe",
   "metadata": {},
   "source": [
    "Next is feature selection; here, we will drop the features that we judge are not useful in our prediction. Note this is not a feature selection based on the model coefficients or feature importance; it is purely based on logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de1f44",
   "metadata": {},
   "source": [
    "The features to be dropped are ```ID```, ```has a mobile phone```, ``` children count```, ```job title```, ```account age```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443bd13",
   "metadata": {},
   "source": [
    "Now the next question is, why are we dropping these features?\n",
    "\n",
    "- ID: ID is not helpful for prediction, it helped us when we were merging the two datasets, but after that, there is no need to keep it.\n",
    "- Has a mobile phone: Since everyone has a mobile phone, this feature does not inform us about anything and is useless for the model.\n",
    "- Children count: is highly correlated with Family member count, and to avoid multicollinearity, we will drop it.\n",
    "- Job title: Has some missing values and the count of each category is not very different to justify using the mode to fill the missing values. So we drop it.\n",
    "- Account age: Because Account age is used to create the target, reusing it will make our model overfit. Plus, this information is unknown while applying for a credit card and is not a predictor feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a44c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropFeatures(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,feature_to_drop = ['ID','Has a mobile phone','Children count','Job title','Account age']):\n",
    "        self.feature_to_drop = feature_to_drop\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if (set(self.feature_to_drop).issubset(df.columns)):\n",
    "            # drop the list of features\n",
    "            df.drop(self.feature_to_drop,axis=1,inplace=True)\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d622dec1",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc0164",
   "metadata": {},
   "source": [
    "This class will convert the features that use days (```Employment length```, ```Age```) to absolute value because we can't have negative days of employment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63649c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeConversionHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feat_with_days = ['Employment length', 'Age']):\n",
    "        self.feat_with_days = feat_with_days\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        if (set(self.feat_with_days).issubset(X.columns)):\n",
    "            # convert days to absolute value using NumPy\n",
    "            X[['Employment length','Age']] = np.abs(X[['Employment length','Age']])\n",
    "            return X\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769f779",
   "metadata": {},
   "source": [
    "The following class will convert the employment length of retirees (set to 365243) to 0 so that it is not considered an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bebf34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetireeHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, df):\n",
    "        return self\n",
    "    def transform(self, df):\n",
    "        if 'Employment length' in df.columns:\n",
    "            # select rows with an employment length is 365243, which corresponds to retirees\n",
    "            df_ret_idx = df['Employment length'][df['Employment length'] == 365243].index\n",
    "            # set those rows with value 365243 to 0\n",
    "            df.loc[df_ret_idx,'Employment length'] = 0\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Employment length is not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fca0ac",
   "metadata": {},
   "source": [
    "Using the cubic root transformation, this class will reduce income and age distribution skewness. Skewed features negatively affect our predictive model's performance, and machine learning models perform better with normally distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkewnessHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,feat_with_skewness=['Income','Age']):\n",
    "        self.feat_with_skewness = feat_with_skewness\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if (set(self.feat_with_skewness).issubset(df.columns)):\n",
    "            # Handle skewness with cubic root transformation\n",
    "            df[self.feat_with_skewness] = np.cbrt(df[self.feat_with_skewness])\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3287ed",
   "metadata": {},
   "source": [
    "This class will change 1 to the character \"Y\" and 0 to \"N,\" which will be more comprehensive when we do a one-hot encoding for these features ```Has a work phone```, ```Has a phone```, ```Has an email```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb016a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinningNumToYN(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,feat_with_num_enc=['Has a work phone','Has a phone','Has an email']):\n",
    "        self.feat_with_num_enc = feat_with_num_enc\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if (set(self.feat_with_num_enc).issubset(df.columns)):\n",
    "            # Change 0 to N and 1 to Y for all the features in feat_with_num_enc\n",
    "            for ft in self.feat_with_num_enc:\n",
    "                df[ft] = df[ft].map({1:'Y',0:'N'})\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d64b6",
   "metadata": {},
   "source": [
    "This class will do one-hot encoding on the categorical features, but also this class will keep the names of the features. We want to keep the feature names instead of an array without names (default) because the feature names will be used for feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dffc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotWithFeatNames(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,one_hot_enc_ft = ['Gender', 'Marital status', 'Dwelling', 'Employment status', 'Has a car', 'Has a property', 'Has a work phone', 'Has a phone', 'Has an email']):\n",
    "        self.one_hot_enc_ft = one_hot_enc_ft\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if (set(self.one_hot_enc_ft).issubset(df.columns)):\n",
    "            # function to one-hot encode the features\n",
    "            def one_hot_enc(df,one_hot_enc_ft):\n",
    "                # instantiate the OneHotEncoder object\n",
    "                one_hot_enc = OneHotEncoder()\n",
    "                # fit the dataframe with the features we want to one-hot encode\n",
    "                one_hot_enc.fit(df[one_hot_enc_ft])\n",
    "                # get output feature names for transformation.\n",
    "                feat_names_one_hot_enc = one_hot_enc.get_feature_names_out(one_hot_enc_ft)\n",
    "                # change the one hot encoding array to a dataframe with the column names\n",
    "                df = pd.DataFrame(one_hot_enc.transform(df[self.one_hot_enc_ft]).toarray(),columns=feat_names_one_hot_enc,index=df.index)\n",
    "                return df\n",
    "            # function to concatenate the one hot encoded features with the rest of the features that were not encoded\n",
    "            def concat_with_rest(df,one_hot_enc_df,one_hot_enc_ft):\n",
    "                # get the rest of the features that are not encoded\n",
    "                rest_of_features = [ft for ft in df.columns if ft not in one_hot_enc_ft]\n",
    "                # concatenate the rest of the features with the one hot encoded features\n",
    "                df_concat = pd.concat([one_hot_enc_df, df[rest_of_features]],axis=1)\n",
    "                return df_concat\n",
    "            # call the one_hot_enc function and stores the dataframe in the one_hot_enc_df variable\n",
    "            one_hot_enc_df = one_hot_enc(df,self.one_hot_enc_ft)\n",
    "            # returns the concatenated dataframe and stores it in the full_df_one_hot_enc variable\n",
    "            full_df_one_hot_enc = concat_with_rest(df,one_hot_enc_df,self.one_hot_enc_ft)\n",
    "            return full_df_one_hot_enc\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda034a4",
   "metadata": {},
   "source": [
    "This class will convert the education level to an ordinal encoding. Here we use ordinal encoding instead of one-hot encoding because we know that the education level is ranked (University is higher than primary school)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022933f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalFeatNames(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,ordinal_enc_ft = ['Education level']):\n",
    "        self.ordinal_enc_ft = ordinal_enc_ft\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if 'Education level' in df.columns:\n",
    "            # instantiate the OrdinalEncoder object\n",
    "            ordinal_enc = OrdinalEncoder()\n",
    "            df[self.ordinal_enc_ft] = ordinal_enc.fit_transform(df[self.ordinal_enc_ft])\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Education level is not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78609df2",
   "metadata": {},
   "source": [
    "This class will scale the feature using min-max scaling while keeping the feature names. You may ask why we have to scale. Well, some of the numerical features range from 0 to 20 (Family member count) while others range from 27000 to 1575000 (Income), so this means that some machine learning algorithms will weight the features with big numbers more than the feature with smaller numbers which should not be the case. So scaling all the numerical feature on the same scale (0 to 1) solve this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxWithFeatNames(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,min_max_scaler_ft = ['Age', 'Income', 'Employment length']):\n",
    "        self.min_max_scaler_ft = min_max_scaler_ft\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if (set(self.min_max_scaler_ft).issubset(df.columns)):\n",
    "            # instantiate the MinMaxScaler object\n",
    "            min_max_enc = MinMaxScaler()\n",
    "            # fit and transform on a scale 0 to 1\n",
    "            df[self.min_max_scaler_ft] = min_max_enc.fit_transform(df[self.min_max_scaler_ft])\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf159ac",
   "metadata": {},
   "source": [
    "This class will change the data type of the target variable to numerical as it is an object data type even though it is 0 and 1's (0 and 1's expressed as strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeToNumTarget(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        # check if the target is part of the dataframe\n",
    "        if 'Is high risk' in df.columns:\n",
    "            # change to a numeric data type using Pandas\n",
    "            df['Is high risk'] = pd.to_numeric(df['Is high risk'])\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Is high risk is not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410b685",
   "metadata": {},
   "source": [
    "This class will oversample the target variable using SMOTE because the minority class (Is high risk = 1) is scarce in the data, as we have seen while doing EDA of the target variable (```1``` only accounts for about 1.71% of the total data while ```0``` represent 98.29%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73016dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oversample(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,df):\n",
    "        return self\n",
    "    def transform(self,df):\n",
    "        if 'Is high risk' in df.columns:\n",
    "            # smote function instantiation to oversample the minority class to fix the imbalance data\n",
    "            oversample = SMOTE(sampling_strategy='minority')\n",
    "            # fit and resample the classes and assign them to X_bal, y_bal variable\n",
    "            X_bal, y_bal = oversample.fit_resample(df.loc[:, df.columns != 'Is high risk'],df['Is high risk'])\n",
    "            # concatenate the balanced classes column-wise\n",
    "            df_bal = pd.concat([pd.DataFrame(X_bal),pd.DataFrame(y_bal)],axis=1)\n",
    "            return df_bal\n",
    "        else:\n",
    "            print(\"Is high risk is not in the dataframe\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7ac3e",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d8b409",
   "metadata": {},
   "source": [
    "Now we are ready to create the data preprocessing pipeline using the built sklearn function ```Pipeline```. This function calls each class in the pipeline sequentially, starting from the outlier remover to the oversample class. The dataset will be transformed consecutively from the first class to the next one till the end. The pipeline will be stored in a variable called pipeline and will call ```fit_transform``` on that variable, pass our dataframe we want to transform and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b98d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline(df):\n",
    "    # Create the pipeline that will call all the classes from OutlierRemoval() to Oversample() in one go\n",
    "    pipeline = Pipeline([\n",
    "        ('outlier_remover', OutlierRemover()),\n",
    "        ('feature_dropper', DropFeatures()),\n",
    "        ('time_conversion_handler', TimeConversionHandler()),\n",
    "        ('retiree_handler', RetireeHandler()),\n",
    "        ('skewness_handler', SkewnessHandler()),\n",
    "        ('binning_num_to_yn', BinningNumToYN()),\n",
    "        ('one_hot_with_feat_names', OneHotWithFeatNames()),\n",
    "        ('ordinal_feat_names', OrdinalFeatNames()),\n",
    "        ('min_max_with_feat_names', MinMaxWithFeatNames()),\n",
    "        ('change_to_num_target', ChangeToNumTarget()),\n",
    "        ('oversample', Oversample())\n",
    "    ])\n",
    "    df_pipe_prep = pipeline.fit_transform(df)\n",
    "    return df_pipe_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4ef73",
   "metadata": {},
   "source": [
    "Now we pass in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2442b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # Hide the warnings\n",
    "cc_train_prep = full_pipeline(cc_train_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f38c49",
   "metadata": {},
   "source": [
    "We check how many rows and columns we have after the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea56e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_train_prep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc09b567",
   "metadata": {},
   "source": [
    "Let's quickly look at the first few rows of the transformed dataframe. We can see that the columns' names have been kept, and all the transformations have taken place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56648d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "cc_train_prep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d7cf3",
   "metadata": {},
   "source": [
    "Now, we extract the target variable ```Is high risk``` from the dataframe and create a new dataframe composed of independent features (also called predictor, aka all the features except the target variable) as ```X_cc_train_prep``` and the target variable as  ```y_cc_train_prep```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f112a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train data into X and y (target)\n",
    "X_cc_train_prep, y_cc_train_prep = cc_train_prep.loc[:, cc_train_prep.columns != 'Is high risk'], cc_train_prep['Is high risk'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376db3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cc_train_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83442c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cc_train_prep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6276506",
   "metadata": {},
   "source": [
    "### Short-list promising models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b42f27",
   "metadata": {},
   "source": [
    "Alright! the moment we have been all waiting for has finally arrived; time to train our models. We first create a dictionary of models and their corresponding names. This dictionary will be used to loop through all the models and train them without having to write them over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e12c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'sgd':SGDClassifier(random_state=42,loss='perceptron'),\n",
    "    'logistic_regression':LogisticRegression(random_state=42,max_iter=1000),\n",
    "    'support_vector_machine':SVC(random_state=42,probability=True),\n",
    "    'decision_tree':DecisionTreeClassifier(random_state=42),\n",
    "    'random_forest':RandomForestClassifier(random_state=42),\n",
    "    'gaussian_naive_bayes':GaussianNB(),\n",
    "    'k_nearest_neighbors':KNeighborsClassifier(),\n",
    "    'gradient_boosting':GradientBoostingClassifier(random_state=42),\n",
    "    'linear_discriminant_analysis':LinearDiscriminantAnalysis(),\n",
    "    'bagging':BaggingClassifier(random_state=42),\n",
    "    'neural_network':MLPClassifier(random_state=42,max_iter=1000),\n",
    "    'adaboost':AdaBoostClassifier(random_state=42),\n",
    "    'extra_trees':ExtraTreesClassifier(random_state=42),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36006c8e",
   "metadata": {},
   "source": [
    "Now we will write some of the functions used for our training model. The first function is a function to plot the feature importance of the model. The feature importance is ranking features that contribute more(or less) than other features to the model prediction. The feature importance varies from one model to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4fb58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_importance_plot(model_trn, model_name):\n",
    "    '''\n",
    "    Function to get the feature importance of the classifier and plot it\n",
    "    '''\n",
    "    # in order to get the feature importance, the model should not be 'sgd','support_vector_machine','gaussian_naive_bayes','k_nearest_neighbors','bagging','neural_network'\n",
    "    if model_name not in ['sgd','support_vector_machine','gaussian_naive_bayes','k_nearest_neighbors','bagging','neural_network']:\n",
    "        # change xtick font size\n",
    "        plt.rcParams['xtick.labelsize'] = 12\n",
    "        plt.rcParams['ytick.labelsize'] = 12\n",
    "        # top 10 most predictive features\n",
    "        top_10_feat = FeatureImportances(model_trn, relative=False, topn=10)\n",
    "        # top 10 least predictive features\n",
    "        bottom_10_feat = FeatureImportances(model_trn, relative=False, topn=-10)\n",
    "        # change the figure size\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        # change x label font size\n",
    "        plt.xlabel('xlabel', fontsize=14)\n",
    "        # fit to get the feature importance\n",
    "        top_10_feat.fit(X_cc_train_prep, y_cc_train_prep)\n",
    "        # show the plot\n",
    "        top_10_feat.show()\n",
    "        print('\\n')\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.xlabel('xlabel', fontsize=14)\n",
    "        # fit to get the feature importance\n",
    "        bottom_10_feat.fit(X_cc_train_prep, y_cc_train_prep)\n",
    "        # show the plot\n",
    "        bottom_10_feat.show()\n",
    "        print('\\n')\n",
    "    else:\n",
    "        print('No feature importance for {0}'.format(model_name))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203a5b1",
   "metadata": {},
   "source": [
    "On the other hand, this function is used to get the y predictions of the model using cross-validation prediction with k fold equal to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0acaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_prediction_func(model_trn,model_name,final_model=False):\n",
    "    '''\n",
    "    Function to get the y prediction\n",
    "    '''\n",
    "    if final_model == False:\n",
    "        # check if y_train_copy_pred files exist; if not, create it\n",
    "        y_cc_train_pred_path = Path('saved_models/{0}/y_train_copy_pred_{0}.sav'.format(model_name))\n",
    "        try:\n",
    "            y_cc_train_pred_path.resolve(strict=True)\n",
    "        # if FileNotFoundError is raised\n",
    "        except FileNotFoundError:\n",
    "            # cross-validation prediction with kfold = 10\n",
    "            y_cc_train_pred = cross_val_predict(model_trn,X_cc_train_prep,y_cc_train_prep,cv=10,n_jobs=-1)\n",
    "            # save the predictions using joblib library\n",
    "            joblib.dump(y_cc_train_pred,y_cc_train_pred_path)\n",
    "            return y_cc_train_pred\n",
    "        else:\n",
    "            # if the file exists, load the predictions\n",
    "            y_cc_train_pred = joblib.load(y_cc_train_pred_path)\n",
    "            return y_cc_train_pred\n",
    "    # When we are dealing with the final model\n",
    "    else:\n",
    "        # check if y_train_copy_pred files exist; if not, create it\n",
    "        y_cc_train_pred_path_final = Path('saved_models_final/{0}/y_train_copy_pred_{0}_final.sav'.format(model_name))\n",
    "        try:\n",
    "            y_cc_train_pred_path_final.resolve(strict=True)\n",
    "        except FileNotFoundError:\n",
    "            # cross validation prediction with kfold = 10\n",
    "            y_cc_train_pred_final = cross_val_predict(model_trn,X_cc_train_prep,y_cc_train_prep,cv=10,n_jobs=-1)\n",
    "            # save the predictions\n",
    "            joblib.dump(y_cc_train_pred_final,y_cc_train_pred_path_final)\n",
    "            return y_cc_train_pred_final\n",
    "        else:\n",
    "            # if it exists load the predictions\n",
    "            y_cc_train_pred_final = joblib.load(y_cc_train_pred_path_final)\n",
    "            return y_cc_train_pred_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fec953f",
   "metadata": {},
   "source": [
    "This function will plot the confusion matrix for each of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a445fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_func(model_trn,model_name,final_model=False):\n",
    "    '''\n",
    "    Function to plot the confusion matrix\n",
    "    '''\n",
    "    if final_model == False:\n",
    "        fig, ax = plt.subplots(figsize=(8,8))\n",
    "        # plot confusion matrix\n",
    "        conf_matrix = ConfusionMatrixDisplay.from_predictions(y_cc_train_prep,y_prediction_func(model_trn,model_name),ax=ax, cmap='Blues',values_format='d')\n",
    "        # remove the grid\n",
    "        plt.grid(visible=None)\n",
    "        # increase the font size of the X and Y labels\n",
    "        plt.xlabel('Predicted label', fontsize=14)\n",
    "        plt.ylabel('True label', fontsize=14)\n",
    "        # give a title to the plot using the model name\n",
    "        plt.title('Confusion Matrix', fontsize=14)\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "    # When we are dealing with the final model\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(8,8))\n",
    "        # plot confusion matrix\n",
    "        conf_matrix_final = ConfusionMatrixDisplay.from_predictions(y_cc_train_prep,y_prediction_func(model_trn,model_name,final_model=True),ax=ax, cmap='Blues',values_format='d')\n",
    "        # remove the grid\n",
    "        plt.grid(visible=None)\n",
    "        # increase the font size of the X and Y labels\n",
    "        plt.xlabel('Predicted label', fontsize=14)\n",
    "        plt.ylabel('True label', fontsize=14)\n",
    "        # give a title to the plot using the model name\n",
    "        plt.title('Confusion Matrix', fontsize=14)\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c6b6d",
   "metadata": {},
   "source": [
    "The following function will plot the ROC curve of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb519db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_func(model_trn,model_name,final_model=False):\n",
    "    '''\n",
    "    Function to plot the roc curve\n",
    "    '''\n",
    "    if final_model == False:\n",
    "        # check if the y probabilities file exists; if not create it\n",
    "        y_proba_path = Path('saved_models/{0}/y_cc_train_proba_{0}.sav'.format(model_name))\n",
    "        try:\n",
    "            y_proba_path.resolve(strict=True)\n",
    "        # if the FileNotFoundError is raised\n",
    "        except FileNotFoundError:\n",
    "            # calculate the y probability\n",
    "            y_cc_train_proba = model_trn.predict_proba(X_cc_train_prep)\n",
    "            # save y_cc_train_proba file at y_proba_path\n",
    "            joblib.dump(y_cc_train_proba,y_proba_path)\n",
    "        else:\n",
    "            # if path exist load the y probabilities file\n",
    "            y_cc_train_proba = joblib.load(y_proba_path)\n",
    "        # plot the roc curve\n",
    "        skplt.metrics.plot_roc(y_cc_train_prep, y_cc_train_proba, title = 'ROC curve for {0}'.format(model_name), cmap='cool',figsize=(8,6), text_fontsize='large')\n",
    "        # remove the grid\n",
    "        plt.grid(visible=None)\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "    # When we are dealing with the final model\n",
    "    else:\n",
    "        # check if y probabilities file exists, if not create it\n",
    "        y_proba_path_final = Path('saved_models_final/{0}/y_cc_train_proba_{0}_final.sav'.format(model_name))\n",
    "        try:\n",
    "            y_proba_path_final.resolve(strict=True)\n",
    "        except FileNotFoundError:\n",
    "            y_cc_train_proba_final = model_trn.predict_proba(X_cc_train_prep)\n",
    "            joblib.dump(y_cc_train_proba_final,y_proba_path_final)\n",
    "        else:\n",
    "            # if path exist load the y probabilities file\n",
    "            y_cc_train_proba_final = joblib.load(y_proba_path_final)\n",
    "        # plot the roc curve\n",
    "        skplt.metrics.plot_roc(y_cc_train_prep, y_cc_train_proba_final, title = 'ROC curve for {0}'.format(model_name), cmap='cool',figsize=(8,6), text_fontsize='large')\n",
    "        # remove the grid\n",
    "        plt.grid(visible=None)\n",
    "        plt.show()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7630f",
   "metadata": {},
   "source": [
    "This other function will print the classification report. A classification report is a table that describes the performance of a classification model and has information like precision, recall, f1-score, support, accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae72da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_func(model_trn, model_name, final_model=False):\n",
    "    '''\n",
    "    Function to display the classification report\n",
    "    '''\n",
    "    if final_model == False:\n",
    "        class_report = classification_report(y_cc_train_prep,y_prediction_func(model_trn,model_name))\n",
    "        print(class_report)\n",
    "    # When we are dealing with the final model\n",
    "    else:\n",
    "        class_report_final = classification_report(y_cc_train_prep,y_prediction_func(model_trn,model_name,final_model=True))\n",
    "        print(class_report_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada445aa",
   "metadata": {},
   "source": [
    "This function will train the models and save them in the ```saved_models``` and ```saved_models_final``` folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c824df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,model_name,final_model=False):\n",
    "    '''\n",
    "    Function to train and save the model\n",
    "    '''\n",
    "    # If we are not training the final model\n",
    "    if final_model == False:\n",
    "        # Check if the model file exists and if not, create, train and save it\n",
    "        model_file_path = Path('saved_models/{0}/{0}_model.sav'.format(model_name))\n",
    "        try:\n",
    "            model_file_path.resolve(strict=True)\n",
    "        except FileNotFoundError:\n",
    "            if model_name == 'sgd':\n",
    "                # for sgd, loss = 'hinge' does not have a predict_proba method. Therefore, we use a calibrated model\n",
    "                calibrated_model = CalibratedClassifierCV(model, cv=10, method='sigmoid')\n",
    "                # train the model\n",
    "                model_trn = calibrated_model.fit(X_cc_train_prep,y_cc_train_prep)\n",
    "            # For the rest of the models\n",
    "            else:\n",
    "                model_trn = model.fit(X_cc_train_prep,y_cc_train_prep)\n",
    "            # save the model\n",
    "            joblib.dump(model_trn,model_file_path)\n",
    "            return model_trn\n",
    "        else:\n",
    "            # if path exist load the model\n",
    "            model_trn = joblib.load(model_file_path)\n",
    "            return model_trn\n",
    "    else:\n",
    "        # check if the final model file exist and if not create, train and save it\n",
    "        final_model_file_path = Path('saved_models_final/{0}/{0}_model.sav'.format(model_name))\n",
    "        try:\n",
    "            final_model_file_path.resolve(strict=True)\n",
    "        except FileNotFoundError:\n",
    "            # train the model\n",
    "            model_trn = model.fit(X_cc_train_prep,y_cc_train_prep)\n",
    "            joblib.dump(model_trn,final_model_file_path)\n",
    "            return model_trn\n",
    "        else:\n",
    "            # if path exist load the model\n",
    "            model_trn = joblib.load(final_model_file_path)\n",
    "            return model_trn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c23a9c",
   "metadata": {},
   "source": [
    "This function below will look at the ```folder_check_model``` which will check if ```saved_models``` folder exists; if not, it will create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d268b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_check_model():\n",
    "    # check if the folder for saving the model exists, if not create it\n",
    "    if not os.path.exists('saved_models/{}'.format(model_name)):\n",
    "        os.makedirs('saved_models/{}'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc176068",
   "metadata": {},
   "source": [
    "Lastly, we create a for loop function that will go through the dictionary of models and call all the functions that we have defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564fa546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all the models\n",
    "for model_name,model in classifiers.items():\n",
    "    # title formatting\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    print('  {}  '.center(50,'-').format(model_name))\n",
    "    print('\\n')\n",
    "    # check if the folder for saving the model exists; if not create it\n",
    "    folder_check_model()\n",
    "    # train the model\n",
    "    model_trn = train_model(model,model_name)\n",
    "    # print the scores from the classification report\n",
    "    score_func(model_trn, model_name)\n",
    "    # plot the ROC curve\n",
    "    roc_curve_func(model_trn,model_name)\n",
    "    # plot the confusion matrix\n",
    "    confusion_matrix_func(model_trn,model_name)\n",
    "    # plot feature importance\n",
    "    feat_importance_plot(model_trn, model_name)\n",
    "    warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931fd0a5",
   "metadata": {},
   "source": [
    "### What metrics to use in order to choose the best model for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe8aad",
   "metadata": {},
   "source": [
    "  Since the objective of this problem is to minimize the risk of a credit default, the metrics to use depends on the current economic situation:\n",
    "\n",
    "  - During a bull market (when the economy is expanding), people feel wealthy and are employed. Money is usually cheap, and the risk of default is low because of economic stability and low unemployment. The financial institution can handle the risk of default; therefore, it is not very strict about giving credit. The financial institution can handle some bad clients as long as most credit card owners are good clients (aka those who pay back their credit in time and in total).In this case, having a good recall (sensitivity) is ideal.\n",
    "\n",
    "  - During a bear market (when the economy is contracting), people lose their jobs and money through the stock market and other investment venues. Many people struggle to meet their financial obligations. The financial institution, therefore, tends to be more conservative in giving out credit or loans. The financial institution can't afford to give out credit to many clients who won't be able to pay back their credit. The financial institution would rather have a smaller number of good clients, even if it means that some good clients are denied credit. In this case, having good precision (specificity) is desirable.\n",
    "\n",
    "    ***Note***: There is always a trade-off between precision and recall. Choosing the right metrics depends on the problem you are solving.\n",
    "\n",
    "    ***Conclusion***: Since the time I worked on this project (beginning in 2022), we have been in the longest bull market (excluding March 2020 flash crash) ever recorded; we will use recall as our metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119177e6",
   "metadata": {},
   "source": [
    "### Top model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435211f8",
   "metadata": {},
   "source": [
    "Using the ROC curve and recall, we can conclude that the best model is:\n",
    "- Gradient boosting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e95c6",
   "metadata": {},
   "source": [
    "Let's look at the picture below to understand how to interpret a ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30614cb0",
   "metadata": {},
   "source": [
    "![heatmap](../assets/post_cont_image/roc_curve.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcdf3de",
   "metadata": {},
   "source": [
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#/media/File:Roc_curve.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82a1d0",
   "metadata": {},
   "source": [
    "With this ROC curve, we can compare the performance of different classifiers. The closer the curve is to the top left corner of the plot without actually reaching the far end of the corner, the better the model\n",
    "\n",
    "- Any classifier's ROC below the dashed red line performs worst than random chance. Random chance is a 50% chance of being correct for a binary classifier.\n",
    "- Any classifier with the ROC curve blended with the dashed red line is no better than tossing a fair coin.\n",
    "- The orange curve is slightly better than the dashed red line, but that would not be considered a good classifier.\n",
    "- The green curve is much better than the orange one but could be better.\n",
    "- The blue curve is the best classifier here; this curve gets closer to the top left without touching the top left corner.\n",
    "- Lastly, the \"perfect\" curve that touches the top left corner is not a good classifier. You might be asked why; well, a classifier with this curve is overfitting, meaning it has learned so well on the training data but can't generalize well on the test data (unseen data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f684957",
   "metadata": {},
   "source": [
    "So what to do when a classifier is overfitting? Well, these are the options to deal with this issue:\n",
    "- Use a simplified model by selecting fewer parameters or constraining the model (also called regularization).\n",
    "- Gather more training data.\n",
    "- Discard outliers and fix missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b3d9df",
   "metadata": {},
   "source": [
    "### Test the final model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18824b6",
   "metadata": {},
   "source": [
    "Now that we have our model trained, we can use it to predict the classes on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a82d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_test_copy.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b177b8",
   "metadata": {},
   "source": [
    "We pass to the scikit-learn pipeline the test set as we did before for the training set to obtain a preprocessed dataset ready for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_test_prep = full_pipeline(cc_test_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f02ff6d",
   "metadata": {},
   "source": [
    "We extract the independent variables/features and the target variable and store them into variables ```X_cc_test_prep``` and ```y_cc_test_prep``` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train data into X and y (target)\n",
    "X_cc_test_prep, y_cc_test_prep = cc_test_prep.loc[:, cc_test_prep.columns != 'Is high risk'], cc_test_prep['Is high risk'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6137bf3",
   "metadata": {},
   "source": [
    "Next, we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model_trn = train_model(classifiers['gradient_boosting'],'gradient_boosting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e59bb",
   "metadata": {},
   "source": [
    "Then predict the dependent variable (predicted target) and store the prediction in the ```final_prediction``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = model_trn.predict(X_cc_test_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8950029",
   "metadata": {},
   "source": [
    "Now we use the ```shape``` method to get the number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c18be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d08342",
   "metadata": {},
   "source": [
    "We use the ```sum``` function to compare the predictions and actual target values. We store the count of the correct predictions in ```n_correct```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb525cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct = sum(final_predictions == y_cc_test_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1c895",
   "metadata": {},
   "source": [
    "We divide the number of correct predictions by the total number of predictions to get the accuracy. We achieved 85% accuracy on the testing set, which is very good! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_correct/len(final_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ed0f1",
   "metadata": {},
   "source": [
    "### Deploying the model on AWS S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b31db",
   "metadata": {},
   "source": [
    "Now we will deploy the gradient boosting model we previously saved on our local machine to AWS S3, but what is an AWS S3 bucket, we may ask?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191d122",
   "metadata": {},
   "source": [
    "AWS S3 (S3 stands for Simple Storage Service) is a cloud storage service that provides access to affordable data storage in the cloud. Our trained gradient boosting model stored on S3 can be accessed with access and secret access keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aab246",
   "metadata": {},
   "source": [
    "Now, let's store the gradient boosting model on AWS S3, but you must create an AWS account first. AWS has a free tier subscription, and hosting this model on an S3 bucket is free of charge; also, remember to create an account as a root user. After creating an account on AWS, sign in as a root user and type on the search bar s3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe51b47",
   "metadata": {},
   "source": [
    "![search bar s3](../assets/post_cont_image/search_bar_s3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40ef19",
   "metadata": {},
   "source": [
    "You should see a dropdown menu; click on the first option with a green bucket logo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e883a4f",
   "metadata": {},
   "source": [
    "It will take you to the Amazon s3 landing page, and click the Create bucket button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2642a72",
   "metadata": {},
   "source": [
    "![create a bucket](../assets/post_cont_image/create_bucket.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e573e",
   "metadata": {},
   "source": [
    "You will be prompted with this page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8f17cb",
   "metadata": {},
   "source": [
    "![create bucket page](../assets/post_cont_image/create_bucket_page.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a595be1",
   "metadata": {},
   "source": [
    "Give the bucket a name; in this case, we can call our bucket name creditcardapproval; in one word, select an AWS region close to your location for better latency. We will keep the default option for the rest, then hit the create bucket button.\n",
    "\n",
    "We see the bucket we just created in the list of buckets on the S3 landing page. Click on that bucket name, and you shall see the page below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4059ce27",
   "metadata": {},
   "source": [
    "![upload to bucket](../assets/post_cont_image/upload_bucket.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc2d15",
   "metadata": {},
   "source": [
    "Click on the upload button, which will prompt you to another page. Click on add files, locate our saved model, hit the upload button and wait for it to upload to the bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe90e94",
   "metadata": {},
   "source": [
    "![add the file to the bucket](../assets/post_cont_image/add_bucket.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc8de3",
   "metadata": {},
   "source": [
    "Our model is uploaded on AWS. The status should be successful if everything goes well, like the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24745e",
   "metadata": {},
   "source": [
    "![succeeded uploaded bucket](../assets/post_cont_image/succeeded_bucket.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a1d88e",
   "metadata": {},
   "source": [
    "We have our model uploaded on the S3; we can now access it and make a prediction using access and secret access keys. So how do we get those two keys? We use IAM user and we need to create one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba6b86",
   "metadata": {},
   "source": [
    "Search for iam and click on users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6327e1c",
   "metadata": {},
   "source": [
    "![search iam](../assets/post_cont_image/users_iam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed886619",
   "metadata": {},
   "source": [
    "Assuming you don't have any IAM users, you must create one by clicking on the add users.\n",
    "\n",
    "Note: I already have mine created, so I will add a new IAM user to show you how to get the keys because we can only access the secret access key once after creating a new user. Once it is created, you can no longer access the secret access key. So keep it private and store it in a safe place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216d7b89",
   "metadata": {},
   "source": [
    "![add IAM user](../assets/post_cont_image/add_iam_user.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9044e622",
   "metadata": {},
   "source": [
    "Give it a name, let's say ```stern-test``` or whatever you want. Check the access key checkbox so we can access our s3 blob storage API; now let's go to the next step, which is the permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe924d0",
   "metadata": {},
   "source": [
    "![IAM user detail](../assets/post_cont_image/user_details.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67202e08",
   "metadata": {},
   "source": [
    "We will attach existing policies directly for the permission page. The existing permission we will be using are ```AmazonS3FullAccess``` and ```AWSCompromisedKeyQuarantineV2``` and check the corresponding checkbox. We will set the user without the permission boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9505e",
   "metadata": {},
   "source": [
    "![permission iam](../assets/post_cont_image/permissions_iam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e368ff",
   "metadata": {},
   "source": [
    "The next page is the tags page. IAM tags are key-value pairs you can add to your user. Tags can include user information, such as an email address, or can be descriptive, such as a job title. You can use the tags to organize, track, or control access for this user. Tags are optional, so it is up to you if you want to set them or not. I did not use them on my end since it is not helpful for this project. Press next for the review of the IAM user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc948d5",
   "metadata": {},
   "source": [
    "The review page is just a summary of the previous pages. Once you have reviewed it and satisfy with it, create the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2924fcc7",
   "metadata": {},
   "source": [
    "Now comes the most crucial page; once the user is created, you will be prompted with the user name, the access and the secret access key. These keys will be used when linking our Streamlit web app with the hosted model on AWS. You can download the two keys as CSV files or copy them on your clipboard.\n",
    "\n",
    "Note: This is the only time AWS will give you access to the secret access key for security purposes. You must create a new IAM user if you lose the secret access key. Please don't share the keys; copy/save them in a safe place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f8e7c",
   "metadata": {},
   "source": [
    "Now that you have saved your access and secret access key, you can close the page, and if you go back to the IAM welcome page, you can see the user you just created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082eb56",
   "metadata": {},
   "source": [
    "![final page iam user](../assets/post_cont_image/final_iam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230abe69",
   "metadata": {},
   "source": [
    "![user creation confirmation](../assets/post_cont_image/confirmation-iam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ee3a3",
   "metadata": {},
   "source": [
    "With our model stored on S3 and the two keys in our possession. We are good to go to our last two sections with Streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4da04",
   "metadata": {},
   "source": [
    "### Streamlit Web Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24166484",
   "metadata": {},
   "source": [
    "So we have our trained model stored on AWS S3. We need an interface for the model where someone can input their information in a sort of form (which is their profile) and predict if they will be approved for a credit card or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede34f9",
   "metadata": {},
   "source": [
    "When I was working on this project I encounter an issue on how to do the data preparation (feature selection, engineering and data preprocessing) of that one applicant who is using the app annd encountered errors that I could not figure out. This got exharbated due to the fact Streamlit does not support jupyter notebooks (.ipynb), only support python files (.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48374860",
   "metadata": {},
   "source": [
    "What I did to overcome this issue, was to do append the profile of our applicant we are prediction to training data and did all the data preprocessing with the training data in a the same python script where we have the streamlit interface (yes I know it is not a good practice) and extracted the last row which correspond to our application.\n",
    "\n",
    "Note: I appended to the training dataset but I did not retrain the model (that may result in overfitting the model). I only did the data preprocessing as we will see shortly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15e5be",
   "metadata": {},
   "source": [
    "The following code is part of one python script (saved as .py) that will be used streamlit web interface deployment, I will explain what each session does but wont explain in details for the data preprocessiong part because most of the function are extact the same as the function from the above section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c7bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries we have already seen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "# new libraries we have not seen\n",
    "import streamlit as st\n",
    "import boto3\n",
    "import tempfile\n",
    "import json\n",
    "import requests\n",
    "from streamlit_lottie import st_lottie_spinner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38f36b",
   "metadata": {},
   "source": [
    "The first libraries in the script below we have already seen them, and pretty much accomplish the same things as seen previously. The second parts are libraries we have not seen yet.\n",
    "\n",
    "- Streamlit is an amazing library that creates an interface for our model, and very easy to deploy using the streamlit share free service\n",
    "- Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2. In this project, we will use it to connect our interface to the train model on AWS S3 through the access and secret access key.\n",
    "- tempfile is a module creates temporary files and directories. In this project it is used to store temporally our trained model into this python script\n",
    "- json is used here form the streamlit hand animation while the model is predicting (This library is optional since it is for the animation, does not affect in no way our predictions)\n",
    "- request is used to get the animation from the server using http request (This library is optional too)\n",
    "- streamlit_lottie is the animation library for streamlit (This library is optional as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc92bf",
   "metadata": {},
   "source": [
    "We will quickly skim through the next section, if you forgot what each function does, feel free to refer to the sections above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945aa5a",
   "metadata": {},
   "source": [
    "So we will import the training and testing data directly stored as raw file on Github\n",
    "\n",
    "Note: This data already has the target variable calculated.\n",
    "\n",
    "So now, we concatenate the training and testing on the row axis and do a resampling(reshuffling), split the data (80% for the training data and 20% for the testing data). We make a copy of them and store them into ```train_copy``` and ```test_copy``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65877d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original = pd.read_csv('https://raw.githubusercontent.com/semasuka/Credit-card-approval-prediction-classification/main/datasets/train.csv')\n",
    "\n",
    "test_original = pd.read_csv('https://raw.githubusercontent.com/semasuka/Credit-card-approval-prediction-classification/main/datasets/test.csv')\n",
    "\n",
    "full_data = pd.concat([train_original, test_original], axis=0)\n",
    "\n",
    "full_data = full_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def data_split(df, test_size):\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
    "    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_original, test_original = data_split(full_data, 0.2)\n",
    "\n",
    "train_copy = train_original.copy()\n",
    "test_copy = test_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b3c417",
   "metadata": {},
   "source": [
    "After this, we reuse the same functions and classes we used for the data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9303fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_cnt_norm_cal(df, feature):\n",
    "    '''Function that will return the value count and frequency of each observation within a feature'''\n",
    "    # get the value counts of each feature\n",
    "    ftr_value_cnt = df[feature].value_counts()\n",
    "    # normalize the value counts on a scale of 100\n",
    "    ftr_value_cnt_norm = df[feature].value_counts(normalize=True) * 100\n",
    "    # concatenate the value counts with normalized value count column wise\n",
    "    ftr_value_cnt_concat = pd.concat(\n",
    "        [ftr_value_cnt, ftr_value_cnt_norm], axis=1)\n",
    "    # give it a column name\n",
    "    ftr_value_cnt_concat.columns = ['Count', 'Frequency (%)']\n",
    "    # return the dataframe\n",
    "    return ftr_value_cnt_concat\n",
    "\n",
    "\n",
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feat_with_outliers=['Family member count', 'Income', 'Employment length']):\n",
    "        # initializing the instance of the object\n",
    "        self.feat_with_outliers = feat_with_outliers\n",
    "\n",
    "    def fit(self, df):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # check if the feature in part of the dataset's features\n",
    "        if (set(self.feat_with_outliers).issubset(df.columns)):\n",
    "            # 25% quantile\n",
    "            Q1 = df[self.feat_with_outliers].quantile(.25)\n",
    "            # 75% quantile\n",
    "            Q3 = df[self.feat_with_outliers].quantile(.75)\n",
    "            IQR = Q3 - Q1\n",
    "            # keep the data within 3 IQR only and discard the rest\n",
    "            df = df[~((df[self.feat_with_outliers] < (Q1 - 3 * IQR)) |\n",
    "                      (df[self.feat_with_outliers] > (Q3 + 3 * IQR))).any(axis=1)]\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df\n",
    "\n",
    "\n",
    "class DropFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_to_drop=['ID', 'Has a mobile phone', 'Children count', 'Job title', 'Account age']):\n",
    "        self.feature_to_drop = feature_to_drop\n",
    "\n",
    "    def fit(self, df):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        if (set(self.feature_to_drop).issubset(df.columns)):\n",
    "            # drop the list of features\n",
    "            df.drop(self.feature_to_drop, axis=1, inplace=True)\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df\n",
    "\n",
    "\n",
    "class TimeConversionHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feat_with_days=['Employment length', 'Age']):\n",
    "        self.feat_with_days = feat_with_days\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if (set(self.feat_with_days).issubset(X.columns)):\n",
    "            # convert days to absolute value using NumPy\n",
    "            X[['Employment length', 'Age']] = np.abs(\n",
    "                X[['Employment length', 'Age']])\n",
    "            return X\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return X\n",
    "\n",
    "\n",
    "class RetireeHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        if 'Employment length' in df.columns:\n",
    "            # select rows with an employment length is 365243, which corresponds to retirees\n",
    "            df_ret_idx = df['Employment length'][df['Employment length'] == 365243].index\n",
    "            # set those rows with value 365243 to 0\n",
    "            df.loc[df_ret_idx, 'Employment length'] = 0\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Employment length is not in the dataframe\")\n",
    "            return df\n",
    "\n",
    "\n",
    "class SkewnessHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feat_with_skewness=['Income', 'Age']):\n",
    "        self.feat_with_skewness = feat_with_skewness\n",
    "\n",
    "    def fit(self, df):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        if (set(self.feat_with_skewness).issubset(df.columns)):\n",
    "            # Handle skewness with cubic root transformation\n",
    "            df[self.feat_with_skewness] = np.cbrt(df[self.feat_with_skewness])\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df\n",
    "\n",
    "\n",
    "class BinningNumToYN(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feat_with_num_enc=['Has a work phone', 'Has a phone', 'Has an email']):\n",
    "        self.feat_with_num_enc = feat_with_num_enc\n",
    "\n",
    "    def fit(self, df):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        if (set(self.feat_with_num_enc).issubset(df.columns)):\n",
    "            # Change 0 to N and 1 to Y for all the features in feat_with_num_enc\n",
    "            for ft in self.feat_with_num_enc:\n",
    "                df[ft] = df[ft].map({1: 'Y', 0: 'N'})\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df\n",
    "\n",
    "\n",
    "class OneHotWithFeatNames(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, one_hot_enc_ft=['Gender', 'Marital status', 'Dwelling', 'Employment status', 'Has a car', 'Has a property', 'Has a work phone', 'Has a phone', 'Has an email']):\n",
    "        self.one_hot_enc_ft = one_hot_enc_ft\n",
    "\n",
    "    def fit(self, df):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        if (set(self.one_hot_enc_ft).issubset(df.columns)):\n",
    "            # function to one-hot encode the features\n",
    "            def one_hot_enc(df, one_hot_enc_ft):\n",
    "                # instantiate the OneHotEncoder object\n",
    "                one_hot_enc = OneHotEncoder()\n",
    "                # fit the dataframe with the features we want to one-hot encode\n",
    "                one_hot_enc.fit(df[one_hot_enc_ft])\n",
    "                # get output feature names for transformation.\n",
    "                feat_names_one_hot_enc = one_hot_enc.get_feature_names_out(\n",
    "                    one_hot_enc_ft)\n",
    "                # change the one hot encoding array to a dataframe with the column names\n",
    "                df = pd.DataFrame(one_hot_enc.transform(df[self.one_hot_enc_ft]).toarray(\n",
    "                ), columns=feat_names_one_hot_enc, index=df.index)\n",
    "                return df\n",
    "            # function to concatenate the one hot encoded features with the rest of the features that were not encoded\n",
    "\n",
    "            def concat_with_rest(df, one_hot_enc_df, one_hot_enc_ft):\n",
    "                # get the rest of the features that are not encoded\n",
    "                rest_of_features = [\n",
    "                    ft for ft in df.columns if ft not in one_hot_enc_ft]\n",
    "                # concatenate the rest of the features with the one hot encoded features\n",
    "                df_concat = pd.concat(\n",
    "                    [one_hot_enc_df, df[rest_of_features]], axis=1)\n",
    "                return df_concat\n",
    "            # call the one_hot_enc function and stores the dataframe in the one_hot_enc_df variable\n",
    "            one_hot_enc_df = one_hot_enc(df, self.one_hot_enc_ft)\n",
    "            # returns the concatenated dataframe and stores it in the full_df_one_hot_enc variable\n",
    "            full_df_one_hot_enc = concat_with_rest(\n",
    "                df, one_hot_enc_df, self.one_hot_enc_ft)\n",
    "            return full_df_one_hot_enc\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df\n",
    "\n",
    "\n",
    "class OrdinalFeatNames(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ordinal_enc_ft=['Education level']):\n",
    "        self.ordinal_enc_ft = ordinal_enc_ft\n",
    "\n",
    "    def fit(self, df):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        if 'Education level' in df.columns:\n",
    "            # instantiate the OrdinalEncoder object\n",
    "            ordinal_enc = OrdinalEncoder()\n",
    "            df[self.ordinal_enc_ft] = ordinal_enc.fit_transform(\n",
    "                df[self.ordinal_enc_ft])\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Education level is not in the dataframe\")\n",
    "            return df\n",
    "\n",
    "\n",
    "class MinMaxWithFeatNames(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_max_scaler_ft=['Age', 'Income', 'Employment length']):\n",
    "        self.min_max_scaler_ft = min_max_scaler_ft\n",
    "\n",
    "    def fit(self, df):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        if (set(self.min_max_scaler_ft).issubset(df.columns)):\n",
    "            # instantiate the MinMaxScaler object\n",
    "            min_max_enc = MinMaxScaler()\n",
    "            # fit and transform on a scale 0 to 1\n",
    "            df[self.min_max_scaler_ft] = min_max_enc.fit_transform(\n",
    "                df[self.min_max_scaler_ft])\n",
    "            return df\n",
    "        else:\n",
    "            print(\"One or more features are not in the dataframe\")\n",
    "            return df\n",
    "\n",
    "\n",
    "class ChangeToNumTarget(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # check if the target is part of the dataframe\n",
    "        if 'Is high risk' in df.columns:\n",
    "            # change to a numeric data type using Pandas\n",
    "            df['Is high risk'] = pd.to_numeric(df['Is high risk'])\n",
    "            return df\n",
    "        else:\n",
    "            print(\"Is high risk is not in the dataframe\")\n",
    "            return df\n",
    "\n",
    "\n",
    "class Oversample(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, df):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        if 'Is high risk' in df.columns:\n",
    "            # smote function instantiation to oversample the minority class to fix the imbalance data\n",
    "            oversample = SMOTE(sampling_strategy='minority')\n",
    "            # fit and resample the classes and assign them to X_bal, y_bal variable\n",
    "            X_bal, y_bal = oversample.fit_resample(\n",
    "                df.loc[:, df.columns != 'Is high risk'], df['Is high risk'])\n",
    "            # concatenate the balanced classes column-wise\n",
    "            df_bal = pd.concat(\n",
    "                [pd.DataFrame(X_bal), pd.DataFrame(y_bal)], axis=1)\n",
    "            return df_bal\n",
    "        else:\n",
    "            print(\"Is high risk is not in the dataframe\")\n",
    "            return df\n",
    "\n",
    "\n",
    "def full_pipeline(df):\n",
    "    # Create the pipeline that will call all the classes from OutlierRemoval() to Oversample() in one go\n",
    "    pipeline = Pipeline([\n",
    "        ('outlier_remover', OutlierRemover()),\n",
    "        ('feature_dropper', DropFeatures()),\n",
    "        ('time_conversion_handler', TimeConversionHandler()),\n",
    "        ('retiree_handler', RetireeHandler()),\n",
    "        ('skewness_handler', SkewnessHandler()),\n",
    "        ('binning_num_to_yn', BinningNumToYN()),\n",
    "        ('one_hot_with_feat_names', OneHotWithFeatNames()),\n",
    "        ('ordinal_feat_names', OrdinalFeatNames()),\n",
    "        ('min_max_with_feat_names', MinMaxWithFeatNames()),\n",
    "        ('change_to_num_target', ChangeToNumTarget()),\n",
    "        ('oversample', Oversample())\n",
    "    ])\n",
    "    df_pipe_prep = pipeline.fit_transform(df)\n",
    "    return df_pipe_prep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d521eb0",
   "metadata": {},
   "source": [
    "Now let's work on the Streamlit interface/dashboard that our applicant will be filling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c607c38",
   "metadata": {},
   "source": [
    "We first start by creating a title and a brief description of our interface and what it does. The streamlit function ```st.write``` will accept within the parantheses markdown markup language. So that fist line that starts with ```#``` is equivalent to HTML heading H1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00df1876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 21:33:14.398 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/sternsemasuka/Desktop/ML/Project/Credit-card-approval-prediction-classification/.Credit_approval_env/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "st.write(\"\"\"\n",
    "# Credit card approval prediction\n",
    "This app predicts if an applicant will be approved for a credit card or not. Just fill in the following information and click on the Predict button.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ea2dd",
   "metadata": {},
   "source": [
    "The first input we will get from the applicant is gender, and the streamlit radio button function to choose between two option is ```st.radio```. We store the output into ```input_gender``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23083cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender input\n",
    "st.write(\"\"\"\n",
    "## Gender\n",
    "\"\"\")\n",
    "input_gender = st.radio('Select you gender',['Male','Female'], index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90666157",
   "metadata": {},
   "source": [
    "For age, we will use a slider instead, with a maximum value of 70 and minimum value of 18 with one step at the time. We are change age to days by multiplying it with 365.25 as we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age input slider\n",
    "st.write(\"\"\"\n",
    "## Age\n",
    "\"\"\")\n",
    "input_age = np.negative(st.slider(\n",
    "    'Select your age', value=42, min_value=18, max_value=70, step=1) * 365.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cf62ff",
   "metadata": {},
   "source": [
    "We use a drop down for marital status. Each marital status string value is mapped to an index to create a dictionary which will be used to return that string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marital status input dropdown\n",
    "st.write(\"\"\"\n",
    "## Marital status\n",
    "\"\"\")\n",
    "# get the index from value_cnt_norm_cal function\n",
    "marital_status_values = list(\n",
    "    value_cnt_norm_cal(full_data, 'Marital status').index)\n",
    "marital_status_key = ['Married', 'Single/not married', 'Civil marriage', 'Separated', 'Widowed']\n",
    "# mapping of the values and keys\n",
    "marital_status_dict = dict(zip(marital_status_key, marital_status_values))\n",
    "# streamlit dropdown menu function, value stored in input_marital_status_key\n",
    "input_marital_status_key = st.selectbox(\n",
    "    'Select your marital status', marital_status_key)\n",
    "\n",
    "# get the corresponding value\n",
    "input_marital_status_val = marital_status_dict.get(input_marital_status_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7cbb28",
   "metadata": {},
   "source": [
    "We again select the family count using streamlit dropdown menu.\n",
    "\n",
    "Note: since we have removed outliers from our training model, we will only the family count to 6 which emcompass most scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b53373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Family member count\n",
    "st.write(\"\"\"\n",
    "## Family member count\n",
    "\"\"\")\n",
    "fam_member_count = float(st.selectbox('Select your family member count', [1,2,3,4,5,6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed468bbd",
   "metadata": {},
   "source": [
    "We use dropdown menu for ```dwelling type``` just like we did for ```Marital status```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac61188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dwelling type dropdown\n",
    "st.write(\"\"\"\n",
    "## Dwelling type\n",
    "\"\"\")\n",
    "dwelling_type_values = list(value_cnt_norm_cal(full_data, 'Dwelling').index)\n",
    "dwelling_type_key = ['House / apartment', 'Live with parents', 'Municipal apartment ', 'Rented apartment', 'Office apartment', 'Co-op apartment']\n",
    "dwelling_type_dict = dict(zip(dwelling_type_key, dwelling_type_values))\n",
    "input_dwelling_type_key = st.selectbox(\n",
    "    'Select the type of dwelling you reside in', dwelling_type_key)\n",
    "input_dwelling_type_val = dwelling_type_dict.get(input_dwelling_type_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57de527",
   "metadata": {},
   "source": [
    "For income, we will input income value in a text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd7f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income\n",
    "st.write(\"\"\"\n",
    "## Income\n",
    "\"\"\")\n",
    "input_income = np.int(st.text_input('Enter your income (in USD)',0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3649cf7",
   "metadata": {},
   "source": [
    "We will proceed the same way for employment status as marital status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employment status dropdown\n",
    "st.write(\"\"\"\n",
    "## Employment status\n",
    "\"\"\")\n",
    "employment_status_values = list(\n",
    "    value_cnt_norm_cal(full_data, 'Employment status').index)\n",
    "employment_status_key = [\n",
    "    'Working', 'Commercial associate', 'Pensioner', 'State servant', 'Student']\n",
    "employment_status_dict = dict(\n",
    "    zip(employment_status_key, employment_status_values))\n",
    "input_employment_status_key = st.selectbox(\n",
    "    'Select your employment status', employment_status_key)\n",
    "input_employment_status_val = employment_status_dict.get(\n",
    "    input_employment_status_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f26876",
   "metadata": {},
   "source": [
    "We use a slider for the employment length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ec74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employment length input slider\n",
    "st.write(\"\"\"\n",
    "## Employment length\n",
    "\"\"\")\n",
    "input_employment_length = np.negative(st.slider(\n",
    "    'Select your employment length', value=6, min_value=0, max_value=30, step=1) * 365.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce0725",
   "metadata": {},
   "source": [
    "We use a dropdown for the education level dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48a94b21",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sternsemasuka/Desktop/blog/ipynb/Credit card approval prediction.ipynb Cell 357\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sternsemasuka/Desktop/blog/ipynb/Credit%20card%20approval%20prediction.ipynb#Z1042sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Education level dropdown\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sternsemasuka/Desktop/blog/ipynb/Credit%20card%20approval%20prediction.ipynb#Z1042sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m st\u001b[39m.\u001b[39mwrite(\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sternsemasuka/Desktop/blog/ipynb/Credit%20card%20approval%20prediction.ipynb#Z1042sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m## Education level\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sternsemasuka/Desktop/blog/ipynb/Credit%20card%20approval%20prediction.ipynb#Z1042sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sternsemasuka/Desktop/blog/ipynb/Credit%20card%20approval%20prediction.ipynb#Z1042sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m edu_level_values \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(value_cnt_norm_cal(full_data, \u001b[39m'\u001b[39m\u001b[39mEducation level\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mindex)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sternsemasuka/Desktop/blog/ipynb/Credit%20card%20approval%20prediction.ipynb#Z1042sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m edu_level_key \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mSecondary school\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mHigher education\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIncomplete higher\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLower secondary\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAcademic degree\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sternsemasuka/Desktop/blog/ipynb/Credit%20card%20approval%20prediction.ipynb#Z1042sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m edu_level_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(edu_level_key, edu_level_values))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Education level dropdown\n",
    "st.write(\"\"\"\n",
    "## Education level\n",
    "\"\"\")\n",
    "edu_level_values = list(value_cnt_norm_cal(full_data, 'Education level').index)\n",
    "edu_level_key = ['Secondary school', 'Higher education', 'Incomplete higher', 'Lower secondary', 'Academic degree']\n",
    "edu_level_dict = dict(zip(edu_level_key, edu_level_values))\n",
    "input_edu_level_key = st.selectbox(\n",
    "    'Select your education status', edu_level_key)\n",
    "input_edu_level_val = edu_level_dict.get(input_edu_level_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232221ee",
   "metadata": {},
   "source": [
    "We use ```st.radio``` streamlit function (radio button select only one input between two choices) for car ownship feature, propterty ownship, work phone input, phone input, email input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f5e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car ownship input\n",
    "st.write(\"\"\"\n",
    "## Car ownship\n",
    "\"\"\")\n",
    "input_car_ownship = st.radio('Do you own a car?', ['Yes', 'No'], index=0)\n",
    "\n",
    "# Property ownship input\n",
    "st.write(\"\"\"\n",
    "## Property ownship\n",
    "\"\"\")\n",
    "input_prop_ownship = st.radio('Do you own a property?', ['Yes', 'No'], index=0)\n",
    "\n",
    "\n",
    "# Work phone input\n",
    "st.write(\"\"\"\n",
    "## Work phone\n",
    "\"\"\")\n",
    "input_work_phone = st.radio(\n",
    "    'Do you have a work phone?', ['Yes', 'No'], index=0)\n",
    "work_phone_dict = {'Yes': 1, 'No': 0}\n",
    "work_phone_val = work_phone_dict.get(input_work_phone)\n",
    "\n",
    "# Phone input\n",
    "st.write(\"\"\"\n",
    "## Phone\n",
    "\"\"\")\n",
    "input_phone = st.radio('Do you have a phone?', ['Yes', 'No'], index=0)\n",
    "work_dict = {'Yes': 1, 'No': 0}\n",
    "phone_val = work_dict.get(input_phone)\n",
    "\n",
    "# Email input\n",
    "st.write(\"\"\"\n",
    "## Email\n",
    "\"\"\")\n",
    "input_email = st.radio('Do you have an email?', ['Yes', 'No'], index=0)\n",
    "email_dict = {'Yes': 1, 'No': 0}\n",
    "email_val = email_dict.get(input_email)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da17a639",
   "metadata": {},
   "source": [
    "The final element on the interface is the predict button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a42679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Button\n",
    "predict_bt = st.button('Predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4482bae",
   "metadata": {},
   "source": [
    "So now that we have the interface ready and all the inputs variables, we can store those inputs variables into a list which is the profile we are predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253da842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all the input variables\n",
    "profile_to_predict = [0,  # ID (which will be dropped in the pipeline)\n",
    "                    input_gender[:1],  # get the first element in gender\n",
    "                    input_car_ownship[:1],  # get the first element in car ownership\n",
    "                    input_prop_ownship[:1],  # get the first element in property ownership\n",
    "                    0, # Children count (which will be dropped in the pipeline)\n",
    "                    input_income,  # Income\n",
    "                    input_employment_status_val,  # Employment status\n",
    "                    input_edu_level_val,  # Education level\n",
    "                    input_marital_status_val,  # Marital status\n",
    "                    input_dwelling_type_val,  # Dwelling type\n",
    "                    input_age,  # Age\n",
    "                    input_employment_length,    # Employment length\n",
    "                    1, # Has a mobile phone (which will be dropped in the pipeline)\n",
    "                    work_phone_val,  # Work phone\n",
    "                    phone_val,  # Phone\n",
    "                    email_val,  # Email\n",
    "                    'to_be_droped', # Job title (which will be dropped in the pipeline)\n",
    "                    fam_member_count,  # Family member count\n",
    "                    0.00, # Account age (which will be dropped in the pipeline)\n",
    "                    0  # target set to 0 as a placeholder\n",
    "                    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6c79a",
   "metadata": {},
   "source": [
    "We will change the list into a 1 row dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae5450",
   "metadata": {},
   "source": [
    "profile_to_predict_df = pd.DataFrame([profile_to_predict],columns=train_copy.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d95485",
   "metadata": {},
   "source": [
    "We will add the profile to predict as a last row in the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab0f0a",
   "metadata": {},
   "source": [
    "train_copy_with_profile_to_pred = pd.concat([train_copy,profile_to_predict_df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a8fcb",
   "metadata": {},
   "source": [
    "We will prepare the whole dataset (profile to predict with the training dataset) with the ```full_pipeline``` function that we have defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd6022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole dataset prepared\n",
    "train_copy_with_profile_to_pred_prep = full_pipeline(train_copy_with_profile_to_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f759c",
   "metadata": {},
   "source": [
    "To get our applicant profile observation, we first get the row with the ID = 0, and then drop the ID with target(which was added as placeholder) column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99ab38eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_copy_with_profile_to_pred_prep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sternsemasuka/Desktop/blog/ipynb/Credit card approval prediction.ipynb Cell 371\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sternsemasuka/Desktop/blog/ipynb/Credit%20card%20approval%20prediction.ipynb#Z1101sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m profile_to_pred_prep \u001b[39m=\u001b[39m train_copy_with_profile_to_pred_prep[train_copy_with_profile_to_pred_prep[\u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mIs high risk\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_copy_with_profile_to_pred_prep' is not defined"
     ]
    }
   ],
   "source": [
    "profile_to_pred_prep = train_copy_with_profile_to_pred_prep[train_copy_with_profile_to_pred_prep['ID'] == 0].drop(columns=['ID','Is high risk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1361e07",
   "metadata": {},
   "source": [
    "Now we will add an optional but cool animation of an impatient hand that will be displayed when the model is doing the prediction, here is how it looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e7e36",
   "metadata": {},
   "source": [
    "![hand animation](../assets/post_cont_image/hand_ani.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175a6d6",
   "metadata": {},
   "source": [
    "Check out the animation in action [here](https://lottiefiles.com/89308-loading-hand-green)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8fcf3c",
   "metadata": {},
   "source": [
    "And here is the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e124bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Animation function\n",
    "@st.experimental_memo\n",
    "def load_lottieurl(url: str):\n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        return None\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "lottie_loading_an = load_lottieurl(\n",
    "    'https://assets3.lottiefiles.com/packages/lf20_szlepvdh.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a50e0",
   "metadata": {},
   "source": [
    "Last but not least, we will create a function that will make the predictions. We first get the client from AWS S3 using the ```boto3.client``` function and store it in the ```client``` variable.\n",
    "\n",
    "Now you might ask, how are we passing the keys to this function yet there is no where pasted our access and secret access key. Well this will be done when we will deploying to streamlit share in the sections below.\n",
    "\n",
    "Now we declare our bucket and model name stored on AWS, then load the model from AWS into temporally file usitn the ```tempfile``` library, download, load and return a prediction as ```0``` (is not high risk) or ```1``` (is high risk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_prediction():\n",
    "    # connect to s3 bucket with the access and secret access key\n",
    "    client = boto3.client(\n",
    "        's3', aws_access_key_id=st.secrets[\"access_key\"], aws_secret_access_key=st.secrets[\"secret_access_key\"])\n",
    "\n",
    "    bucket_name = \"creditapplipred\"\n",
    "    key = \"gradient_boosting_model.sav\"\n",
    "\n",
    "    # load the model from s3 in a temporary file\n",
    "    with tempfile.TemporaryFile() as fp:\n",
    "        # download our model from AWS\n",
    "        client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "        # change the position of the File Handle to the beginning of the file\n",
    "        fp.seek(0)\n",
    "        # load the model using joblib library\n",
    "        model = joblib.load(fp)\n",
    "\n",
    "    # prediction from the model, returns 0 or 1\n",
    "    return model.predict(profile_to_pred_prep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d007f",
   "metadata": {},
   "source": [
    "Now let's create an if statement that will call the function above only when someone click on the predict button. The following code will be execute only when ```predict_bt``` is ```1``` meaning when some click the predict button.\n",
    "\n",
    "Once the animation will run as long as the ```make_prediction``` function is running and will stop once the function has finished executing. If the result from the prediction is ```0```, another if statement will display a green banner for success, it it is ```1``` a red banner will be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if predict_bt:\n",
    "    # will run the animation as long as the function is running, if final_pred exit, then stop displaying the loading animation\n",
    "    with st_lottie_spinner(lottie_loading_an, quality='high', height='200px', width='200px'):\n",
    "        final_pred = make_prediction()\n",
    "    # the prediction is 0\n",
    "    if final_pred[0] == 0:\n",
    "        # display a green banner for success\n",
    "        st.success('## You have been approved for a credit card')\n",
    "        # display the streamlit ballon\n",
    "        st.balloons()\n",
    "    # if prediction is 1\n",
    "    elif final_pred[0] == 1:\n",
    "        # display a red banner for error/failure\n",
    "        st.error('## Unfortunately, you have not been approved for a credit card')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941b48d",
   "metadata": {},
   "source": [
    "That is it guys!! We have our Streamlit interface ready to go, now we need to deploy it on Streamlit share and share it with the world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4075202",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ef617426",
    "71feece0",
    "eb6ac952",
    "82b8daa8",
    "43cf4692",
    "e4ef4b64",
    "383a486c",
    "d24b8b90",
    "22b2affe",
    "c9786e58",
    "dae7bc71",
    "62891180",
    "b4837d9f",
    "032271c2",
    "ac1b59bf",
    "750c1952",
    "5d71d8db",
    "bb1b82aa",
    "92fa8814",
    "637435e4",
    "a117639a",
    "4a84ab5f",
    "c96fdc01",
    "dd46cce2",
    "d9f9562a",
    "a53e3e40",
    "b2f11d79",
    "adc52e6f",
    "542165dc",
    "1820631d",
    "fa853213",
    "4802cb32",
    "1bdc73b4",
    "59d18a0f",
    "719010d1",
    "9bfabb3c",
    "f8688612",
    "a341a4d9",
    "bf224df0",
    "e3ef0138",
    "d1bf7741",
    "bb580e40",
    "392a66c6",
    "f3dd9af0",
    "0d3e8cac",
    "f8f3ae43",
    "7e9e0265",
    "bf36a844",
    "e1323b10",
    "e8cdac8a",
    "266e10fe",
    "4cf706ff",
    "66c5e7b5",
    "6a42126b",
    "c6dd1f85",
    "c1a9d426",
    "08c9a89b"
   ],
   "name": "Credit card approval prediction.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "0b79941bf6abfa97c11cce899e2bfe6606585cc2329e00a10bb8e91ee8e76a75"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
